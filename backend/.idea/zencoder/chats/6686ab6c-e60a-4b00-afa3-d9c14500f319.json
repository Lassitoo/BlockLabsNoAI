{
    "checksum": "b2d2a70dbecc2667e354b7c659b99d2c4775cec56a283fea4cc8c07349ba0a88",
    "data": {
    "id": "6686ab6c-e60a-4b00-afa3-d9c14500f319",
    "createdAt": 1760831416492,
    "updatedAt": 1760838829512,
    "title": "Probl√®me de sauvegarde des m√©tadonn√©es",
    "messages": [
        {
            "id": "72319ee3-b7de-4c41-990b-9858c01161b5",
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Dans la partie upload pour extraire les metadonn√©es l edition ne marche je veux que l edition pour la bas comme dans la page edit_metatdata mais ca me donne ca quand j essaie de sauvegarder "
                },
                {
                    "type": "text",
                    "text": "Not Found: /rawdocs/save-structured-edits/305/\n\nNot Found: /rawdocs/save-structured-edits/305/\n\n[19/Oct/2025 00:47:40] \"POST /rawdocs/save-structured-edits/305/ HTTP/1.1\" 404 19912\n\n\n\nC'est bon je peux editer mais quand je met sauvegarder on me met erreur reseau lors de la sauvegarde"
                }
            ],
            "context": {
                "currentFile": {
                    "path": "rawdocs/utils.py",
                    "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/utils.py",
                    "sources": [
                        "opened_tab"
                    ],
                    "content": "# rawdocs/utils.py\nimport os\nimport re\nimport json\nimport requests\nfrom pathlib import Path\nfrom datetime import datetime\nfrom urllib.parse import urlparse\nimport json\nimport requests\nfrom groq import Groq\n\nimport spacy\nfrom PyPDF2 import PdfReader\nfrom langdetect import detect\nfrom dotenv import load_dotenv\n\nimport subprocess\nimport json\nfrom pdfminer.high_level import extract_text\nfrom langdetect import detect\nimport spacy\nfrom pathlib import Path\nfrom docx import Document\nimport pdfplumber\nimport fitz  # PyMuPDF pour l'extraction d'images\nfrom PIL import Image\nimport io\nimport base64\n\n# Charger les variables d'environnement depuis .env\nload_dotenv()\n\n# Chargez une fois les mod√®les spaCy :\nNLP_FR = spacy.load(\"fr_core_news_sm\")\nNLP_EN = spacy.load(\"en_core_web_sm\")\n\n# Stopwords FR/EN basiques pour nettoyage\nSTOPWORDS = {\n    \"le\", \"la\", \"les\", \"de\", \"des\", \"du\", \"un\", \"une\", \"et\", \"en\", \"√†\", \"dans\", \"que\", \"qui\",\n    \"pour\", \"par\", \"sur\", \"avec\", \"au\", \"aux\", \"ce\", \"ces\", \"se\", \"ses\", \"est\",\n    \"the\", \"and\", \"of\", \"to\", \"in\", \"that\", \"it\", \"is\", \"was\", \"for\", \"on\", \"are\", \"with\",\n    \"as\", \"i\", \"at\", \"be\", \"by\", \"this\"\n}\n\n\ndef extract_exif_metadata(file_path):\n    \"\"\"Extraction brute avec ExifTool\"\"\"\n    print(f\"üîç Tentative d'extraction ExifTool pour {file_path}\")\n    try:\n        result = subprocess.run(\n            [\"exiftool\", \"-json\", file_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        if result.returncode != 0:\n            print(f\"‚ùå Erreur ExifTool: {result.stderr}\")\n            return {}\n        metadata = json.loads(result.stdout)[0]\n        print(f\"‚úÖ ExifTool a extrait {len(metadata)} champs: {list(metadata.keys())}\")\n        return metadata\n    except Exception as e:\n        print(f\"‚ùå Erreur ExifTool: {e}\")\n        print(f\"PATH actuel: {os.environ['PATH']}\")\n        return {}\n\n\ndef extract_text_content(file_path):\n    \"\"\"Extraction du texte brut selon le type de fichier\"\"\"\n    ext = Path(file_path).suffix.lower()\n    text = \"\"\n    print(f\"üîç D√©but de l'extraction texte pour {file_path} (type: {ext})\")\n    if ext == \".pdf\":\n        try:\n            text = extract_text(file_path)\n            print(f\"‚úÖ Texte extrait avec succ√®s ({len(text)} caract√®res): {text[:100]}...\")\n        except Exception as e:\n            print(f\"‚ùå Erreur extraction texte PDF: {e}\")\n    elif ext in [\".docx\"]:\n        try:\n            doc = Document(file_path)\n            text = \"\\n\".join([p.text for p in doc.paragraphs])\n            print(f\"‚úÖ Texte extrait avec succ√®s ({len(text)} caract√®res)\")\n        except Exception as e:\n            print(f\"‚ùå Erreur extraction texte DOCX: {e}\")\n    return text\n\n\ndef enhanced_nlp_analysis(text):\n    \"\"\"Analyse NLP renforc√©e sans LLM\"\"\"\n    meta = {}\n    if not text.strip():\n        print(\"‚ö†Ô∏è Aucun texte √† analyser pour NLP\")\n        return meta\n\n    print(\"üîç Lancement de l'analyse NLP renforc√©e\")\n\n    # D√©tection du type de document par mots-cl√©s\n    doc_types = {\n        'guideline': ['guideline', 'guidance', 'recommendations', 'best practices', 'guide'],\n        'regulation': ['regulation', 'directive', 'law', 'legal', 'article', 'decree'],\n        'report': ['report', 'assessment', 'evaluation', 'analysis', 'study'],\n        'procedure': ['procedure', 'protocol', 'method', 'process', 'operating'],\n        'specifications': ['specifications', 'specs', 'requirements', 'criteria', 'specification'],\n        'manufacturer': ['manufacturer', 'manufacturing', 'production', 'facility', 'plant'],\n        'certificate': ['certificate', 'certification', 'approval', 'authorization'],\n        'standard': ['standard', 'norm', 'iso', 'quality standard']\n    }\n\n    text_lower = text.lower()\n    for doc_type, keywords in doc_types.items():\n        if any(keyword in text_lower for keyword in keywords):\n            meta['type'] = doc_type\n            print(f\"‚úÖ Type d√©tect√©: {doc_type}\")\n            break\n\n    # Extraction de dates am√©lior√©e avec regex\n    date_patterns = [\n        r'\\b(\\d{1,2})\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{4})\\b',\n        r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{1,2}),?\\s+(\\d{4})\\b',\n        r'\\b(\\d{1,2})/(\\d{1,2})/(\\d{4})\\b',\n        r'\\b(\\d{4})-(\\d{1,2})-(\\d{1,2})\\b',\n        r'\\b(\\d{1,2})\\.(\\d{1,2})\\.(\\d{4})\\b'\n    ]\n\n    for pattern in date_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            meta['publication_date'] = match.group(0)\n            print(f\"‚úÖ Date d√©tect√©e: {meta['publication_date']}\")\n            break\n\n    # Extraction de version am√©lior√©e\n    version_patterns = [\n        r'version\\s*[:\\-]?\\s*(\\d+\\.?\\d*)',\n        r'v\\.?\\s*(\\d+\\.?\\d*)',\n        r'rev\\.?\\s*(\\d+)',\n        r'r(\\d+)',\n        r'step\\s*(\\d+)',\n        r'corr\\.?\\s*(\\d+)',\n        r'EN_(\\d+\\.?\\d*)',\n        r'_(\\d+\\.?\\d*)\\.pdf'\n    ]\n\n    for pattern in version_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            meta['version'] = match.group(1)\n            print(f\"‚úÖ Version d√©tect√©e: {meta['version']}\")\n            break\n\n    # Extraction d'organisations\n    org_patterns = [\n        r'(European Medicines Agency|EMA)',\n        r'(Food and Drug Administration|FDA)',\n        r'(Agence nationale de s√©curit√© du m√©dicament|ANSM)',\n        r'(Medicines and Healthcare products Regulatory Agency|MHRA)',\n        r'(International Council for Harmonisation|ICH)',\n        r'(World Health Organization|WHO)'\n    ]\n\n    for pattern in org_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            meta['source_organization'] = match.group(1)\n            print(f\"‚úÖ Organisation d√©tect√©e: {meta['source_organization']}\")\n            break\n\n    # Extraction de pays/r√©gion\n    country_patterns = [\n        r'\\b(European Union|EU)\\b',\n        r'\\b(United States|USA|US)\\b',\n        r'\\b(France|FR)\\b',\n        r'\\b(Ireland|IE)\\b',\n        r'\\b(Poland|PL)\\b',\n        r'\\b(Germany|DE)\\b',\n        r'\\b(United Kingdom|UK|GB)\\b'\n    ]\n\n    for pattern in country_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            country_code = {\n                'European Union': 'EU', 'EU': 'EU',\n                'United States': 'US', 'USA': 'US', 'US': 'US',\n                'France': 'FR', 'FR': 'FR',\n                'Ireland': 'IE', 'IE': 'IE',\n                'Poland': 'PL', 'PL': 'PL',\n                'Germany': 'DE', 'DE': 'DE',\n                'United Kingdom': 'UK', 'UK': 'UK', 'GB': 'UK'\n            }\n            meta['country'] = country_code.get(match.group(1), match.group(1))\n            print(f\"‚úÖ Pays d√©tect√©: {meta['country']}\")\n            break\n\n    try:\n        meta[\"language\"] = detect(text)\n        print(f\"‚úÖ Langue d√©tect√©e: {meta['language']}\")\n    except Exception as e:\n        print(f\"‚ùå Erreur d√©tection langue: {e}\")\n        meta[\"language\"] = \"unknown\"\n\n    # Extraction d'entit√©s avec spaCy\n    try:\n        doc = NLP_FR(text[:1000])  # Limiter pour performance\n        people = [ent.text for ent in doc.ents if ent.label_ == \"PER\"]\n        orgs = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n        dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n\n        if people and not meta.get('source_organization'):\n            meta[\"author_person\"] = people[0]\n            print(f\"‚úÖ Personne d√©tect√©e: {meta['author_person']}\")\n        if orgs and not meta.get('source_organization'):\n            meta[\"organization\"] = orgs[0]\n            print(f\"‚úÖ Organisation d√©tect√©e: {meta['organization']}\")\n        if dates and not meta.get('publication_date'):\n            meta[\"date\"] = dates[0]\n            print(f\"‚úÖ Date NLP d√©tect√©e: {meta['date']}\")\n    except Exception as e:\n        print(f\"‚ùå Erreur analyse spaCy: {e}\")\n\n    # Extraction de titre intelligent\n    lines = [l.strip() for l in text.split(\"\\n\") if len(l.strip()) > 10]\n    if lines:\n        # Nettoyer le titre des codes techniques\n        title_candidate = lines[0]\n        # Supprimer les codes techniques en d√©but\n        title_candidate = re.sub(r'^[A-Z0-9_/\\-\\.]+\\s+', '', title_candidate)\n        if len(title_candidate) > 10:\n            meta[\"title_inferred\"] = title_candidate\n            print(f\"‚úÖ Titre inf√©r√©: {meta['title_inferred']}\")\n\n    return meta\n\n\ndef extract_structured_metadata(file_path):\n    \"\"\"Extraction bas√©e sur la structure du document\"\"\"\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            if not pdf.pages:\n                print(\"‚ö†Ô∏è Aucune page trouv√©e dans le PDF\")\n                return {'title_structured': '', 'header_info': ''}\n\n            first_page = pdf.pages[0]\n\n            # Extraire le texte avec position\n            text_objects = first_page.extract_words()\n\n            if not text_objects:\n                print(\"‚ö†Ô∏è Aucun texte extrait de la premi√®re page\")\n                # Essayer un fallback avec pdfminer\n                try:\n                    text = extract_text(file_path)  # Utiliser pdfminer comme secours\n                    first_line = text.split('\\n')[0].strip() if text else ''\n                    return {\n                        'title_structured': first_line,\n                        'header_info': first_line\n                    }\n                except Exception as e:\n                    print(f\"‚ùå Erreur fallback pdfminer: {e}\")\n                    return {'title_structured': '', 'header_info': ''}\n\n            # Analyser les en-t√™tes (texte en haut de page)\n            header_text = []\n            for obj in text_objects:\n                if obj.get('top', float('inf')) < 100:  # 100 pixels du haut\n                    header_text.append(obj['text'])\n\n            header_string = ' '.join(header_text) if header_text else ''\n\n            # Extraire titre (g√©n√©ralement le texte le plus gros)\n            title_candidates = []\n            sizes = [obj.get('size', 0) for obj in text_objects if obj.get('size') is not None]\n\n            if not sizes:\n                print(\"‚ö†Ô∏è Aucune taille de texte trouv√©e\")\n                return {\n                    'title_structured': header_string or Path(file_path).stem,\n                    'header_info': header_string\n                }\n\n            max_size = max(sizes)\n            for obj in text_objects:\n                if obj.get('size', 0) >= max_size - 2:  # Texte de taille importante\n                    title_candidates.append(obj['text'])\n\n            title = ' '.join(title_candidates[:15]) if title_candidates else header_string or Path(file_path).stem\n\n            return {\n                'title_structured': title,\n                'header_info': header_string\n            }\n    except Exception as e:\n        print(f\"‚ùå Erreur extraction structur√©e: {e}\")\n        return {'title_structured': '', 'header_info': ''}\n\n\ndef extract_extended_pdf_properties(file_path):\n    \"\"\"Extraction propri√©t√©s PDF √©tendues\"\"\"\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            metadata = pdf.metadata or {}\n\n            # Analyser les m√©tadonn√©es XMP si disponibles\n            xmp_metadata = {}\n            try:\n                if hasattr(pdf, 'doc') and hasattr(pdf.doc, 'xmp_metadata'):\n                    xmp = pdf.doc.xmp_metadata\n                    if xmp:\n                        xmp_metadata = {\n                            'creator': getattr(xmp, 'creator', ''),\n                            'description': getattr(xmp, 'description', ''),\n                            'subject': getattr(xmp, 'subject', ''),\n                            'keywords': getattr(xmp, 'keywords', '')\n                        }\n            except Exception:\n                pass\n\n            return {**metadata, **xmp_metadata}\n    except Exception as e:\n        print(f\"‚ùå Erreur propri√©t√©s PDF: {e}\")\n        return {}\n\n\ndef regex_metadata_extraction(text):\n    \"\"\"Extraction par regex sp√©cialis√©es pour documents r√©glementaires\"\"\"\n    patterns = {\n        'ema_reference': r'EMA/[A-Z]+/[A-Z]+/\\d+/\\d+',\n        'ich_reference': r'ICH/[A-Z0-9]+/\\d+',\n        'step_version': r'Step\\s+(\\d+[a-z]?)',\n        'correction': r'Corr\\.?\\s*(\\d+)',\n        'european_directive': r'Directive\\s+(\\d{4}/\\d+/EC)',\n        'organization_regex': r'(European Medicines Agency|EMA|FDA|ANSM|MHRA)',\n        'effective_date': r'effective\\s+(?:from\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n        'published_date': r'published\\s+(?:on\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n        'adoption_date': r'adopted\\s+(?:on\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})'\n    }\n\n    extracted = {}\n    for key, pattern in patterns.items():\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            extracted[key] = match.group(1) if match.groups() else match.group(0)\n\n    return extracted\n\n\ndef merge_metadata_with_priority(metadata_sources):\n    \"\"\"Fusionner les m√©tadonn√©es avec priorit√©\"\"\"\n    merged = {}\n\n    field_mapping = {\n        'title': ['Title', 'title', 'title_inferred', 'title_structured', 'Subject'],\n        'type': ['type', 'DM_Category', 'doc_type'],\n        'source': ['Author', 'Creator', 'organization', 'source_organization', 'ema_reference', 'organization_regex'],\n        'version': ['version', 'DM_Version', 'step_version', 'correction'],\n        'publication_date': ['CreateDate', 'publication_date', 'effective_date', 'published_date', 'date'],\n        'language': ['Language', 'language', 'DM_Language'],\n        'country': ['country'],\n        'context': ['DM_Category', 'DM_Path', 'context']\n    }\n\n    for field, possible_keys in field_mapping.items():\n        for source in metadata_sources:\n            for key in possible_keys:\n                if key in source and source[key] and not merged.get(field):\n                    merged[field] = source[key]\n                    break\n            if merged.get(field):\n                break\n\n    return merged\n\n\ndef infer_additional_metadata(text):\n    \"\"\"Analyse NLP du contenu pour d√©duire d'autres m√©tadonn√©es (version am√©lior√©e)\"\"\"\n    return enhanced_nlp_analysis(text)\n\n\ndef extract_metadata_local_pipeline(file_path):\n    \"\"\"Pipeline complet am√©lior√© pour uploads locaux (sans LLM/prompts)\"\"\"\n    print(f\"üìú Pipeline d'extraction am√©lior√© pour fichier local: {file_path}\")\n\n    # 1. ExifTool (existant)\n    exif_meta = extract_exif_metadata(file_path)\n\n    # 2. Propri√©t√©s PDF √©tendues\n    pdf_meta = extract_extended_pdf_properties(file_path)\n\n    # 3. Analyse structurelle\n    struct_meta = extract_structured_metadata(file_path)\n\n    # 4. Extraction de texte compl√®te\n    text = extract_text_content(file_path)\n\n    # 5. NLP am√©lior√©e\n    nlp_meta = enhanced_nlp_analysis(text)\n\n    # 6. Regex sp√©cialis√©es\n    regex_meta = regex_metadata_extraction(text)\n\n    # 7. Fusion intelligente avec priorit√©s\n    raw_meta = merge_metadata_with_priority([\n        exif_meta,  # Priorit√© 1\n        pdf_meta,  # Priorit√© 2\n        struct_meta,  # Priorit√© 3\n        nlp_meta,  # Priorit√© 4\n        regex_meta  # Priorit√© 5\n    ])\n\n    # Mapper vers le format attendu de l'app\n    mapped_meta = {\n        \"title\": raw_meta.get(\"title\", \"\"),\n        \"type\": raw_meta.get(\"type\", \"\"),\n        \"publication_date\": raw_meta.get(\"publication_date\", \"\"),\n        \"version\": raw_meta.get(\"version\", \"\"),\n        \"source\": raw_meta.get(\"source\", \"\"),\n        \"context\": raw_meta.get(\"context\", \"\"),\n        \"country\": raw_meta.get(\"country\", \"\"),\n        \"language\": raw_meta.get(\"language\", \"\"),\n        \"url_source\": \"\"\n    }\n\n    # Calcul de qualit√© am√©lior√©\n    conf_scores = {\n        'title': 85 if mapped_meta['title'] else 0,\n        'type': 75 if mapped_meta['type'] else 0,\n        'publication_date': 70 if mapped_meta['publication_date'] else 0,\n        'version': 60 if mapped_meta['version'] else 0,\n        'source': 80 if mapped_meta['source'] else 0,\n        'context': 60 if mapped_meta['context'] else 0,\n        'country': 70 if mapped_meta['country'] else 0,\n        'language': 90 if mapped_meta['language'] else 0,\n        'url_source': 0\n    }\n\n    overall_quality = sum(conf_scores.values()) / len(conf_scores) if conf_scores else 0\n    mapped_meta['quality'] = {\n        'extraction_rate': overall_quality,\n        'field_scores': conf_scores,\n        'extraction_reasoning': {'general': 'Extracted via Enhanced Local Pipeline (ExifTool + Advanced NLP + Regex)'},\n        'extracted_fields': len([v for v in mapped_meta.values() if v]),\n        'total_fields': len(mapped_meta),\n        'llm_powered': False\n    }\n\n    print(\n        f\"üìö Fin du pipeline local am√©lior√©, m√©tadonn√©es: {list(mapped_meta.keys())} avec {len([v for v in mapped_meta.values() if v])} valeurs non vides\")\n    return mapped_meta\n\n\ndef call_mistral_with_confidence(text_chunk, document_url=\"\", filename=\"\"):\n    \"\"\"\n    Get extraction + confidence scores from Mistral in one call.\n    La cl√© API est lue depuis la variable d'environnement MISTRAL_API_KEY.\n    \"\"\"\n    api_key = os.getenv(\n        \"MISTRAL_API_KEY\") or \"j2wOKpM86nlZhhlvkXXG7rFd4bhM4PN5\"  # Fallback √† la cl√© hardcod√©e si .env non disponible\n\n    try:\n        url = \"https://api.mistral.ai/v1/chat/completions\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n\n        prompt = f\"\"\"\n        You are an expert document analyzer. Analyze this document and extract metadata with confidence scores.\n\n        DOCUMENT TEXT (first 5000 chars):\n        {text_chunk[:5000]}\n\n        SOURCE URL: {document_url}\n        FILENAME: {filename}\n\n        HINT: Look for version numbers in headers like \"DTC_S006597_PF_R_FAB_17319_EN_15.0\" where 15.0 is the version\n\n        TASK: Return ONLY a JSON object with extracted metadata AND your confidence scores:\n\n        {{\n            \"metadata\": {{\n                \"title\": \"the MAIN document title (not technical headers)\",\n                \"type\": \"guideline|regulation|directive|report|procedure|standard|manufacturer|certificate|authorization|specifications|other\",\n                \"publication_date\": \"exact date (DD Month YYYY format)\",\n                \"version\": \"document version/reference number\",\n                \"source\": \"organization name\",\n                \"context\": \"main domain (pharmaceutical, medical, legal, etc.)\",\n                \"country\": \"country code (EU, US, FR, IE, PL, etc.)\",\n                \"language\": \"language code (en, fr, etc.)\"\n            }},\n            \"confidence_scores\": {{\n                \"title\": 85,\n                \"type\": 90,\n                \"publication_date\": 95,\n                \"version\": 20,\n                \"source\": 90,\n                \"context\": 80,\n                \"country\": 95,\n                \"language\": 98\n            }},\n            \"extraction_reasoning\": {{\n                \"title\": \"Found clear title in document header\",\n                \"type\": \"Document explicitly mentions 'guideline' in title\",\n                \"publication_date\": \"Detected header date indicating publication\",\n                \"version\": \"No clear version number found\",\n                \"source\": \"URL or internal org name used\",\n                \"context\": \"Multiple pharmaceutical terms detected\",\n                \"country\": \"URL domain or regulatory hints used\",\n                \"language\": \"Detected by language model\"\n            }}\n        }}\n\n        INSTRUCTIONS FOR DOCUMENT TYPE DETECTION:\n        - \"manufacturer\": Documents listing manufacturing sites, manufacturer information, production details\n        - \"specifications\": Documents detailing product specifications, technical specifications, drug product specs\n        - \"guideline\": Official guidance documents from regulatory agencies\n        - \"regulation\": Legal regulatory texts, laws, regulations\n        - \"directive\": EU directives and similar regulatory directives  \n        - \"report\": Assessment reports, evaluation reports, inspection reports\n        - \"procedure\": Standard operating procedures, process descriptions\n        - \"standard\": Technical standards, quality standards\n        - \"certificate\": Certificates, authorizations, approvals\n        - \"authorization\": Marketing authorizations, permits, licenses\n        - \"other\": Any other document type\n\n\n        RULES FOR PUBLICATION DATE DETECTION:\n\n        Use the date that appears:\n        - At the top near the agency name (e.g. \"EMA/CHMP/ICH/24235/2006 Corr.2 23 January 2025\" ‚Üí use 23 January 2025)\n        - In the format \"Published: ...\" or \"Publication date: ...\"\n        - Next to \"Official Journal of the European Union\" (this is official)\n\n        DO NOT use:\n        - \"Adopted by CHMP on ‚Ä¶\"\n        - \"Date of coming into effect ‚Ä¶\"\n        - \"Finalization date\"\n        - References to old laws like \"Directive 2008/57/EC of 15 July 2008\"\n\n        Examples:\n        ‚úÖ GOOD ‚Üí \"23 January 2025\" near header  \n        ‚úÖ GOOD ‚Üí \"Published in Official Journal: 2 August 2013\"  \n        ‚ùå BAD ‚Üí \"Directive 2008/123/EC of 24 November 2008\"  \n        ‚ùå BAD ‚Üí \"Date for coming into effect: 26 July 2023\"\n\n        Additional Notes:\n        - Prioritize clarity over completeness\n        - Return no extra text outside JSON\n\n        TYPE DETECTION KEYWORDS:\n        - \"SPECIFICATIONS OF\" ‚Üí type: \"specifications\"\n        - \"MANUFACTURERS\" ‚Üí type: \"manufacturer\"  \n        - \"GUIDELINE\" ‚Üí type: \"guideline\"\n        - \"PROCEDURE\" ‚Üí type: \"procedure\"\n        - Look for explicit type indicators in document titles and headers\n\n        INSTRUCTIONS FOR TITLE EXTRACTION:\n        - Extract the MAIN document title from the body content, not technical headers\n        - Ignore technical codes like \"DTC_S006597_PF_R_FAB_17319_EN_15.0\"\n        - Look for the primary subject/topic in the document body\n        - For manufacturer docs: \"MANUFACTURERS\" + product name + description\n        - Combine logical title elements: \"MANUFACTURERS S 6597 Film-coated tablet...\"\n        - Avoid including addresses, company names, or metadata in the title\n\n        INSTRUCTIONS FOR VERSION DETECTION:\n\n\n        1. If a regulatory code is present like:\n        - EMA/CHMP/ICH/xxxxx/yyyy\n        - ICH/yyyy/stepX\n        ‚Üí extract that full code as version\n\n        2. If \"Corr.2\", \"Rev.1\", \"(R1)\", etc. appears after the code or title ‚Üí append it\n\n        3. If \"Step 5\", \"Step 2b\", etc. appears near the header ‚Üí append it\n\n        4. Accept also formats like:\n        - M4Q(R2) ‚Üí version = \"M4Q(R2)\"\n        - CTD_QR_Rev.2 ‚Üí version = \"Rev.2\"\n        - EN_15.0 ‚Üí version = \"15.0\"\n\n        ‚Äî EXAMPLES ‚Äî\n        ‚úÖ EMA/CHMP/ICH/214732/2007 Step 5\n        ‚úÖ EMA/CHMP/ICH/24235/2006 Corr.2 Step 5\n        ‚úÖ M4Q(R2)\n        ‚úÖ Rev. 1\n        ‚úÖ Version 3.0\n\n        ‚Äî DO NOT EXTRACT ‚Äî\n        ‚ùå Directive 2008/57/EC\n        ‚ùå IDs not linked to versioning\n\n\n        If no clear version is found, return an empty string: `\"\"`\n\n\n        INSTRUCTIONS FOR SOURCE DETECTION:\n        - Look for explicit organization names in the document\n        - Check for regulatory agency indicators (EMA, FDA, ANSM, MHRA, etc.)\n        - For manufacturer documents, use the company name mentioned\n        - For specifications documents, look for regulatory authority context\n        - Check for \"European Pharmacopoeia\" ‚Üí likely EMA/European regulatory context\n        - If from URL domain: .ema.europa.eu = \"EMA\", .fda.gov = \"FDA\", etc.\n        - If unclear, analyze content to determine likely source organization\n        - Do NOT default to EMA unless clearly an EMA document\n\n        INSTRUCTIONS FOR COUNTRY DETECTION:\n        - Look for country-specific indicators in text\n        - Check addresses, phone numbers, regulatory codes\n        - Analyze URL domain for country hints\n        - For manufacturer docs, use country where facilities are located\n        - Use specific country codes (FR, IE, PL) not just EU when possible\n\n        CONFIDENCE SCORING:\n        - 90-100: Explicitly stated in document\n        - 70-89: Strong indicators/evidence  \n        - 50-69: Reasonable inference from context\n        - 30-49: Weak indicators, uncertain\n        - 0-29: Not found or very unclear\n\n        TITLE EXTRACTION EXAMPLES:\n        ‚úÖ GOOD: \"MANUFACTURERS S 6597 Film-coated tablet containing 10 mg of perindopril arginine and 2.5 mg of indapamide\"\n        ‚úÖ GOOD: \"SPECIFICATIONS OF THE DRUG PRODUCT S 20098 Film-coated tablets containing 25 mg of drug substance\"\n        ‚ùå BAD: \"DTC_S006597_PF_R_FAB_17319_EN_15.0 MANUFACTURERS S 6597...\"\n        ‚ùå BAD: Including addresses or company details in title\n\n        Return ONLY the JSON, no other text.\n        \"\"\"\n\n        data = {\n            \"model\": \"mistral-large-latest\",\n            \"messages\": [{\"role\": \"user\",\n                          \"content\": prompt + \"\\n\\nReturn ONLY one JSON object or array. No markdown, no code fences, no explanations, no ellipses in values.\"}],\n            \"temperature\": 0.1,\n            \"max_tokens\": 1200\n        }\n\n        response = requests.post(url, headers=headers, json=data, timeout=30)\n        response.raise_for_status()\n\n        if response.status_code == 200:\n            result = response.json()\n            response_text = result['choices'][0]['message']['content']\n\n            # Try to robustly parse JSON from the response\n            def _sanitize_and_parse(candidate: str):\n                original = candidate\n                # Normalize smart quotes\n                candidate = candidate.replace('\\u201c', '\"').replace('\\u201d', '\"').replace('\\u2019', \"'\")\n                # Remove trailing commas before } or ]\n                candidate = re.sub(r',\\s*([}\\]])', r'\\1', candidate)\n                # Fix single-quoted keys: 'key': value -> \"key\": value\n                candidate = re.sub(r\"(?<=\\{|,)\\s*'([A-Za-z0-9_]+)'\\s*:\\s*\", r'\"\\1\": ', candidate)\n                # Fix single-quoted string values: key: 'value' -> key: \"value\"\n                candidate = re.sub(r\":\\s*'([^'\\\\]*(?:\\\\.[^'\\\\]*)*)'\", r': \"\\1\"', candidate)\n                # Fix unquoted keys only when at object boundaries\n                candidate = re.sub(r'(?<=\\{|,)\\s*([A-Za-z0-9_]+)\\s*:', r'\"\\1\":', candidate)\n                try:\n                    return json.loads(candidate)\n                except Exception:\n                    return None\n\n            def _extract_balanced_block(text: str):\n                # Support either object {..} or array [..]\n                start_obj = text.find('{')\n                start_arr = text.find('[')\n                starts = [s for s in [start_obj, start_arr] if s != -1]\n                if not starts:\n                    return None\n                start = min(starts)\n                opener = text[start]\n                closer = '}' if opener == '{' else ']'\n                i = start\n                depth = 0\n                in_str = False\n                esc = False\n                while i < len(text):\n                    ch = text[i]\n                    if in_str:\n                        if esc:\n                            esc = False\n                        elif ch == '\\\\':\n                            esc = True\n                        elif ch == '\"':\n                            in_str = False\n                    else:\n                        if ch == '\"':\n                            in_str = True\n                        elif ch == opener:\n                            depth += 1\n                        elif ch == closer:\n                            depth -= 1\n                            if depth == 0:\n                                return text[start:i + 1]\n                    i += 1\n                return None\n\n            def _try_parse_json(text: str):\n                # 1) Direct parse\n                try:\n                    return json.loads(text)\n                except Exception:\n                    pass\n                # 2) Fenced code block ```json:disable-run\n                m = re.search(r\"```(?:json|JSON)?\\s*([\\[{][\\s\\S]*?[\\]}])\\s*```\", text)\n                if m:\n                    fragment = m.group(1)\n                    try:\n                        return json.loads(fragment)\n                    except Exception:\n                        parsed = _sanitize_and_parse(fragment)\n                        if parsed is not None:\n                            return parsed\n                # 3) Balanced block extraction (object or array)\n                candidate = _extract_balanced_block(text)\n                if candidate:\n                    try:\n                        return json.loads(candidate)\n                    except Exception:\n                        parsed = _sanitize_and_parse(candidate)\n                        if parsed is not None:\n                            return parsed\n                return None\n\n            full_result = _try_parse_json(response_text)\n            if not full_result:\n                print(\"‚ùå No valid JSON found or parse failed\")\n                return None\n\n            # Add quality metrics to metadata\n            if 'metadata' in full_result and 'confidence_scores' in full_result:\n                confidence_scores = full_result.get('confidence_scores', {})\n                overall_quality = calculate_overall_quality(confidence_scores)\n\n                full_result['metadata']['quality'] = {\n                    'extraction_rate': overall_quality,\n                    'field_scores': confidence_scores,\n                    'extraction_reasoning': full_result.get('extraction_reasoning', {}),\n                    'extracted_fields': len([v for v in full_result['metadata'].values() if v]),\n                    'total_fields': len(full_result['metadata']),\n                    'llm_powered': True\n                }\n\n            print(\"‚úÖ Mistral extraction with confidence successful!\")\n            return full_result\n        else:\n            print(f\"‚ùå Mistral API error: {response.status_code}\")\n            return None\n\n    except Exception as e:\n        print(f\"‚ùå Mistral API error: {e}\")\n        return None\n\n\ndef calculate_overall_quality(confidence_scores):\n    \"\"\"\n    Calculate overall extraction quality from LLM confidence scores\n    \"\"\"\n    if not confidence_scores:\n        return 0\n\n    weights = {\n        'title': 1.5,\n        'type': 1.2,\n        'publication_date': 1.3,\n        'source': 1.2,\n        'context': 1.0,\n        'language': 0.8,\n        'country': 0.8,\n        'version': 0.7\n    }\n\n    total_weighted_score = 0\n    total_weight = 0\n    for field, score in confidence_scores.items():\n        w = weights.get(field, 1.0)\n        total_weighted_score += score * w\n        total_weight += w\n\n    return int(total_weighted_score / total_weight) if total_weight else 0\n\n\ndef extract_tables_from_pdf(file_path):\n    \"\"\"\n    Extrait tous les tableaux du PDF avec pdfplumber\n    \"\"\"\n    tables_data = []\n\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            for page_num, page in enumerate(pdf.pages, 1):\n                # Extraire les tableaux de la page\n                tables = page.extract_tables()\n\n                for table_num, table in enumerate(tables, 1):\n                    if table and len(table) > 1:  # Au moins un header + une ligne\n                        # Nettoyer le tableau (supprimer les cellules vides)\n                        cleaned_table = []\n                        for row in table:\n                            cleaned_row = [cell.strip() if cell else \"\" for cell in row]\n                            if any(cleaned_row):  # Garder seulement les lignes non vides\n                                cleaned_table.append(cleaned_row)\n\n                        if len(cleaned_table) > 1:\n                            tables_data.append({\n                                'page': page_num,\n                                'table_id': f\"table_{page_num}_{table_num}\",\n                                'headers': cleaned_table[0],\n                                'rows': cleaned_table[1:],\n                                'total_rows': len(cleaned_table) - 1,\n                                'total_columns': len(cleaned_table[0]) if cleaned_table else 0\n                            })\n\n    except Exception as e:\n        print(f\"‚ùå Erreur lors de l'extraction des tableaux: {e}\")\n\n    return tables_data\n\n\ndef call_llm_with_learned_prompt(prompt):\n    \"\"\"Call LLM with the adaptive prompt\"\"\"\n    try:\n        api_key = os.getenv(\"GROQ_API_KEY\") or \"your_actual_groq_api_key_here\"\n        client = Groq(api_key=api_key)\n\n        response = client.chat.completions.create(\n            messages=[{\n                \"role\": \"user\",\n                \"content\": prompt + \"\\n\\nReturn ONLY one JSON object or array. No markdown, no code fences, no explanations, no ellipses in values.\"\n            }],\n            model=\"llama3-8b-8192\",\n            temperature=0.1,\n            max_tokens=1500\n        )\n\n        result = response.choices[0].message.content\n\n        # Robust JSON parsing similar to Mistral path\n        def _sanitize_and_parse(candidate: str):\n            # Normalize smart quotes\n            candidate = candidate.replace('\\u201c', '\"').replace('\\u201d', '\"').replace('\\u2019', \"'\")\n            # Remove trailing commas before } or ]\n            candidate = re.sub(r',\\s*([}\\]])', r'\\1', candidate)\n            # Fix single-quoted keys: 'key': value -> \"key\": value\n            candidate = re.sub(r\"(?<=\\{|,)\\s*'([A-Za-z0-9_]+)'\\s*:\\s*\", r'\"\\1\": ', candidate)\n            # Fix single-quoted string values: key: 'value' -> key: \"value\"\n            candidate = re.sub(r\":\\s*'([^'\\\\]*(?:\\\\.[^'\\\\]*)*)'\", r': \"\\1\"', candidate)\n            # Fix unquoted keys only when at object boundaries\n            candidate = re.sub(r'(?<=\\{|,)\\s*([A-Za-z0-9_]+)\\s*:', r'\"\\1\":', candidate)\n            try:\n                return json.loads(candidate)\n            except Exception:\n                return None\n\n        def _extract_balanced_json(text: str):\n            start = text.find('{')\n            if start == -1:\n                return None\n            i = start\n            depth = 0\n            in_str = False\n            esc = False\n            while i < len(text):\n                ch = text[i]\n                if in_str:\n                    if esc:\n                        esc = False\n                    elif ch == '\\\\':\n                        esc = True\n                    elif ch == '\"':\n                        in_str = False\n                else:\n                    if ch == '\"':\n                        in_str = True\n                    elif ch == '{':\n                        depth += 1\n                    elif ch == '}':\n                        depth -= 1\n                        if depth == 0:\n                            return text[start:i + 1]\n                i += 1\n            return None\n\n        def _try_parse_json(text: str):\n            # 1) Direct parse\n            try:\n                return json.loads(text)\n            except Exception:\n                pass\n            # 2) Fenced code block ```json ... ``` or ``` ... ```\n            m = re.search(r\"```(?:json|JSON)?\\s*(\\{[\\s\\S]*?\\})\\s*```\", text)\n            if m:\n                try:\n                    return json.loads(m.group(1))\n                except Exception:\n                    parsed = _sanitize_and_parse(m.group(1))\n                    if parsed is not None:\n                        return parsed\n            # 3) Balanced-brace extraction\n            candidate = _extract_balanced_json(text)\n            if candidate:\n                try:\n                    return json.loads(candidate)\n                except Exception:\n                    parsed = _sanitize_and_parse(candidate)\n                    if parsed is not None:\n                        return parsed\n            return {}\n\n        return _try_parse_json(result)\n\n    except Exception as e:\n        print(f\"LLM call error: {e}\")\n        # Fallback to your existing Mistral extraction\n        return call_mistral_basic_extraction(prompt)\n\n\ndef call_mistral_basic_extraction(prompt):\n    \"\"\"Fallback to basic extraction if Groq fails\"\"\"\n    # Use your existing call_mistral_with_confidence logic here\n    # or return empty dict to use fallback\n    return {}\n\n\ndef extract_images_from_pdf(file_path):\n    \"\"\"\n    Extrait toutes les images du PDF avec PyMuPDF\n    \"\"\"\n    images_data = []\n\n    try:\n        pdf_document = fitz.open(file_path)\n\n        for page_num in range(len(pdf_document)):\n            page = pdf_document[page_num]\n            image_list = page.get_images()\n\n            for img_index, img in enumerate(image_list):\n                try:\n                    # R√©cup√©rer l'image\n                    xref = img[0]\n                    pix = fitz.Pixmap(pdf_document, xref)\n\n                    # Convertir en format utilisable\n                    if pix.n - pix.alpha < 4:  # GRAY ou RGB\n                        img_data = pix.tobytes(\"png\")\n\n                        # Cr√©er un objet PIL pour obtenir les dimensions\n                        pil_image = Image.open(io.BytesIO(img_data))\n                        width, height = pil_image.size\n\n                        # Encoder en base64 pour stockage/affichage\n                        img_base64 = base64.b64encode(img_data).decode()\n\n                        # Cr√©er une version preview (plus petite pour l'affichage dans la liste)\n                        preview_image = pil_image.copy()\n                        preview_image.thumbnail((300, 200), Image.Resampling.LANCZOS)\n                        preview_buffer = io.BytesIO()\n                        preview_image.save(preview_buffer, format='PNG')\n                        preview_base64 = base64.b64encode(preview_buffer.getvalue()).decode()\n\n                        images_data.append({\n                            'page': page_num + 1,\n                            'image_id': f\"img_{page_num + 1}_{img_index + 1}\",\n                            'width': width,\n                            'height': height,\n                            'format': 'PNG',\n                            'size_bytes': len(img_data),\n                            'base64_data': img_base64[:100] + \"...\" if len(img_base64) > 100 else img_base64,\n                            # Tronquer pour l'affichage\n                            'preview_base64': preview_base64,  # Version preview pour l'affichage\n                            'full_base64': img_base64  # Donn√©es compl√®tes\n                        })\n\n                    pix = None  # Lib√©rer la m√©moire\n\n                except Exception as e:\n                    print(f\"‚ùå Erreur lors de l'extraction de l'image {img_index} page {page_num + 1}: {e}\")\n                    continue\n\n        pdf_document.close()\n\n    except Exception as e:\n        print(f\"‚ùå Erreur lors de l'extraction des images: {e}\")\n\n    return images_data\n\n\ndef extract_full_text(file_path):\n    \"\"\"\n    Extraction de texte utilisant directement pdfplumber\n    \"\"\"\n    # Utiliser directement le fallback pdfplumber qui est maintenant la m√©thode principale\n    return _extract_full_text_fallback(file_path)\n\n\ndef _extract_full_text_fallback(file_path):\n    \"\"\"Fallback vers l'ancienne m√©thode pdfplumber\"\"\"\n    text = \"\"\n    with pdfplumber.open(file_path) as pdf:\n        # extraire en priorit√© les 1√®res pages\n        for page in pdf.pages[:2]:\n            page_text = page.extract_text() or \"\"\n            text += \"\\n\" + page_text\n    # ensuite le reste si besoin\n    with pdfplumber.open(file_path) as pdf:\n        for page in pdf.pages[2:]:\n            text += \"\\n\" + (page.extract_text() or \"\")\n    return text\n\n\ndef extract_metadonnees(pdf_path, url=\"\"):\n    \"\"\"Extract metadata with fallback to working system\"\"\"\n\n    if url:\n        print(f\"üì° Utilisation du prompt LLM pour l'extraction des m√©tadonn√©es (URL: {url})\")\n        # First try the learning-enhanced extraction\n        try:\n            # Get learning insights\n            learning_prompts = get_learned_field_improvements()\n\n            # If we have learning data, use adaptive extraction\n            if learning_prompts:\n                text = extract_full_text(pdf_path)\n                if text:\n                    instructions = build_adaptive_field_instructions(learning_prompts)\n                    mistakes_prompt = get_common_mistakes_prompt(learning_prompts)\n\n                    prompt = f\"\"\"Extract metadata from this document:\n        {text[:2000]}\n\n        Fields to extract: {json.dumps(instructions, indent=2)}\n\n        Common mistakes to avoid: {mistakes_prompt}\n\n        Return ONLY JSON with keys: title, type, publication_date, version, source, context, country, language, url_source\"\"\"\n\n                    result = call_llm_with_learned_prompt(prompt)\n                    if result and isinstance(result, dict):\n                        return result\n\n        except Exception as e:\n            print(f\"Learning extraction failed: {e}\")\n\n        # Fallback to your working Mistral extraction\n        try:\n            result = call_mistral_with_confidence(extract_full_text(pdf_path), url, pdf_path)\n            if result and 'metadata' in result:\n                return result['metadata']\n        except Exception as e:\n            print(f\"Mistral extraction failed: {e}\")\n\n        # Final fallback\n        return extract_basic_fallback(pdf_path, url)\n    else:\n        return extract_metadata_local_pipeline(pdf_path)\n\n\ndef get_learned_field_improvements():\n    \"\"\"Get field-specific improvements from feedback data\"\"\"\n    try:\n        from .models import MetadataFeedback\n\n        field_insights = {}\n\n        for feedback in MetadataFeedback.objects.all():\n            corrections = feedback.corrections_made\n\n            # Analyze wrong fields\n            for wrong in corrections.get('corrected_fields', []):\n                field = wrong.get('field')\n                ai_value = wrong.get('ai_value', '')\n                correct_value = wrong.get('human_value', '')\n\n                if field not in field_insights:\n                    field_insights[field] = {'mistakes': [], 'patterns': []}\n\n                field_insights[field]['mistakes'].append({\n                    'wrong': ai_value,\n                    'correct': correct_value\n                })\n\n        return field_insights\n\n    except Exception as e:\n        print(f\"Learning insights error: {e}\")\n        return {}\n\n\ndef build_adaptive_field_instructions(learning_prompts):\n    \"\"\"Build field instructions that incorporate learning\"\"\"\n    instructions = {}\n\n    base_fields = {\n        'title': \"Document title\",\n        'type': \"Document type (guide, report, etc)\",\n        'publication_date': \"Publication date\",\n        'source': \"Source organization\",\n        'language': \"Document language code (en, fr, etc.)\",\n        'context': \"The field of the document content (pharmaceutical, legal, etc.) \",\n    }\n\n    for field, base_desc in base_fields.items():\n        if field in learning_prompts:\n            mistakes = learning_prompts[field]['mistakes']\n            if mistakes:\n                # Add specific guidance based on past mistakes\n                common_errors = [m['wrong'] for m in mistakes[-3:]]  # Last 3 mistakes\n                instructions[field] = f\"{base_desc}. AVOID these patterns: {', '.join(common_errors)}\"\n            else:\n                instructions[field] = base_desc\n        else:\n            instructions[field] = base_desc\n\n    return instructions\n\n\ndef get_common_mistakes_prompt(learning_prompts):\n    \"\"\"Generate prompt section about common mistakes\"\"\"\n    mistakes = []\n\n    for field, data in learning_prompts.items():\n        if data['mistakes']:\n            latest_mistakes = data['mistakes'][-2:]  # Last 2 mistakes\n            for mistake in latest_mistakes:\n                mistakes.append(f\"Don't confuse {field}: '{mistake['wrong']}' should be '{mistake['correct']}'\")\n\n    return \"\\n\".join(mistakes[:5])\n\n\ndef extract_basic_fallback(file_path: str, source_url: str) -> dict:\n    \"\"\"Basic fallback with honest low confidence scores\"\"\"\n    reader = PdfReader(file_path)\n    info = reader.metadata or {}\n    title = info.title or Path(file_path).stem\n    full_text = extract_full_text(file_path)\n\n    try:\n        lang = detect(full_text)\n    except:\n        lang = \"en\"\n\n    basic_meta = {\n        \"title\": title,\n        \"type\": \"unknown\",\n        \"publication_date\": \"\",\n        \"version\": \"\",\n        \"source\": \"\",\n        \"context\": \"\",\n        \"country\": \"\",\n        \"language\": lang,\n        \"url_source\": source_url\n    }\n\n    conf_scores = {\n        'title': 30 if title else 0,\n        'type': 0,\n        'publication_date': 0,\n        'version': 0,\n        'source': 0,\n        'context': 0,\n        'country': 0,\n        'language': 80 if lang else 0\n    }\n\n    overall_quality = calculate_overall_quality(conf_scores)\n\n    basic_meta['quality'] = {\n        'extraction_rate': overall_quality,\n        'field_scores': conf_scores,\n        'extraction_reasoning': {\n            'title': 'Basic PDF metadata extraction',\n            'type': 'Could not determine document type',\n            'source': 'No source identification possible'\n        },\n        'extracted_fields': 1,\n        'total_fields': len(conf_scores),\n        'llm_powered': False\n    }\n    return basic_meta",
                    "language": "python"
                }
            },
            "createdAt": 1760831416547,
            "isInternal": false,
            "rawContent": {
                "type": "doc",
                "content": [
                    {
                        "type": "paragraph",
                        "content": [
                            {
                                "type": "text",
                                "text": "Dans la partie upload pour extraire les metadonn√©es l edition ne marche je veux que l edition pour la bas comme dans la page edit_metatdata mais ca me donne ca quand j essaie de sauvegarder "
                            }
                        ]
                    },
                    {
                        "type": "paragraph"
                    },
                    {
                        "type": "paragraph",
                        "content": [
                            {
                                "type": "text",
                                "text": "Not Found: /rawdocs/save-structured-edits/305/\n\nNot Found: /rawdocs/save-structured-edits/305/\n\n[19/Oct/2025 00:47:40] \"POST /rawdocs/save-structured-edits/305/ HTTP/1.1\" 404 19912\n\n\n\nC'est bon je peux editer mais quand je met sauvegarder on me met erreur reseau lors de la sauvegarde"
                            }
                        ]
                    }
                ]
            },
            "autoAppliedRules": []
        },
        {
            "id": "747cafdd-b13b-4a4a-95cc-a62f1bb2ce0c",
            "role": "assistant",
            "content": [
                {
                    "type": "thought",
                    "text": "**Preparing to view repo rules file**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_HbpAqkbv0QIPELEkfqNCtZbL",
                    "title": "",
                    "statusText": "File not found",
                    "status": "WARNING",
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Identifying missing file and planning investigation**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_3If6bsME6PvHDm77GU9MDMFe",
                    "statusText": "Viewing directory",
                    "status": "IN_PROGRESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_ys3QdBNqWjnZXZCJIdLmhbPW",
                    "title": "File search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_xsCRKcorVFgk7Kl3gyX32M0T",
                    "statusText": "Viewing file",
                    "status": "IN_PROGRESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_NwQJsNPtsryCXVvUjB4TY86n",
                    "title": "File search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_PwAY3C2aQgsTAaeZiI7STLCy",
                    "statusText": "Viewing file",
                    "status": "IN_PROGRESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_Qccuwym6V8MUs69J4QCp1KSO",
                    "statusText": "Viewed file (lines 1:331)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "README.md",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/README.md"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Investigating metadata save 404 error**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_4yKnNNq5bxv885Ug96xR90Fr",
                    "statusText": "Viewed file (lines 1:336)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/urls.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/urls.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_UHsQyEhbcxpIKOZxo4cT1h52",
                    "statusText": "Viewed file (from the beginning up to line 400)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/views.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/views.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_D28vMp5j2suSpeWqf78HA95N",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_ejnIJ9wUztQ4Olbp0mE5mI6R",
                    "statusText": "Viewed file (lines 1800:2059)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/views.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/views.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_aSPbbuxNniGBjrhmbpTy2nnZ",
                    "statusText": "Viewed file (from the beginning up to line 400)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "templates/rawdocs/upload.html",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Preparing JavaScript save handler**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_pG9Y3HneirsXw7YblUAxW4wi",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_GDBOYwRe3N9WRHw8N5s2EzhF",
                    "statusText": "Viewed file (lines 560:679)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "static/js/pdf-upload.js",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_hA2mA8y3NmC0a3IKaV3XA1UU",
                    "statusText": "Viewed file (lines 1:126)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "MyProject/ai_views.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/MyProject/ai_views.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_sbDTsRjjoDLcMqjJTYoY2ZlC",
                    "statusText": "Viewed file (lines 320:519)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "templates/rawdocs/upload.html",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_hoUZFRpwf1oYY2pmm67bFxfV",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_AryKmfh9LEdILTs1I9XwFUZX",
                    "statusText": "Viewed file (from the beginning up to line 200)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/models.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/models.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_D87i9oE57TG379Ai4P27xVzn",
                    "statusText": "Viewed file (lines 1780:1819)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/views.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/views.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "text",
                    "text": "# Analyse du probl√®me\n\nVous pouvez maintenant activer l‚Äô√©dition, mais la sauvegarde renvoie‚ÄØ:\n- 404 `Not Found: /rawdocs/save-structured-edits/305/`\n- puis, une fois corrig√©e, **erreur r√©seau lors de la sauvegarde** (statut ‚â†‚ÄØ200 c√¥t√© fetch).\n\nCes messages indiquent que l‚ÄôURL ou les donn√©es envoy√©es depuis l‚ÄôUI ne correspondent pas aux param√®tres attendus par la vue Django `save_structured_edits`.\n\n# Causes probables1\n\n. **URL g√©n√©r√©e c√¥t√© front**  \n   Le script `static/js/pdf-upload.js` construit l‚ÄôURL en dur¬†:  \n   ```javascript\n   const saveUrl = `/rawdocs/save-structured-edits/${docId}/`\n   ```  \n   Cela suppose que la vue est expos√©e exactement √† ce chemin. Si le reverse proxy, le sous-domaine ou la configuration Django utilise un autre pr√©fixe (par exemple `/app/rawdocs/...`), on obtient un 404. L‚Äô√©dition fonctionne c√¥t√© page `edit_metadata` parce que le template consomme `{% url 'rawdocs:save_structured_edits' doc.id %}` (r√©solution Django).\n\n2. **Corps de requ√™te JSON invalide**  \n   `save_structured_edits` attend un JSON contenant `formatted_content` **ou** une liste `edits`. Le front envoie `formatted_content` mais aussi `extracted_content` (cl√© inattendue)‚ÄØ; ce n‚Äôest pas bloquant mais inutile. En revanche, si `formatted_content` est vide ou tr√®s volumineux (>5‚ÄØMB) la vue renverra une erreur JSON (413 ou 400), affich√©e c√¥t√© front comme ‚ÄúErreur r√©seau‚Äù.\n\n3. **Permissions / CSRF**  \n   La vue est prot√©g√©e (`@login_required` + `@csrf_protect`). Le fetch transmet bien le cookie CSRF via `Utils.getCookie(\"csrftoken\")`, mais un utilisateur non connect√©, ou un token absent/expir√©, produirait une r√©ponse 403 ‚Üí message ‚ÄúErreur r√©seau‚Äù.\n\n# Correctifs recommand√©s\n\n## 1. Utiliser les URLs g√©n√©r√©es par Django\n\nDans `templates/rawdocs/upload.html`, d√©finir l‚Äôattribut `data-save-url` sur le bouton de sauvegarde :\n\n```django\n<button type=\"button\"\n        class=\"control-btn-white save-content-btn\"\n        data-doc-id=\"{{ doc_data.doc.id }}\"\n        data-doc-index=\"{{ forloop.counter0 }}\"\n        data-save-url=\"{% url 'rawdocs:save_structured_edits' doc_data.doc.id %}\">\n  <i class=\"fas fa-save\"></i>\n</button>\n```\n\nDans la section ‚Äúdocument unique‚Äù¬†:\n\n```django\n<button type=\"button\"\n        class=\"btn btn-primary\"\n        id=\"saveContentBtnSingle\"\n        data-doc-id=\"{{ doc.id }}\"\n        data-save-url=\"{% url 'rawdocs:save_structured_edits' doc.id %}\">\n  <i class=\"fas fa-save\"></i> Sauvegarder\n</button>\n```\n\n## 2. C√¥t√© JS, lire cette URL au lieu de la construire\n\nDans `static/js/pdf-upload.js`, adapter `saveContent` :\n\n```javascript\nconst saveBtn = document.querySelector(\n  `.save-content-btn[data-doc-index=\"${docIndex}\"]`\n)\ndocId = saveBtn ? saveBtn.getAttribute(\"data-doc-id\") : null\nconst saveUrl = saveBtn ? saveBtn.getAttribute(\"data-save-url\") : null\n```\n\net pour le cas ‚Äúsingle‚Äù¬†:\n\n```javascript\nconst saveBtn = document.getElementById(\"saveContentBtnSingle\")\ndocId = saveBtn ? saveBtn.getAttribute(\"data-doc-id\") : null\nconst saveUrl = saveBtn ? saveBtn.getAttribute(\"data-save-url\") : null\n```\n\nEnsuite, v√©rifier avant le fetch‚ÄØ:\n\n```javascript\nif (!saveUrl) {\n  Utils.showToast(\"error\", \"URL de sauvegarde introuvable\")\n  return\n}\n```\n\n## 3. Nettoyer la charge utile\n\nEnvoyer uniquement les cl√©s utilis√©es :\n\n```javascript\nbody: JSON.stringify({\n  formatted_content: formatted_content,\n  extraction_score: null\n})\n```\n\nou supprimez `extracted_content`.\n\n## 4. Traiter les r√©ponses non-200\n\nAjouter un check‚ÄØ:\n\n```javascript\nif (!response.ok) {\n  const text = await response.text()\n  throw new Error(`HTTP ${response.status} ‚Äì ${text}`)\n}\n```\n\nCela vous donnera la vraie erreur dans la console (peut-√™tre ‚ÄúJSON invalide‚Äù, ‚ÄúDocument non trouv√©‚Äù, etc.) plut√¥t que ‚ÄúErreur r√©seau‚Äù.\n\n# Plan d‚Äôaction\n\n1. Ajuster les templates pour stocker l‚ÄôURL de sauvegarde via `{% url ... %}`.\n2. Modifier `pdf-upload.js` pour lire cette URL et ajouter un contr√¥le des r√©ponses HTTP.\n3. Retester la sauvegarde.\n   - Si vous √™tes derri√®re un reverse proxy ou un sous-dossier, l‚ÄôURL r√©solue dynamiquement fonctionnera.\n   - Si l‚Äôerreur persiste, la console donnera l‚Äôerreur pr√©cise renvoy√©e par Django, utile pour la suite (taille excessive, permissions‚Ä¶).\n\nBesoin d‚Äôaide pour appliquer les modifications‚ÄØ? Dites-moi sur quelle partie commencer et je vous guiderai √©tape par √©tape."
                }
            ],
            "context": {
                "author": "Code agent",
                "operationId": "dc66c6b2-1bc9-4742-b829-b1007c3f42c7",
                "usageStatistics": {
                    "llmCallsUsedForAnswer": 20.0,
                    "llmCallsUsedTotal": 20.0,
                    "llmCallsLimit": 30.0,
                    "limitRenewalMillisecondTimestamp": 1760906432963
                }
            },
            "createdAt": 1760831422899,
            "isInternal": false
        },
        {
            "id": "c659be5d-775c-46fc-ac8e-74663d946460",
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Commencez par le commencememnt et faites le necessaire pour regler le souci"
                }
            ],
            "context": {
                "currentFile": {
                    "path": "static/css/pdf-upload.css",
                    "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/css/pdf-upload.css",
                    "sources": [
                        "opened_tab"
                    ],
                    "content": ":root {\n  --primary: #0ea5e9;\n  --primary-light: #38bdf8;\n  --primary-hover: #0284c7;\n  --accent: #10b981;\n  --accent-light: #34d399;\n  --text: #000000;\n  --text-secondary: #6b7280;\n  --card-bg: rgba(255, 255, 255, 0.09);\n  --card-border: rgba(14, 165, 233, 0.15);\n  --shadow: 0 8px 32px rgba(14, 165, 233, 0.12);\n  --success-green: #10b981;\n  --danger-red: #ef4444;\n  --glass-bg: rgba(255, 255, 255, 0.06);\n  --text-muted: #6b7280;\n  --primary-dark: #0284c7;\n}\n\n/* Layout */\n.upload-container {\n  max-width: 1400px;\n  margin: 2.5rem auto 0;\n  padding: 3rem 1.5rem;\n}\n\n/* Upload Form */\n.upload-form {\n  background: var(--card-bg);\n  border: 1px solid var(--card-border);\n  box-shadow: var(--shadow);\n  border-radius: 24px;\n  padding: 3rem 2.5rem;\n  margin-bottom: 3rem;\n  backdrop-filter: blur(20px);\n  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);\n}\n\n.upload-form:hover {\n  box-shadow: 0 16px 40px 0 rgba(31, 72, 164, 0.17);\n}\n\n.upload-title {\n  font-size: 1.75rem;\n  font-weight: 800;\n  color: var(--accent);\n  margin-bottom: 2.5rem;\n  display: flex;\n  align-items: center;\n  gap: 1rem;\n  letter-spacing: -0.02em;\n  text-align: center;\n  justify-content: center;\n}\n\n/* Form Elements */\n.form-group {\n  margin-bottom: 1.7rem;\n}\n\n.form-group label {\n  color: var(--text-secondary);\n  font-size: 1rem;\n  font-weight: 500;\n  margin-bottom: 0.4rem;\n  display: block;\n  letter-spacing: 0.01em;\n}\n\n.form-input {\n  width: 100%;\n  padding: 1.25rem 1.5rem;\n  background: rgba(255, 255, 255, 0.08);\n  border: 2px solid rgba(14, 165, 233, 0.15);\n  border-radius: 16px;\n  color: var(--text);\n  font-size: 1.1rem;\n  font-weight: 500;\n  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);\n  backdrop-filter: blur(10px);\n  font-family: inherit;\n}\n\n.form-input:focus {\n  outline: none;\n  border-color: var(--accent);\n  box-shadow: 0 0 0 4px rgba(14, 165, 233, 0.1);\n  background: rgba(255, 255, 255, 0.12);\n  transform: translateY(-1px);\n}\n\n.form-input::placeholder {\n  color: #000000;\n  opacity: 0.6;\n  font-weight: 400;\n}\n\n.form-separator {\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  margin: 1.5rem 0;\n  color: var(--text-secondary);\n  font-weight: 600;\n  letter-spacing: 0.07em;\n  text-transform: uppercase;\n  font-size: 0.95rem;\n}\n\n.form-separator::before,\n.form-separator::after {\n  content: \"\";\n  flex: 1;\n  height: 2px;\n  background: linear-gradient(90deg, transparent, rgba(56, 189, 248, 0.25), transparent);\n}\n\n.form-separator::before {\n  margin-right: 1rem;\n}\n\n.form-separator::after {\n  margin-left: 1rem;\n}\n\n/* Drop Zone */\n.drop-zone {\n  border: 2px dashed rgba(14, 165, 233, 0.3);\n  border-radius: 16px;\n  padding: 3rem 2rem;\n  text-align: center;\n  cursor: pointer;\n  transition: all 0.3s ease;\n  background: rgba(255, 255, 255, 0.03);\n}\n\n.drop-zone:hover,\n.drop-zone.dragover {\n  border-color: var(--accent);\n  background: rgba(14, 165, 233, 0.05);\n  transform: scale(1.01);\n}\n\n.drop-zone .drop-icon {\n  font-size: 3rem;\n  color: var(--accent-light);\n  margin-bottom: 1rem;\n  transition: transform 0.3s ease;\n}\n\n.drop-zone:hover .drop-icon {\n  transform: scale(1.1);\n}\n\n.drop-zone .drop-text {\n  font-size: 1.1rem;\n  font-weight: 600;\n  color: var(--text-secondary);\n  margin-bottom: 0.5rem;\n}\n\n.drop-zone .drop-subtext {\n  font-size: 0.9rem;\n  color: var(--text-muted);\n}\n\n.drop-zone .file-info {\n  display: none;\n  font-size: 0.9rem;\n  color: var(--success-green);\n  font-weight: 600;\n}\n\n.drop-zone.has-file .file-info {\n  display: block;\n}\n\n.drop-zone.has-file .drop-icon,\n.drop-zone.has-file .drop-text,\n.drop-zone.has-file .drop-subtext {\n  display: none;\n}\n\n/* Buttons */\n.btn {\n  display: inline-flex;\n  align-items: center;\n  gap: 0.65rem;\n  padding: 0.8rem 1.7rem;\n  border-radius: 10px;\n  font-weight: 600;\n  font-size: 1rem;\n  border: none;\n  cursor: pointer;\n  transition: all 0.2s ease;\n  text-decoration: none;\n}\n\n.btn-primary {\n  background: linear-gradient(135deg, var(--accent), var(--primary-light));\n  color: #fff;\n  box-shadow: 0 8px 25px rgba(14, 165, 233, 0.16);\n  font-size: 1.1rem;\n  padding: 1.1rem 2.2rem;\n}\n\n.btn-primary:hover {\n  background: linear-gradient(135deg, var(--primary-light), var(--accent));\n  transform: translateY(-3px) scale(1.02);\n  box-shadow: 0 15px 40px rgba(14, 165, 233, 0.25);\n}\n\n.btn-outline {\n  background: transparent;\n  color: var(--accent);\n  border: 1.5px solid var(--accent-light);\n  padding: 0.55rem 1.15rem;\n  font-size: 0.97rem;\n}\n\n.btn-outline:hover {\n  background: var(--accent-light);\n  color: #fff;\n  box-shadow: 0 6px 24px 0 rgba(56, 189, 248, 0.13);\n  transform: translateY(-1px) scale(1.04);\n}\n\n.btn-refuse {\n  background: linear-gradient(135deg, #ef4444, #dc2626);\n  color: white;\n}\n\n.btn-refuse:hover {\n  background: linear-gradient(135deg, #dc2626, #b91c1c);\n  transform: translateY(-2px);\n  box-shadow: 0 8px 25px rgba(239, 68, 68, 0.3);\n}\n\n.btn-view {\n  background: linear-gradient(135deg, #10b981, #059669);\n  color: white;\n}\n\n.btn-view:hover {\n  background: linear-gradient(135deg, #059669, #047857);\n  transform: translateY(-2px);\n  box-shadow: 0 8px 25px rgba(16, 185, 129, 0.3);\n}\n\n/* Cards */\n.card {\n  background: var(--card-bg);\n  border: 1.5px solid var(--card-border);\n  border-radius: 18px;\n  padding: 2rem;\n  margin-bottom: 2rem;\n  box-shadow: var(--shadow);\n  backdrop-filter: blur(8px);\n  transition: box-shadow 0.3s;\n}\n\n.card:hover {\n  box-shadow: 0 16px 40px 0 rgba(31, 72, 164, 0.16);\n}\n\n.card-title {\n  font-size: 1.25rem;\n  font-weight: 700;\n  color: var(--primary-dark);\n  margin-bottom: 1.5rem;\n  display: flex;\n  align-items: center;\n  gap: 0.8rem;\n}\n\n.card-header {\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n  border-bottom: 1.5px solid var(--card-border);\n  padding-bottom: 1rem;\n  margin-bottom: 2rem;\n}\n\n.card-actions {\n  display: flex;\n  gap: 0.7rem;\n  flex-wrap: wrap;\n}\n\n/* Form Grid */\n.form-grid {\n  display: grid;\n  grid-template-columns: repeat(2, 1fr);\n  gap: 1.5rem;\n  margin-bottom: 2rem;\n}\n\n.form-field {\n  display: flex;\n  flex-direction: column;\n}\n\n.form-field.full-width {\n  grid-column: span 2;\n}\n\n.field-label {\n  color: var(--text-secondary);\n  font-size: 0.95rem;\n  font-weight: 600;\n  margin-bottom: 0.5rem;\n  display: flex;\n  align-items: center;\n  gap: 0.5rem;\n}\n\n.auto-badge {\n  background: linear-gradient(135deg, var(--accent), var(--primary-light));\n  color: white;\n  font-size: 0.7rem;\n  padding: 0.2rem 0.6rem;\n  border-radius: 6px;\n  font-weight: 600;\n  text-transform: uppercase;\n  letter-spacing: 0.05em;\n}\n\n/* Split View */\n.split-view-container {\n  display: flex;\n  gap: 1.5rem;\n  margin-top: 0;\n}\n\n.split-panel {\n  flex: 1;\n  min-width: 0;\n}\n\n.split-panel.full {\n  flex: 1 1 100%;\n}\n\n.split-panel.hidden {\n  display: none;\n}\n\n.panel-content {\n  background: white;\n  border: 1px solid #e5e7eb;\n  border-radius: 12px;\n  overflow: hidden;\n  box-shadow: 0 4px 16px rgba(0, 0, 0, 0.08);\n  min-height: 600px;\n  max-height: 800px;\n  display: flex;\n  flex-direction: column;\n}\n\n/* Updated panel header with gradient backgrounds */\n.panel-header {\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n  padding: 1rem 1.5rem;\n  margin-bottom: 0;\n  border-bottom: none;\n}\n\n.panel-header.gradient-blue {\n  background: linear-gradient(135deg, #5b8def 0%, #8b7fe8 100%);\n}\n\n.panel-header.gradient-red {\n  background: linear-gradient(135deg, #e74c3c 0%, #ff6b4a 100%);\n}\n\n.panel-title {\n  font-size: 1.1rem;\n  font-weight: 700;\n  color: white;\n  display: flex;\n  align-items: center;\n  gap: 0.5rem;\n  margin: 0;\n}\n\n.panel-controls {\n  display: flex;\n  gap: 0.5rem;\n}\n\n/* New white button style for gradient headers */\n.control-btn-white {\n  background: rgba(255, 255, 255, 0.2);\n  border: 1px solid rgba(255, 255, 255, 0.3);\n  color: white;\n  padding: 0.5rem 0.75rem;\n  border-radius: 6px;\n  cursor: pointer;\n  transition: all 0.2s;\n  font-size: 0.9rem;\n  backdrop-filter: blur(10px);\n}\n\n.control-btn-white:hover {\n  background: rgba(255, 255, 255, 0.3);\n  border-color: rgba(255, 255, 255, 0.5);\n  transform: translateY(-1px);\n}\n\n.control-btn {\n  background: transparent;\n  border: 1px solid var(--card-border);\n  color: var(--text-secondary);\n  padding: 0.4rem 0.8rem;\n  border-radius: 8px;\n  cursor: pointer;\n  transition: all 0.2s;\n  font-size: 0.9rem;\n}\n\n.control-btn:hover {\n  background: var(--accent-light);\n  color: white;\n  border-color: var(--accent-light);\n}\n\n/* Updated structured content area */\n.structured-content {\n  padding: 1.5rem;\n  overflow: auto;\n  flex: 1;\n  background: white;\n}\n\n/* PDF Viewer */\n.pdf-viewer-container {\n  width: 100%;\n  height: 100%;\n  min-height: 550px;\n  position: relative;\n  background: #f9fafb;\n}\n\n.pdf-viewer {\n  width: 100%;\n  height: 100%;\n  min-height: 550px;\n  border: none;\n  background: white;\n}\n\n/* History Sidebar */\n.history-sidebar {\n  flex: 0 0 320px;\n  min-width: 320px;\n}\n\n.history-entry {\n  background: rgba(255, 255, 255, 0.05);\n  border: 1px solid var(--card-border);\n  border-radius: 12px;\n  padding: 1rem;\n  margin-bottom: 1rem;\n  transition: all 0.2s;\n}\n\n.history-entry:hover {\n  background: rgba(255, 255, 255, 0.08);\n  transform: translateX(-2px);\n}\n\n.history-field {\n  font-weight: 600;\n  color: var(--primary-dark);\n  margin-bottom: 0.3rem;\n}\n\n.history-date {\n  font-size: 0.85rem;\n  color: var(--text-muted);\n  margin-bottom: 0.5rem;\n}\n\n.history-values {\n  font-size: 0.9rem;\n  color: var(--text-secondary);\n  margin-bottom: 0.3rem;\n  word-break: break-word;\n}\n\n.history-user {\n  font-size: 0.85rem;\n  color: var(--text-muted);\n  display: flex;\n  align-items: center;\n  gap: 0.3rem;\n}\n\n/* Alerts */\n.alert {\n  padding: 1rem 1.5rem;\n  border-radius: 12px;\n  margin-bottom: 1rem;\n  display: flex;\n  align-items: center;\n  gap: 0.8rem;\n  font-weight: 500;\n}\n\n.alert-success {\n  background-color: #d4edda;\n  color: #155724;\n  border: 1px solid #c3e6cb;\n}\n\n.alert-error {\n  background-color: #f8d7da;\n  color: #721c24;\n  border: 1px solid #f5c6cb;\n}\n\n.alert-info {\n  background-color: #d1ecf1;\n  color: #0c5460;\n  border: 1px solid #bee5eb;\n}\n\n/* Modal */\n.modal-overlay {\n  position: fixed;\n  inset: 0;\n  background: rgba(0, 0, 0, 0.55);\n  backdrop-filter: blur(2px);\n  z-index: 1000;\n  display: none;\n}\n\n.modal-overlay.active {\n  display: block;\n}\n\n.modal {\n  position: fixed;\n  top: 50%;\n  left: 50%;\n  transform: translate(-50%, -50%);\n  background: #ffffff;\n  border-radius: 14px;\n  border: 1px solid rgba(0, 0, 0, 0.08);\n  padding: 2rem;\n  box-shadow: 0 20px 60px rgba(0, 0, 0, 0.25);\n  max-width: 520px;\n  width: calc(100% - 2rem);\n  text-align: center;\n  color: #111827;\n  z-index: 1001;\n}\n\n.modal-icon {\n  font-size: 2.5rem;\n  margin-bottom: 1.2rem;\n}\n\n.modal-icon.warning {\n  color: #ef4444;\n}\n\n.modal-title {\n  color: #0f172a;\n  font-weight: 800;\n  margin-bottom: 0.75rem;\n  font-size: 1.25rem;\n}\n\n.modal-text {\n  color: #334155;\n  font-size: 1.05rem;\n  margin-bottom: 1.5rem;\n}\n\n.modal-actions {\n  display: flex;\n  justify-content: center;\n  gap: 1rem;\n}\n\n/* Responsive */\n@media (max-width: 1024px) {\n  .split-view-container {\n    flex-direction: column;\n  }\n\n  .history-sidebar {\n    flex: 1;\n    min-width: 100%;\n  }\n\n  .form-grid {\n    grid-template-columns: 1fr;\n  }\n\n  .form-field.full-width {\n    grid-column: span 1;\n  }\n}\n\n@media (max-width: 768px) {\n  .upload-container {\n    padding: 1rem;\n  }\n\n  .upload-form {\n    padding: 1.5rem;\n  }\n\n  .card {\n    padding: 1.5rem;\n  }\n\n  .card-header {\n    flex-direction: column;\n    align-items: flex-start;\n    gap: 1rem;\n  }\n\n  .card-actions {\n    width: 100%;\n  }\n\n  .btn {\n    font-size: 0.9rem;\n    padding: 0.7rem 1.2rem;\n  }\n\n  .panel-content {\n    min-height: 400px;\n  }\n}\n\n/* Added tab navigation styles */\n.view-tabs {\n  display: flex;\n  gap: 0.5rem;\n  margin-bottom: 1.5rem;\n  padding: 0.5rem;\n  background: rgba(255, 255, 255, 0.03);\n  border-radius: 12px;\n  border: 1px solid var(--card-border);\n}\n\n.tab-btn {\n  flex: 1;\n  padding: 0.75rem 1.5rem;\n  background: transparent;\n  border: none;\n  border-radius: 8px;\n  color: var(--text-secondary);\n  font-weight: 600;\n  font-size: 0.95rem;\n  cursor: pointer;\n  transition: all 0.2s ease;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  gap: 0.5rem;\n}\n\n.tab-btn:hover {\n  background: rgba(14, 165, 233, 0.1);\n  color: var(--primary);\n}\n\n.tab-btn.active {\n  background: linear-gradient(135deg, #3b82f6, #2563eb);\n  color: white;\n  box-shadow: 0 4px 16px rgba(59, 130, 246, 0.3);\n  transform: translateY(-2px);\n}\n\n/* Horizontal Document Selector Carousel */\n.doc-selector-carousel {\n  position: relative;\n  display: flex;\n  align-items: center;\n  gap: 1rem;\n  margin-bottom: 2rem;\n  padding: 1rem 0;\n}\n\n.carousel-nav {\n  flex: 0 0 48px;\n  width: 48px;\n  height: 48px;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  background: linear-gradient(135deg, var(--accent), var(--primary-light));\n  color: white;\n  border: none;\n  border-radius: 12px;\n  cursor: pointer;\n  transition: all 0.3s ease;\n  font-size: 1.2rem;\n  box-shadow: 0 4px 12px rgba(14, 165, 233, 0.2);\n  z-index: 10;\n}\n\n.carousel-nav:hover:not(:disabled) {\n  transform: scale(1.1);\n  box-shadow: 0 6px 20px rgba(14, 165, 233, 0.35);\n}\n\n.carousel-nav:disabled {\n  opacity: 0.3;\n  cursor: not-allowed;\n  transform: none;\n}\n\n.carousel-container {\n  flex: 1;\n  overflow: hidden;\n  position: relative;\n}\n\n.carousel-track {\n  display: flex;\n  gap: 1rem;\n  overflow-x: auto;\n  scroll-behavior: smooth;\n  padding: 0.5rem 0;\n  scrollbar-width: thin;\n  scrollbar-color: rgba(91, 141, 239, 0.5) rgba(255, 255, 255, 0.1);\n}\n\n.carousel-track::-webkit-scrollbar {\n  height: 8px;\n}\n\n.carousel-track::-webkit-scrollbar-track {\n  background: rgba(255, 255, 255, 0.05);\n  border-radius: 4px;\n}\n\n.carousel-track::-webkit-scrollbar-thumb {\n  background: linear-gradient(90deg, #5b8def, #8b7fe8);\n  border-radius: 4px;\n}\n\n.carousel-track::-webkit-scrollbar-thumb:hover {\n  background: linear-gradient(90deg, #4a7de0, #7a6ed7);\n}\n\n/* Document Selector Card */\n.doc-selector-card {\n  flex: 0 0 280px;\n  min-width: 280px;\n  background: var(--card-bg);\n  border: 2px solid var(--card-border);\n  border-radius: 16px;\n  padding: 1.5rem;\n  cursor: pointer;\n  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);\n  position: relative;\n  backdrop-filter: blur(10px);\n  box-shadow: 0 4px 16px rgba(14, 165, 233, 0.08);\n}\n\n.doc-selector-card:hover {\n  transform: translateY(-4px);\n  border-color: var(--primary-light);\n  box-shadow: 0 8px 24px rgba(14, 165, 233, 0.2);\n  background: rgba(14, 165, 233, 0.05);\n}\n\n.doc-selector-card.active {\n  background: linear-gradient(135deg, rgba(91, 141, 239, 0.15), rgba(139, 127, 232, 0.15));\n  border-color: #5b8def;\n  border-width: 3px;\n  box-shadow: 0 8px 32px rgba(91, 141, 239, 0.3);\n  transform: translateY(-4px) scale(1.02);\n}\n\n.selector-card-number {\n  position: absolute;\n  top: 12px;\n  right: 12px;\n  width: 32px;\n  height: 32px;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  background: linear-gradient(135deg, #5b8def, #8b7fe8);\n  color: white;\n  font-size: 0.9rem;\n  font-weight: 800;\n  border-radius: 8px;\n  box-shadow: 0 2px 8px rgba(91, 141, 239, 0.3);\n}\n\n.selector-card-icon {\n  width: 60px;\n  height: 60px;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  background: linear-gradient(135deg, #e74c3c, #ff6b4a);\n  border-radius: 12px;\n  color: white;\n  font-size: 2rem;\n  margin-bottom: 1rem;\n  box-shadow: 0 4px 12px rgba(231, 76, 60, 0.3);\n  transition: all 0.3s ease;\n}\n\n.doc-selector-card.active .selector-card-icon {\n  background: linear-gradient(135deg, #5b8def, #8b7fe8);\n  box-shadow: 0 4px 12px rgba(91, 141, 239, 0.4);\n  transform: scale(1.05);\n}\n\n.selector-card-content {\n  display: flex;\n  flex-direction: column;\n  gap: 0.75rem;\n}\n\n.selector-card-title {\n  font-size: 1rem;\n  font-weight: 700;\n  color: var(--text);\n  line-height: 1.4;\n  overflow: hidden;\n  text-overflow: ellipsis;\n  display: -webkit-box;\n  -webkit-line-clamp: 2;\n  -webkit-box-orient: vertical;\n  min-height: 2.8em;\n  transition: color 0.2s ease;\n}\n\n.doc-selector-card:hover .selector-card-title {\n  color: var(--primary);\n}\n\n.doc-selector-card.active .selector-card-title {\n  color: #5b8def;\n  font-weight: 800;\n}\n\n.selector-card-meta {\n  display: flex;\n  gap: 0.5rem;\n  flex-wrap: wrap;\n}\n\n.meta-tag {\n  display: inline-flex;\n  align-items: center;\n  gap: 0.25rem;\n  font-size: 0.75rem;\n  font-weight: 600;\n  color: var(--text-secondary);\n  background: rgba(14, 165, 233, 0.1);\n  padding: 0.3rem 0.6rem;\n  border-radius: 6px;\n  border: 1px solid rgba(14, 165, 233, 0.2);\n}\n\n.selector-card-status {\n  display: flex;\n  align-items: center;\n  gap: 0.5rem;\n}\n\n.status-dot {\n  width: 12px;\n  height: 12px;\n  border-radius: 50%;\n  box-shadow: 0 0 8px currentColor;\n  animation: pulse 2s ease-in-out infinite;\n}\n\n.status-dot.validated {\n  background: #10b981;\n  color: #10b981;\n}\n\n.status-dot.pending {\n  background: #f59e0b;\n  color: #f59e0b;\n}\n\n@keyframes pulse {\n  0%,\n  100% {\n    opacity: 1;\n  }\n  50% {\n    opacity: 0.6;\n  }\n}\n\n/* Full-width document viewer */\n.doc-viewer-full-width {\n  width: 100%;\n}\n\n/* Responsive improvements for horizontal carousel */\n@media (max-width: 1024px) {\n  .doc-selector-card {\n    flex: 0 0 240px;\n    min-width: 240px;\n    padding: 1.25rem;\n  }\n\n  .selector-card-icon {\n    width: 50px;\n    height: 50px;\n    font-size: 1.75rem;\n  }\n\n  .selector-card-number {\n    width: 28px;\n    height: 28px;\n    font-size: 0.8rem;\n  }\n}\n\n@media (max-width: 768px) {\n  .doc-selector-carousel {\n    flex-direction: column;\n    gap: 0.75rem;\n  }\n\n  .carousel-nav {\n    display: none;\n  }\n\n  .carousel-container {\n    width: 100%;\n  }\n\n  .doc-selector-card {\n    flex: 0 0 220px;\n    min-width: 220px;\n  }\n\n  .multi-doc-header-improved {\n    flex-direction: column;\n    gap: 1rem;\n    padding: 1.25rem;\n  }\n\n  .header-left {\n    flex-direction: column;\n    align-items: flex-start;\n    width: 100%;\n  }\n\n  .header-right {\n    width: 100%;\n    justify-content: space-between;\n  }\n}\n\n/* Clean multi-document interface styles */\n.multi-document-container-clean {\n  margin-top: 2rem;\n}\n\n/* Browser-style document tabs */\n.document-tabs-bar {\n  display: flex;\n  align-items: center;\n  gap: 1rem;\n  background: var(--card-bg);\n  border: 1px solid var(--card-border);\n  border-radius: 16px 16px 0 0;\n  padding: 0.75rem 1rem;\n  backdrop-filter: blur(10px);\n  box-shadow: 0 2px 8px rgba(14, 165, 233, 0.08);\n}\n\n.tabs-wrapper {\n  flex: 1;\n  display: flex;\n  gap: 0.5rem;\n  overflow-x: auto;\n  scrollbar-width: thin;\n  scrollbar-color: rgba(14, 165, 233, 0.3) transparent;\n}\n\n.tabs-wrapper::-webkit-scrollbar {\n  height: 6px;\n}\n\n.tabs-wrapper::-webkit-scrollbar-thumb {\n  background: rgba(14, 165, 233, 0.3);\n  border-radius: 3px;\n}\n\n.document-tab {\n  display: flex;\n  align-items: center;\n  gap: 0.75rem;\n  padding: 0.75rem 1.25rem;\n  background: rgba(255, 255, 255, 0.03);\n  border: 1px solid rgba(14, 165, 233, 0.15);\n  border-radius: 10px;\n  cursor: pointer;\n  transition: all 0.2s ease;\n  min-width: 200px;\n  max-width: 280px;\n  position: relative;\n}\n\n.document-tab:hover {\n  background: rgba(14, 165, 233, 0.08);\n  border-color: var(--primary-light);\n  transform: translateY(-2px);\n}\n\n.document-tab.active {\n  background: linear-gradient(135deg, rgba(91, 141, 239, 0.2), rgba(139, 127, 232, 0.2));\n  border-color: #5b8def;\n  border-width: 2px;\n  box-shadow: 0 4px 16px rgba(91, 141, 239, 0.25);\n}\n\n.tab-number {\n  flex: 0 0 32px;\n  width: 32px;\n  height: 32px;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  background: linear-gradient(135deg, var(--accent), var(--primary-light));\n  color: white;\n  font-size: 0.9rem;\n  font-weight: 800;\n  border-radius: 8px;\n  box-shadow: 0 2px 8px rgba(14, 165, 233, 0.3);\n}\n\n.document-tab.active .tab-number {\n  background: linear-gradient(135deg, #5b8def, #8b7fe8);\n}\n\n.tab-info {\n  flex: 1;\n  display: flex;\n  flex-direction: column;\n  gap: 0.25rem;\n  min-width: 0;\n}\n\n.tab-title {\n  font-size: 0.95rem;\n  font-weight: 700;\n  color: var(--text);\n  white-space: nowrap;\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n\n.document-tab.active .tab-title {\n  color: #5b8def;\n}\n\n.tab-subtitle {\n  font-size: 0.75rem;\n  font-weight: 500;\n  color: var(--text-secondary);\n  white-space: nowrap;\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n\n.tab-status {\n  flex: 0 0 10px;\n  width: 10px;\n  height: 10px;\n  border-radius: 50%;\n  box-shadow: 0 0 8px currentColor;\n}\n\n.tab-status.validated {\n  background: #10b981;\n  color: #10b981;\n}\n\n.tab-status.pending {\n  background: #f59e0b;\n  color: #f59e0b;\n}\n\n.tabs-actions {\n  display: flex;\n  align-items: center;\n  gap: 0.75rem;\n}\n\n.doc-counter {\n  font-size: 0.9rem;\n  font-weight: 600;\n  color: var(--text-secondary);\n  padding: 0.5rem 1rem;\n  background: rgba(14, 165, 233, 0.1);\n  border-radius: 8px;\n}\n\n.nav-btn {\n  width: 36px;\n  height: 36px;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  background: linear-gradient(135deg, var(--accent), var(--primary-light));\n  color: white;\n  border: none;\n  border-radius: 8px;\n  cursor: pointer;\n  transition: all 0.2s ease;\n  font-size: 0.9rem;\n}\n\n.nav-btn:hover:not(:disabled) {\n  transform: scale(1.1);\n  box-shadow: 0 4px 12px rgba(14, 165, 233, 0.3);\n}\n\n.nav-btn:disabled {\n  opacity: 0.3;\n  cursor: not-allowed;\n}\n\n/* Document content area */\n.document-content {\n  background: var(--card-bg);\n  border: 1px solid var(--card-border);\n  border-top: none;\n  border-radius: 0 0 16px 16px;\n  padding: 1.5rem;\n  backdrop-filter: blur(10px);\n  box-shadow: 0 4px 16px rgba(14, 165, 233, 0.08);\n  margin-bottom: 2rem;\n}\n\n.document-content.hidden {\n  display: none;\n}\n\n/* Clean view tabs */\n.view-tabs-clean {\n  display: flex;\n  gap: 0.5rem;\n  margin-bottom: 1.5rem;\n  padding: 0.5rem;\n  background: rgba(255, 255, 255, 0.03);\n  border-radius: 12px;\n  border: 1px solid var(--card-border);\n}\n\n.view-tab {\n  flex: 1;\n  padding: 0.75rem 1.5rem;\n  background: transparent;\n  border: none;\n  border-radius: 8px;\n  color: var(--text-secondary);\n  font-weight: 600;\n  font-size: 0.95rem;\n  cursor: pointer;\n  transition: all 0.2s ease;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  gap: 0.5rem;\n}\n\n.view-tab:hover {\n  background: rgba(14, 165, 233, 0.1);\n  color: var(--primary);\n}\n\n.view-tab.active {\n  background: linear-gradient(135deg, #3b82f6, #2563eb);\n  color: white;\n  box-shadow: 0 4px 16px rgba(59, 130, 246, 0.3);\n}\n\n.view-content {\n  display: none;\n}\n\n.view-content.active {\n  display: block;\n}\n\n/* Responsive adjustments */\n@media (max-width: 1024px) {\n  .document-tabs-bar {\n    flex-direction: column;\n    align-items: stretch;\n  }\n\n  .tabs-wrapper {\n    order: 2;\n  }\n\n  .tabs-actions {\n    order: 1;\n    justify-content: space-between;\n    width: 100%;\n    padding-bottom: 0.75rem;\n    border-bottom: 1px solid var(--card-border);\n    margin-bottom: 0.75rem;\n  }\n\n  .document-tab {\n    min-width: 180px;\n  }\n}\n\n@media (max-width: 768px) {\n  .document-tab {\n    min-width: 160px;\n    padding: 0.65rem 1rem;\n  }\n\n  .tab-number {\n    width: 28px;\n    height: 28px;\n    font-size: 0.8rem;\n  }\n\n  .tab-title {\n    font-size: 0.85rem;\n  }\n\n  .tab-subtitle {\n    font-size: 0.7rem;\n  }\n\n  .view-tabs-clean {\n    flex-direction: column;\n  }\n\n  .view-tab {\n    justify-content: flex-start;\n  }\n}\n\n/* Content Editor Styles */\n.zoom-container-single,\n.zoom-container-multi {\n  transition: transform 0.3s ease;\n  transform-origin: top left;\n}\n\n/* Edit mode styles */\n[contenteditable=\"true\"] {\n  outline: 2px dashed rgba(14, 165, 233, 0.3);\n  outline-offset: 2px;\n  background: rgba(14, 165, 233, 0.02);\n  transition: all 0.3s ease;\n  cursor: text;\n  min-height: 1.5em;\n  padding: 0.25rem;\n  border-radius: 4px;\n}\n\n[contenteditable=\"true\"]:hover {\n  outline-color: rgba(14, 165, 233, 0.5);\n  background: rgba(14, 165, 233, 0.05);\n}\n\n[contenteditable=\"true\"]:focus {\n  outline: 2px solid var(--primary);\n  background: rgba(14, 165, 233, 0.08);\n  box-shadow: 0 0 0 4px rgba(14, 165, 233, 0.1);\n}\n\n/* Save button animation when visible */\n.save-content-btn {\n  animation: slideIn 0.3s ease;\n}\n\n@keyframes slideIn {\n  from {\n    opacity: 0;\n    transform: translateX(-10px);\n  }\n  to {\n    opacity: 1;\n    transform: translateX(0);\n  }\n}\n\n/* Toast notification styles */\n#toast {\n  position: fixed;\n  top: 20px;\n  right: 20px;\n  padding: 1rem 1.5rem;\n  border-radius: 12px;\n  font-weight: 600;\n  z-index: 9999;\n  display: none;\n  box-shadow: 0 8px 24px rgba(0, 0, 0, 0.15);\n  animation: slideInRight 0.3s ease;\n}\n\n@keyframes slideInRight {\n  from {\n    opacity: 0;\n    transform: translateX(100px);\n  }\n  to {\n    opacity: 1;\n    transform: translateX(0);\n  }\n}\n\n/* Edit mode indicator */\n.edit-mode-indicator {\n  position: fixed;\n  bottom: 20px;\n  left: 20px;\n  background: linear-gradient(135deg, #f59e0b, #fb923c);\n  color: white;\n  padding: 0.75rem 1.25rem;\n  border-radius: 12px;\n  font-weight: 600;\n  font-size: 0.9rem;\n  box-shadow: 0 4px 16px rgba(245, 158, 11, 0.3);\n  z-index: 100;\n  display: flex;\n  align-items: center;\n  gap: 0.5rem;\n  animation: pulse 2s ease-in-out infinite;\n}\n\n@keyframes pulse {\n  0%, 100% {\n    transform: scale(1);\n  }\n  50% {\n    transform: scale(1.05);\n  }\n}\n\n/* Editable content highlighting */\n.structured-content [contenteditable=\"true\"] p,\n.structured-content [contenteditable=\"true\"] h1,\n.structured-content [contenteditable=\"true\"] h2,\n.structured-content [contenteditable=\"true\"] h3,\n.structured-content [contenteditable=\"true\"] h4,\n.structured-content [contenteditable=\"true\"] h5,\n.structured-content [contenteditable=\"true\"] h6,\n.structured-content [contenteditable=\"true\"] li,\n.structured-content [contenteditable=\"true\"] td,\n.structured-content [contenteditable=\"true\"] th {\n  position: relative;\n}\n\n/* Smooth transitions for all interactive elements */\n.control-btn-white,\n.btn,\n.tab-btn,\n.view-tab,\n.document-tab {\n  transition: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);\n}\n\n/* Loading state for save button */\n.save-content-btn.loading {\n  pointer-events: none;\n  opacity: 0.6;\n}\n\n.save-content-btn.loading::after {\n  content: '';\n  position: absolute;\n  width: 16px;\n  height: 16px;\n  border: 2px solid rgba(255, 255, 255, 0.3);\n  border-top-color: white;\n  border-radius: 50%;\n  animation: spin 0.6s linear infinite;\n}\n\n@keyframes spin {\n  to {\n    transform: rotate(360deg);\n  }\n}\n",
                    "language": "css"
                }
            },
            "createdAt": 1760831637447,
            "isInternal": false,
            "rawContent": {
                "type": "doc",
                "content": [
                    {
                        "type": "paragraph",
                        "content": [
                            {
                                "type": "text",
                                "text": "Commencez par le commencememnt et faites le necessaire pour regler le souci "
                            }
                        ]
                    }
                ]
            },
            "autoAppliedRules": []
        },
        {
            "id": "cdf8a917-9ce1-4286-bff0-d67c9aaa2612",
            "role": "assistant",
            "content": [
                {
                    "type": "thought",
                    "text": "**Preparing to check repository rules file**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_JGcIxl4AhyvfunwEzts6d28c",
                    "title": "",
                    "statusText": "File not found",
                    "status": "WARNING",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_SsJgmxpzWQnwyvLSkG5eUhGu",
                    "statusText": "Viewed file (lines 1860:2079)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/views.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/views.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_XocarpPDVOpoGLNSKtEtO6m9",
                    "statusText": "Viewed file (lines 190:229)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/urls.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/urls.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_DqiRG30vBsE6QmEjVZuv1kMQ",
                    "statusText": "Viewed file (lines 360:579)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "templates/rawdocs/upload.html",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Considering multi-case handling**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_seySrch4NEOZanoWgWS4UYCh",
                    "statusText": "Viewed file (lines 120:339)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "templates/rawdocs/upload.html",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_Kl5699WILkSbsMtMKf2tt22g",
                    "statusText": "Viewed file (lines 580:679)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "static/js/pdf-upload.js",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Planning template and JS enhancements**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_HRRtfYwBFdFsrrsfRsdXd4zG",
                    "statusText": "Change generated",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "templates/rawdocs/upload.html",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "codeHunk",
                    "lineBasedDiff": "--- C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html\n+++ C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html\n@@ -236,7 +236,7 @@\n                     <button type=\"button\" class=\"control-btn-white toggle-edit-btn\" data-doc-index=\"{{ forloop.counter0 }}\" title=\"Mode √©dition\">\n                       <i class=\"fas fa-edit\"></i>\n                     </button>\n-                    <button type=\"button\" class=\"control-btn-white save-content-btn\" data-doc-id=\"{{ doc_data.doc.id }}\" data-doc-index=\"{{ forloop.counter0 }}\" title=\"Sauvegarder\" style=\"display:none;\">\n+                    <button type=\"button\" class=\"control-btn-white save-content-btn\" data-doc-id=\"{{ doc_data.doc.id }}\" data-doc-index=\"{{ forloop.counter0 }}\" data-save-url=\"{% url 'rawdocs:save_structured_edits' doc_data.doc.id %}\" title=\"Sauvegarder\" style=\"display:none;\">\n                       <i class=\"fas fa-save\"></i>\n                     </button>\n                     <button type=\"button\" class=\"control-btn-white copy-html-btn\" data-doc-index=\"{{ forloop.counter0 }}\" title=\"Copier\">",
                    "refToolCallId": "call_HRRtfYwBFdFsrrsfRsdXd4zG"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_3jctWpCc1YAvaPKJKYX0Xtqi",
                    "statusText": "Change generated",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "templates/rawdocs/upload.html",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "codeHunk",
                    "lineBasedDiff": "--- C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html\n+++ C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html\n@@ -541,7 +541,7 @@\n                 <button type=\"button\" class=\"control-btn-white\" id=\"toggleEditBtnSingle\" title=\"Mode √©dition\">\n                   <i class=\"fas fa-edit\"></i>\n                 </button>\n-                <button type=\"button\" class=\"control-btn-white\" id=\"saveContentBtnSingle\" data-doc-id=\"{{ doc.id }}\" title=\"Sauvegarder\" style=\"display:none;\">\n+                <button type=\"button\" class=\"control-btn-white\" id=\"saveContentBtnSingle\" data-doc-id=\"{{ doc.id }}\" data-save-url=\"{% url 'rawdocs:save_structured_edits' doc.id %}\" title=\"Sauvegarder\" style=\"display:none;\">\n                   <i class=\"fas fa-save\"></i>\n                 </button>\n                 <button type=\"button\" class=\"control-btn-white\" id=\"copyHtmlBtnSingle\" title=\"Copier HTML\">",
                    "refToolCallId": "call_3jctWpCc1YAvaPKJKYX0Xtqi"
                },
                {
                    "type": "thought",
                    "text": "**Preparing JavaScript edits**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_xTwo6bAZ1QieksuewZSEvoKv",
                    "statusText": "Change generated",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "static/js/pdf-upload.js",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "codeHunk",
                    "lineBasedDiff": "--- C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js\n+++ C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js\n@@ -602,21 +602,22 @@\n       container = document.getElementById(\"zoomContainerSingle\")\n       const saveBtn = document.getElementById(\"saveContentBtnSingle\")\n       docId = saveBtn ? saveBtn.getAttribute(\"data-doc-id\") : null\n+      saveUrl = saveBtn ? saveBtn.getAttribute(\"data-save-url\") : null\n     } else {\n       container = document.getElementById(`zoom-container-multi-${docIndex}`)\n       const saveBtn = document.querySelector(\n         `.save-content-btn[data-doc-index=\"${docIndex}\"]`\n       )\n       docId = saveBtn ? saveBtn.getAttribute(\"data-doc-id\") : null\n-    }\n-\n-    if (!container || !docId) {\n+      saveUrl = saveBtn ? saveBtn.getAttribute(\"data-save-url\") : null\n+    }\n+\n+    if (!container || !docId || !saveUrl) {\n       Utils.showToast(\"error\", \"Impossible de sauvegarder le contenu\")\n       return\n     }\n \n     const formatted_content = container.innerHTML\n-    const saveUrl = `/rawdocs/save-structured-edits/${docId}/`\n \n     Utils.showToast(\"info\", \"Sauvegarde en cours...\")\n ",
                    "refToolCallId": "call_xTwo6bAZ1QieksuewZSEvoKv"
                },
                {
                    "type": "thought",
                    "text": "**Updating fetch response handling variable and** declarations"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_eFh2rLryfL5Wiqbk3Lca7M9C",
                    "statusText": "Change generated",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "static/js/pdf-upload.js",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "codeHunk",
                    "lineBasedDiff": "--- C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js\n+++ C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js\n@@ -596,7 +596,9 @@\n \n   // Save content for a specific document\n   async saveContent(docIndex) {\n-    let container, docId\n+    let container\n+    let docId\n+    let saveUrl\n \n     if (docIndex === \"single\") {\n       container = document.getElementById(\"zoomContainerSingle\")",
                    "refToolCallId": "call_eFh2rLryfL5Wiqbk3Lca7M9C"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_fLZydhbSbVPuICL1wEl9QIzV",
                    "statusText": "Change generated",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "static/js/pdf-upload.js",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "codeHunk",
                    "lineBasedDiff": "--- C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js\n+++ C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js\n@@ -632,9 +632,13 @@\n         },\n         body: JSON.stringify({\n           formatted_content: formatted_content,\n-          extracted_content: \"\",\n         }),\n       })\n+\n+      if (!response.ok) {\n+        const errorText = await response.text()\n+        throw new Error(`HTTP ${response.status}: ${errorText}`)\n+      }\n \n       const data = await response.json()\n ",
                    "refToolCallId": "call_fLZydhbSbVPuICL1wEl9QIzV"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_VYwljWGvzoGblSk0TLvsSRXF",
                    "statusText": "Change generated",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "static/js/pdf-upload.js",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "codeHunk",
                    "lineBasedDiff": "--- C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js\n+++ C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js\n@@ -654,7 +654,8 @@\n         )\n       }\n     } catch (error) {\n-      Utils.showToast(\"error\", \"Erreur r√©seau lors de la sauvegarde\")\n+      const message = error && error.message ? error.message : \"Erreur r√©seau lors de la sauvegarde\"\n+      Utils.showToast(\"error\", message)\n       console.error(\"Save error:\", error)\n     }\n   }",
                    "refToolCallId": "call_VYwljWGvzoGblSk0TLvsSRXF"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_IeoEy4mxel3hM6QmnDqzqtnP",
                    "statusText": "Viewing file",
                    "status": "IN_PROGRESS",
                    "context": {
                        "files": [
                            {
                                "path": "static/js/pdf-upload.js",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "text",
                    "text": "#ifs Correct appliqu√©s\n\n## 1. URL de sauvegarde r√©solue c√¥t√© template\n- **Fichier**¬†: `templates/upload.html`\n  - Les boutons ‚ÄúSauvegarder‚Äù (vue multi-document et vue simple) portent d√©sormais un attribut `data-save-url` g√©n√©r√© par `{% url 'rawdocs:save_structured_edits' ... %}`.  \n  - Cela garantit que le front utilise l‚ÄôURL exacte expos√©e par Django, √©liminant les 404 li√©s auxdocs chemins constru/rawits en dur.\n\n## 2. Lecture de l‚ÄôURL c√¥t√© JavaScript\n- **Fichier**¬†: `static/js/pdf-upload.js`\n  - La m√©thode `saveContent` r√©cup√®re maintenant `docId` **et** `saveUrl` depuis ces attributs `data-*` pour les panneaux single/multi.\n  - V√©rification renforc√©e : on refuse la sauvegarde si l‚Äôun d‚Äôeux manque, avec un toast explicite.\n\n## 3. Payload et gestion d‚Äôerreurs\n- Suppression de la cl√© inutile `extracted_content`.\n- Ajout d‚Äôun contr√¥le `response.ok` avant le `response.json()` :\n  ```javascript\n  if (!response.ok) {\n    const errorText = await response.text()\n    throw new Error(`HTTP ${response.status}: ${errorText}`)\n  }\n  ```\n  - En cas d‚Äô√©chec, l‚Äôexception transporte le statut + le corps, ce qui alimente le toast avec un message pr√©cis plut√¥t qu‚Äôun g√©n√©rique ‚ÄúErreur r√©seau‚Äù.\n\n## √âtapes suivantes\n\n1. **Tester** la sauvegarde depuis l‚ÄôUI :\n   - Assurez-vous d‚Äô√™tre connect√© et propri√©taire du document.\n   - Activez le mode √©dition, modifiez le contenu, puis cliquez sur ‚ÄúSauvegarder‚Äù.\n\n2. **Observer la console et les toasts** :\n   - Succ√®s : toast vert avec le message renvoy√© par la vue.\n   - √âchec : toast rouge affichant (si dispo) `HTTP <code>: <erreur>` pour diagnostiquer rapidement.\n   - En cas de 404, v√©rifier les permissions (document non appartenant √† l‚Äôutilisateur courant).\n\n3. **Si tout est OK**, committez :\n   ```bash\n   git status\n   git add templates/rawdocs/upload.html static/js/pdf-upload.js\n   git commit -m \"Fix structured content save URL and error handling\"\n   ```\n\nBesoin d‚Äôun accompagnement pour les tests ou la suite (permissions, compression HTML, etc.)¬†? Je suis disponible."
                },
                {
                    "type": "codePatch",
                    "diffs": [
                        {
                            "path": "templates/rawdocs/upload.html",
                            "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/templates/rawdocs/upload.html",
                            "patch": "@@ -10092,32 +10092,106 @@\n oop.counter0 %7D%7D%22\n+ data-save-url=%22%7B%25 url 'rawdocs:save_structured_edits' doc_data.doc.id %25%7D%22\n  title=%22Sauvegar\n@@ -22832,24 +22832,89 @@\n %7B doc.id %7D%7D%22\n+ data-save-url=%22%7B%25 url 'rawdocs:save_structured_edits' doc.id %25%7D%22\n  title=%22Sauv\n",
                            "isNew": false,
                            "isApplied": true,
                            "isUnread": true,
                            "timestamp": 1760831718416
                        },
                        {
                            "path": "static/js/pdf-upload.js",
                            "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/static/js/pdf-upload.js",
                            "patch": "@@ -17347,39 +17347,64 @@\n    let container\n-, docId\n+%0D%0A    let docId%0D%0A    let saveUrl\n %0D%0A%0D%0A    if (docI\n@@ -17626,24 +17626,96 @@\n d%22) : null%0D%0A\n+      saveUrl = saveBtn ? saveBtn.getAttribute(%22data-save-url%22) : null%0D%0A\n     %7D else %7B\n@@ -17974,24 +17974,96 @@\n d%22) : null%0D%0A\n+      saveUrl = saveBtn ? saveBtn.getAttribute(%22data-save-url%22) : null%0D%0A\n     %7D%0D%0A%0D%0A   \n@@ -18083,24 +18083,36 @@\n er %7C%7C !docId\n+ %7C%7C !saveUrl\n ) %7B%0D%0A      U\n@@ -18253,72 +18253,8 @@\n TML%0D\n-%0A    const saveUrl = %60/rawdocs/save-structured-edits/$%7BdocId%7D/%60%0D\n %0A%0D%0A \n@@ -18620,45 +18620,165 @@\n     \n-  extracted_content: %22%22,%0D%0A        %7D),\n+%7D),%0D%0A      %7D)%0D%0A%0D%0A      if (!response.ok) %7B%0D%0A        const errorText = await response.text()%0D%0A        throw new Error(%60HTTP $%7Bresponse.status%7D: $%7BerrorText%7D%60)\n %0D%0A  \n@@ -18774,33 +18774,32 @@\n Text%7D%60)%0D%0A      %7D\n-)\n %0D%0A%0D%0A      const \n@@ -19165,70 +19165,142 @@\n     \n-Utils.showToast(%22error%22, %22Erreur r%C3%A9seau lors de la sauvegarde%22\n+const message = error && error.message ? error.message : %22Erreur r%C3%A9seau lors de la sauvegarde%22%0D%0A      Utils.showToast(%22error%22, message\n )%0D%0A \n",
                            "isNew": false,
                            "isApplied": true,
                            "isUnread": true,
                            "timestamp": 1760831731151
                        }
                    ]
                }
            ],
            "context": {
                "author": "Code agent",
                "operationId": "150226e7-cbd0-44fc-93b9-ff40d8f56f26",
                "usageStatistics": {
                    "llmCallsUsedForAnswer": 14.0,
                    "llmCallsUsedTotal": 35.0,
                    "llmCallsLimit": 30.0,
                    "limitRenewalMillisecondTimestamp": 1760906432947
                }
            },
            "createdAt": 1760831650333,
            "isInternal": false
        },
        {
            "id": "36d16d11-84b3-4d3b-a07d-18ede6f7be27",
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "En fait quand vous consultez utils py et que vous regardez l extraction Exiftool pour les documents upload√© pourquoi le titre du document s affiche pas et comment regler ca pas que le titre il y a d autres champs qui s affiche pas aussi"
                }
            ],
            "context": {
                "currentFile": {
                    "path": "rawdocs/utils.py",
                    "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/utils.py",
                    "sources": [
                        "opened_tab"
                    ],
                    "content": "# rawdocs/utils.py\nimport os\nimport re\nimport json\nimport requests\nfrom pathlib import Path\nfrom datetime import datetime\nfrom urllib.parse import urlparse\nimport json\nimport requests\nfrom groq import Groq\nimport dateparser\n\nimport spacy\nfrom PyPDF2 import PdfReader\nfrom langdetect import detect\nfrom dotenv import load_dotenv\n\nimport subprocess\nimport json\nfrom pdfminer.high_level import extract_text\nfrom langdetect import detect\nimport spacy\nfrom pathlib import Path\nfrom docx import Document\nimport pdfplumber\nimport fitz  # PyMuPDF pour l'extraction d'images\nfrom PIL import Image\nimport io\nimport base64\n\n# Charger les variables d'environnement depuis .env\nload_dotenv()\n\n# Chargez une fois les mod√É¬®les spaCy :\nNLP_FR = spacy.load(\"fr_core_news_sm\")\nNLP_EN = spacy.load(\"en_core_web_sm\")\n\n# Stopwords FR/EN basiques pour nettoyage\nSTOPWORDS = {\n    \"le\", \"la\", \"les\", \"de\", \"des\", \"du\", \"un\", \"une\", \"et\", \"en\", \"√É \", \"dans\", \"que\", \"qui\",\n    \"pour\", \"par\", \"sur\", \"avec\", \"au\", \"aux\", \"ce\", \"ces\", \"se\", \"ses\", \"est\",\n    \"the\", \"and\", \"of\", \"to\", \"in\", \"that\", \"it\", \"is\", \"was\", \"for\", \"on\", \"are\", \"with\",\n    \"as\", \"i\", \"at\", \"be\", \"by\", \"this\"\n}\n\n\ndef extract_exif_metadata(file_path):\n    \"\"\"Extraction brute avec ExifTool\"\"\"\n    print(f\"√∞≈∏‚Äù¬ç Tentative d'extraction ExifTool pour {file_path}\")\n    try:\n        result = subprocess.run(\n            [\"exiftool\", \"-json\", file_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        if result.returncode != 0:\n            print(f\"√¢¬ù≈í Erreur ExifTool: {result.stderr}\")\n            return {}\n        metadata = json.loads(result.stdout)[0]\n        print(f\"√¢≈ì‚Ä¶ ExifTool a extrait {len(metadata)} champs: {list(metadata.keys())}\")\n        return metadata\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur ExifTool: {e}\")\n        print(f\"PATH actuel: {os.environ['PATH']}\")\n        return {}\n\n\ndef extract_text_content(file_path):\n    \"\"\"Extraction du texte brut selon le type de fichier\"\"\"\n    ext = Path(file_path).suffix.lower()\n    text = \"\"\n    print(f\"√∞≈∏‚Äù¬ç D√É¬©but de l'extraction texte pour {file_path} (type: {ext})\")\n    if ext == \".pdf\":\n        try:\n            text = extract_text(file_path)\n            print(f\"√¢≈ì‚Ä¶ Texte extrait avec succ√É¬®s ({len(text)} caract√É¬®res): {text[:100]}...\")\n        except Exception as e:\n            print(f\"√¢¬ù≈í Erreur extraction texte PDF: {e}\")\n    elif ext in [\".docx\"]:\n        try:\n            doc = Document(file_path)\n            text = \"\\n\".join([p.text for p in doc.paragraphs])\n            print(f\"√¢≈ì‚Ä¶ Texte extrait avec succ√É¬®s ({len(text)} caract√É¬®res)\")\n        except Exception as e:\n            print(f\"√¢¬ù≈í Erreur extraction texte DOCX: {e}\")\n    return text\n\n\ndef enhanced_nlp_analysis(text):\n    \"\"\"Analyse NLP renforc√©e sans LLM\"\"\"\n    meta = {}\n    if not text.strip():\n        print(\"‚ö†Ô∏è Aucun texte √† analyser pour NLP\")\n        return meta\n\n    print(\"üîç Lancement de l'analyse NLP renforc√©e\")\n\n    # D√âTECTION DU TYPE DE DOCUMENT\n    doc_types = {\n        'guideline': ['guideline', 'guidance', 'recommendations', 'best practices', 'guide', 'notice', 'instructions',\n                      'ligne directrice', 'recommandations'],\n        'regulation': ['regulation', 'directive', 'law', 'legal', 'article', 'decree', 'r√®glement', 'directive'],\n        'report': ['report', 'assessment', 'evaluation', 'analysis', 'study', 'rapport', '√©valuation'],\n        'procedure': ['procedure', 'protocol', 'method', 'process', 'operating', 'utilisation', 'pr√©cautions',\n                      'proc√©dure', 'protocole'],\n        'specifications': ['specifications', 'specs', 'requirements', 'criteria', 'specification', 'sp√©cifications',\n                           'exigences'],\n        'manufacturer': ['manufacturer', 'manufacturing', 'production', 'facility', 'plant', 'fabricant',\n                         'manufacturier'],\n        'certificate': ['certificate', 'certification', 'approval', 'authorization', 'certificat', 'approbation'],\n        'standard': ['standard', 'norm', 'iso', 'quality standard', 'norme', 'standard qualit√©']\n    }\n\n    text_lower = text.lower()\n    for doc_type, keywords in doc_types.items():\n        if any(keyword in text_lower for keyword in keywords):\n            meta['type'] = {\n                'value': doc_type,\n                'confidence': 85,\n                'method': 'nlp_keyword_match'\n            }\n            print(f\"‚úÖ Type d√©tect√©: {doc_type}\")\n            break\n\n    # EXTRACTION DE DATES AM√âLIOR√âE AVEC PRIORIT√â SUR LES LABELS EXPLICITES\n    date_candidates = []\n\n    # D√©tection de langue\n    language_info = detect_document_language(text)\n    lang = language_info.get('value', 'en') if language_info else 'en'\n    print(f\"Langue d√©tect√©e pour dates: {lang}\")\n\n    # D√©finir les noms de mois par langue\n    month_names_by_lang = {\n        'en': \"(?:January|February|March|April|May|June|July|August|September|October|November|December)\",\n        'fr': \"(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\"\n    }\n    month_names = month_names_by_lang.get(lang, month_names_by_lang['en'])\n\n    # Pattern 1: Label explicite \"Date de publication:\" (priorit√© maximale)\n    pub_label_pattern = rf'(?:Date\\s+de\\s+publication|Published\\s+on)[:\\s\\|]+(\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}})'\n    for match in re.finditer(pub_label_pattern, text, re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date:\n            normalized_date = parsed_date.strftime('%d %B %Y')\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': 99,  # Priorit√© maximale pour labels explicites\n                'context': 'publication_date_label',\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e (Publication label): {date_str} ‚Üí {normalized_date}\")\n\n    # Pattern 2: Date dans en-t√™te avec mois/ann√©e (format EMA typique)\n    header_date_pattern = rf'^([A-Z][a-z]+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})'\n    for match in re.finditer(header_date_pattern, text, re.MULTILINE | re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date and not any(\n                c['context'] == 'publication_date_label' for c in date_candidates):  # √âviter si label trouv√©\n            normalized_date = parsed_date.strftime('1 %B %Y')\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': 93,\n                'context': 'header_date',\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e (en-t√™te): {date_str} ‚Üí {normalized_date}\")\n\n    # Pattern 3: \"Step X dated DD Month YYYY\" (priorit√© √©lev√©e)\n    step_dated_pattern = rf'Step\\s+\\d+[a-z]?\\s+(?:dated\\s+)?(\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}})'\n    for match in re.finditer(step_dated_pattern, text, re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date and not any(c['context'] == 'publication_date_label' for c in date_candidates):\n            normalized_date = parsed_date.strftime('%d %B %Y')\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': 98,\n                'context': 'step_dated',\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e (Step dated): {date_str}\")\n\n    # Pattern 4: \"Final adoption by CHMP\" (priorit√© haute)\n    adoption_patterns = [\n        (rf'Final\\s+adoption\\s+by\\s+CHMP[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 92),\n        (rf'Final\\s+adoption[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 90),\n        (rf'Adopted\\s+by\\s+CHMP[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 85),\n        (rf'Adoption\\s+finale\\s+par\\s+CHMP[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 92)\n    ]\n    for pattern, priority in adoption_patterns:\n        for match in re.finditer(pattern, text, re.IGNORECASE):\n            date_str = match.group(1)\n            parsed_date = dateparser.parse(date_str, languages=[lang])\n            if parsed_date and not any(c['context'] == 'publication_date_label' for c in date_candidates):\n                normalized_date = parsed_date.strftime('1 %B %Y')\n                date_candidates.append({\n                    'date': normalized_date,\n                    'priority': priority,\n                    'context': 'final_adoption',\n                    'raw': date_str\n                })\n                print(f\"üìÖ Date trouv√©e (Final adoption): {date_str} ‚Üí {normalized_date}\")\n\n    # Pattern 5: Format DD Month YYYY (g√©n√©rique)\n    generic_date_pattern = rf'\\b(\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}})\\b'\n    for match in re.finditer(generic_date_pattern, text, re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date and not any(c['context'] == 'publication_date_label' for c in date_candidates):\n            normalized_date = parsed_date.strftime('%d %B %Y')\n            start_pos = max(0, match.start() - 100)\n            end_pos = min(len(text), match.end() + 100)\n            context = text[start_pos:end_pos].lower()\n            if 'coming into effect' in context or 'entr√©e en vigueur' in context or 'deadline' in context:\n                priority = 50\n                ctx_type = 'effective_date'\n            elif 'published' in context or 'publication' in context or 'publi√©' in context:\n                priority = 94\n                ctx_type = 'publication_date'\n            elif 'adopted' in context or 'adoption' in context or 'adopt√©' in context:\n                priority = 90\n                ctx_type = 'adoption_date'\n            else:\n                priority = 75\n                ctx_type = 'generic_date'\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': priority,\n                'context': ctx_type,\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e ({ctx_type}): {date_str}\")\n\n    # Fallback avec dateparser sur candidats bruts\n    raw_candidates = re.findall(rf'\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}}|\\d{{1,2}}\\.\\d{{1,2}}\\.\\d{{4}}', text,\n                                re.IGNORECASE)\n    for raw in raw_candidates:\n        parsed = dateparser.parse(raw, languages=[lang, 'en'])\n        if parsed and not any(c['raw'] == raw for c in date_candidates):\n            normalized = parsed.strftime('%d %B %Y')\n            if 'directive' in text_lower or 'law' in text_lower or re.search(r'\\d{4}/\\d+/EC', text):\n                priority = 40\n            else:\n                priority = 70\n            date_candidates.append({\n                'date': normalized,\n                'priority': priority,\n                'context': 'fallback_dateparser',\n                'raw': raw\n            })\n            print(f\"üìÖ Date fallback (dateparser): {raw} ‚Üí {normalized}\")\n\n    # S√©lection de la meilleure date (priorit√© sur labels explicites)\n    if date_candidates:\n        date_candidates.sort(key=lambda x: (-x['priority'], datetime.strptime(x['date'], '%d %B %Y') if '%d' in x[\n            'date'] else datetime.min), reverse=True)\n        selected_date = date_candidates[0]['date']\n        meta['publication_date'] = {\n            'value': selected_date,\n            'confidence': date_candidates[0]['priority'],\n            'context': date_candidates[0]['context'],\n            'method': 'nlp_pattern_match_multilingual'\n        }\n        print(\n            f\"‚úÖ Date de publication s√©lectionn√©e: {selected_date} (priorit√© {date_candidates[0]['priority']}, contexte: {date_candidates[0]['context']})\")\n\n    # EXTRACTION DE VERSION\n    version_patterns = [\n        r'version\\s*[:\\-]?\\s*(\\d+\\.?\\d*)',\n        r'v\\.?\\s*(\\d+\\.?\\d*)',\n        r'rev\\.?\\s*(\\d+)',\n        r'r(\\d+)',\n        r'step\\s*(\\d+)',\n        r'corr\\.?\\s*(\\d+)',\n        r'EN_(\\d+\\.?\\d*)',\n        r'_(\\d+\\.?\\d*)\\.pdf'\n    ]\n    for pattern in version_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            meta['version'] = {\n                'value': match.group(1),\n                'confidence': 80,\n                'method': 'regex_version'\n            }\n            print(f\"‚úÖ Version d√©tect√©e: {match.group(1)}\")\n            break\n\n    return meta\n\n\ndef extract_structured_metadata(file_path):\n    \"\"\"Extraction bas√É¬©e sur la structure du document\"\"\"\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            if not pdf.pages:\n                print(\"√¢≈° √Ø¬∏¬è Aucune page trouv√É¬©e dans le PDF\")\n                return {'title_structured': '', 'header_info': ''}\n\n            first_page = pdf.pages[0]\n\n            # Extraire le texte avec position\n            text_objects = first_page.extract_words()\n\n            if not text_objects:\n                print(\"√¢≈° √Ø¬∏¬è Aucun texte extrait de la premi√É¬®re page\")\n                # Essayer un fallback avec pdfminer\n                try:\n                    text = extract_text(file_path)  # Utiliser pdfminer comme secours\n                    first_line = text.split('\\n')[0].strip() if text else ''\n                    return {\n                        'title_structured': first_line,\n                        'header_info': first_line\n                    }\n                except Exception as e:\n                    print(f\"√¢¬ù≈í Erreur fallback pdfminer: {e}\")\n                    return {'title_structured': '', 'header_info': ''}\n\n            # Analyser les en-t√É¬™tes (texte en haut de page)\n            header_text = []\n            for obj in text_objects:\n                if obj.get('top', float('inf')) < 100:  # 100 pixels du haut\n                    header_text.append(obj['text'])\n\n            header_string = ' '.join(header_text) if header_text else ''\n\n            # Extraire titre (g√É¬©n√É¬©ralement le texte le plus gros)\n            title_candidates = []\n            sizes = [obj.get('size', 0) for obj in text_objects if obj.get('size') is not None]\n\n            if not sizes:\n                print(\"√¢≈° √Ø¬∏¬è Aucune taille de texte trouv√É¬©e\")\n                return {\n                    'title_structured': header_string or Path(file_path).stem,\n                    'header_info': header_string\n                }\n\n            max_size = max(sizes)\n            for obj in text_objects:\n                if obj.get('size', 0) >= max_size - 2:  # Texte de taille importante\n                    title_candidates.append(obj['text'])\n\n            title = ' '.join(title_candidates[:15]) if title_candidates else header_string or Path(file_path).stem\n\n            return {\n                'title_structured': title,\n                'header_info': header_string\n            }\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur extraction structur√É¬©e: {e}\")\n        return {'title_structured': '', 'header_info': ''}\n\n\ndef extract_extended_pdf_properties(file_path):\n    \"\"\"Extraction propri√É¬©t√É¬©s PDF √É¬©tendues\"\"\"\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            metadata = pdf.metadata or {}\n\n            # Analyser les m√É¬©tadonn√É¬©es XMP si disponibles\n            xmp_metadata = {}\n            try:\n                if hasattr(pdf, 'doc') and hasattr(pdf.doc, 'xmp_metadata'):\n                    xmp = pdf.doc.xmp_metadata\n                    if xmp:\n                        xmp_metadata = {\n                            'creator': getattr(xmp, 'creator', ''),\n                            'description': getattr(xmp, 'description', ''),\n                            'subject': getattr(xmp, 'subject', ''),\n                            'keywords': getattr(xmp, 'keywords', '')\n                        }\n            except Exception:\n                pass\n\n            return {**metadata, **xmp_metadata}\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur propri√É¬©t√É¬©s PDF: {e}\")\n        return {}\n\n\ndef regex_metadata_extraction(text):\n    \"\"\"Extraction par regex sp√É¬©cialis√É¬©es pour documents r√É¬©glementaires\"\"\"\n    patterns = {\n        'ema_reference': r'EMA/[A-Z]+/[A-Z]+/\\d+/\\d+',\n        'ich_reference': r'ICH/[A-Z0-9]+/\\d+',\n        'step_version': r'Step\\s+(\\d+[a-z]?)',\n        'correction': r'Corr\\.?\\s*(\\d+)',\n        'european_directive': r'Directive\\s+(\\d{4}/\\d+/EC)',\n        'organization_regex': r'(European Medicines Agency|EMA|FDA|ANSM|MHRA)',\n        'effective_date': r'effective\\s+(?:from\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n        'published_date': r'published\\s+(?:on\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n        'adoption_date': r'adopted\\s+(?:on\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})'\n    }\n\n    extracted = {}\n    for key, pattern in patterns.items():\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            extracted[key] = match.group(1) if match.groups() else match.group(0)\n\n    return extracted\n\n\ndef merge_metadata_with_priority(metadata_sources):\n    \"\"\"Fusionner les m√É¬©tadonn√É¬©es avec priorit√É¬©\"\"\"\n    merged = {}\n\n    field_mapping = {\n        'title': ['Title', 'title', 'title_inferred', 'title_structured', 'Subject'],\n        'type': ['type', 'DM_Category', 'doc_type'],\n        'source': ['Author', 'Creator', 'organization', 'source_organization', 'ema_reference', 'organization_regex'],\n        'version': ['version', 'DM_Version', 'step_version', 'correction'],\n        'publication_date': ['CreateDate', 'publication_date', 'effective_date', 'published_date', 'date'],\n        'language': ['Language', 'language', 'DM_Language'],\n        'country': ['country'],\n        'context': ['DM_Category', 'DM_Path', 'context']\n    }\n\n    for field, possible_keys in field_mapping.items():\n        for source in metadata_sources:\n            for key in possible_keys:\n                if key in source and source[key] and not merged.get(field):\n                    merged[field] = source[key]\n                    break\n            if merged.get(field):\n                break\n\n    return merged\n\n\ndef infer_additional_metadata(text):\n    \"\"\"Analyse NLP du contenu pour d√É¬©duire d'autres m√É¬©tadonn√É¬©es (version am√É¬©lior√É¬©e)\"\"\"\n    return enhanced_nlp_analysis(text)\n\n\ndef detect_document_language(text, file_path=\"\"):\n    \"\"\"\n    D√©tecte la langue du document avec plusieurs m√©thodes et retourne un score de confiance\n    \"\"\"\n    if not text or len(text.strip()) < 50:\n        print(\"‚ö†Ô∏è Texte trop court pour d√©tecter la langue\")\n        return None\n\n    detected_langs = []\n\n    # M√©thode 1 : langdetect sur le texte complet\n    try:\n        lang_full = detect(text[:5000])  # Analyse des premiers 5000 caract√®res\n        detected_langs.append(('langdetect_full', lang_full, 90))\n        print(f\"‚úÖ Langue d√©tect√©e (texte complet): {lang_full}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Erreur langdetect (texte complet): {e}\")\n\n    # M√©thode 2 : langdetect sur plusieurs √©chantillons (plus fiable)\n    try:\n        samples = [\n            text[:1000],\n            text[len(text) // 2:len(text) // 2 + 1000] if len(text) > 2000 else text,\n            text[-1000:] if len(text) > 1000 else text\n        ]\n        lang_counts = {}\n        for sample in samples:\n            if len(sample.strip()) > 50:\n                try:\n                    lang = detect(sample)\n                    lang_counts[lang] = lang_counts.get(lang, 0) + 1\n                except:\n                    pass\n\n        if lang_counts:\n            # Langue la plus fr√©quente\n            most_common_lang = max(lang_counts, key=lang_counts.get)\n            confidence = 85 + (lang_counts[most_common_lang] * 5)  # Max 100\n            detected_langs.append(('langdetect_samples', most_common_lang, min(confidence, 98)))\n            print(\n                f\"‚úÖ Langue d√©tect√©e (√©chantillons multiples): {most_common_lang} (consensus: {lang_counts[most_common_lang]}/3)\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Erreur langdetect (√©chantillons): {e}\")\n\n    # M√©thode 3 : Analyse du nom de fichier\n    if file_path:\n        filename = Path(file_path).stem.upper()\n        lang_indicators = {\n            'EN': ['_EN_', '_EN.', 'ENGLISH', '_ENG_'],\n            'FR': ['_FR_', '_FR.', 'FRENCH', '_FRA_'],\n            'DE': ['_DE_', '_DE.', 'GERMAN', '_DEU_'],\n            'ES': ['_ES_', '_ES.', 'SPANISH', '_ESP_'],\n            'IT': ['_IT_', '_IT.', 'ITALIAN', '_ITA_'],\n            'PL': ['_PL_', '_PL.', 'POLISH', '_POL_']\n        }\n\n        for lang_code, indicators in lang_indicators.items():\n            if any(ind in filename for ind in indicators):\n                detected_langs.append(('filename', lang_code.lower(), 80))\n                print(f\"‚úÖ Langue d√©tect√©e (nom de fichier): {lang_code.lower()}\")\n                break\n\n    # M√©thode 4 : Analyse des mots-cl√©s de langue\n    text_sample = text[:3000].lower()\n    keyword_scores = {\n        'en': sum([\n            text_sample.count(' the ') * 2,\n            text_sample.count(' and '),\n            text_sample.count(' of '),\n            text_sample.count(' to '),\n            text_sample.count(' in '),\n            text_sample.count('guideline'),\n            text_sample.count('regulation')\n        ]),\n        'fr': sum([\n            text_sample.count(' le ') * 2,\n            text_sample.count(' la ') * 2,\n            text_sample.count(' les ') * 2,\n            text_sample.count(' de '),\n            text_sample.count(' du '),\n            text_sample.count(' des '),\n            text_sample.count('r√®glement'),\n            text_sample.count('directive')\n        ]),\n        'de': sum([\n            text_sample.count(' der ') * 2,\n            text_sample.count(' die ') * 2,\n            text_sample.count(' das ') * 2,\n            text_sample.count(' und '),\n            text_sample.count(' ist '),\n            text_sample.count('richtlinie')\n        ])\n    }\n\n    if keyword_scores:\n        best_lang = max(keyword_scores, key=keyword_scores.get)\n        if keyword_scores[best_lang] > 10:  # Seuil minimal\n            confidence = min(70 + (keyword_scores[best_lang] // 5), 85)\n            detected_langs.append(('keywords', best_lang, confidence))\n            print(f\"‚úÖ Langue d√©tect√©e (mots-cl√©s): {best_lang} (score: {keyword_scores[best_lang]})\")\n\n    # Consolidation : choisir la meilleure d√©tection\n    if not detected_langs:\n        print(\"‚ùå Aucune langue d√©tect√©e\")\n        return None\n\n    # Trier par confiance et choisir la meilleure\n    detected_langs.sort(key=lambda x: x[2], reverse=True)\n    best_method, best_lang, best_confidence = detected_langs[0]\n\n    # V√©rifier la coh√©rence entre m√©thodes\n    lang_votes = {}\n    for method, lang, conf in detected_langs:\n        lang_votes[lang] = lang_votes.get(lang, 0) + 1\n\n    # Si plusieurs m√©thodes sont d'accord, augmenter la confiance\n    if lang_votes.get(best_lang, 0) >= 2:\n        best_confidence = min(best_confidence + 10, 98)\n        print(f\"üéØ Consensus sur la langue: {best_lang} ({lang_votes[best_lang]} m√©thodes concordantes)\")\n\n    return {\n        'value': best_lang,\n        'confidence': best_confidence,\n        'method': f'language_detection_{best_method}'\n    }\n\n\ndef extract_metadata_local_pipeline(file_path):\n    \"\"\"Point d'entr√©e principal pour l'extraction locale\"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"üöÄ EXTRACTION DYNAMIQUE PROFESSIONNELLE\")\n    print(f\"üìÑ Fichier: {Path(file_path).name}\")\n    print(f\"{'=' * 80}\\n\")\n\n    # 1. Extraction du texte\n    full_text = extract_text_content(file_path)\n\n    # 1b. D√©tection de la langue (NOUVEAU)\n    detected_language = detect_document_language(full_text, file_path)\n\n    # 2. Analyse NLP renforc√©e comme source principale\n    nlp_meta = enhanced_nlp_analysis(full_text)\n\n    # 3. Extraction compl√©mentaire via Exif (si disponible)\n    exif_meta = extract_exif_metadata(file_path)\n\n    # 4. Normalisation : convertir toutes les donn√©es en format uniforme\n    normalized_meta = {}\n\n    # Traiter les donn√©es NLP\n    for field, value in nlp_meta.items():\n        if isinstance(value, dict) and 'value' in value:\n            # D√©j√† au bon format\n            normalized_meta[field] = value\n        elif value:  # Valeur simple (comme type='guideline')\n            # Convertir en format structur√©\n            normalized_meta[field] = {\n                'value': value,\n                'confidence': 80,  # Confidence par d√©faut pour d√©tection NLP\n                'method': 'nlp_analysis'\n            }\n\n    # Ajouter les donn√©es Exif si elles am√©liorent la qualit√©\n    if exif_meta:\n        # Title depuis Exif (si pas d√©j√† trouv√© avec meilleure confidence)\n        if 'Title' in exif_meta and exif_meta['Title']:\n            if 'title' not in normalized_meta or normalized_meta['title']['confidence'] < 75:\n                normalized_meta['title'] = {\n                    'value': exif_meta['Title'],\n                    'confidence': 75,\n                    'method': 'exif'\n                }\n\n        # Source depuis Author\n        if 'Author' in exif_meta and exif_meta['Author']:\n            if 'source' not in normalized_meta or normalized_meta['source']['confidence'] < 70:\n                normalized_meta['source'] = {\n                    'value': exif_meta['Author'],\n                    'confidence': 70,\n                    'method': 'exif'\n                }\n\n        # Language depuis Exif (priorit√© moyenne)\n        if 'Language' in exif_meta and exif_meta['Language']:\n            if 'language' not in normalized_meta or normalized_meta['language']['confidence'] < 75:\n                normalized_meta['language'] = {\n                    'value': exif_meta['Language'],\n                    'confidence': 75,\n                    'method': 'exif'\n                }\n\n    # Ajouter la langue d√©tect√©e (si pas de meilleure source)\n    if detected_language:\n        if 'language' not in normalized_meta or normalized_meta['language']['confidence'] < detected_language[\n            'confidence']:\n            normalized_meta['language'] = detected_language\n\n    # 5. Consolidation finale : extraire les valeurs et calculer les scores\n    final_metadata = {}\n    confidence_scores = {}\n\n    # Liste des champs attendus\n    expected_fields = ['title', 'type', 'publication_date', 'version', 'source', 'context', 'country', 'language',\n                       'url_source']\n\n    for field in expected_fields:\n        if field in normalized_meta:\n            meta_obj = normalized_meta[field]\n            final_metadata[field] = meta_obj['value']\n            confidence_scores[field] = meta_obj['confidence']\n            print(f\"   ‚Ä¢ {field}: {meta_obj['value']} (confiance: {meta_obj['confidence']}% - {meta_obj['method']})\")\n        else:\n            final_metadata[field] = \"\"\n            confidence_scores[field] = 0\n            print(f\"   ‚Ä¢ {field}: (non d√©tect√©)\")\n\n    # 6. Calcul de qualit√© globale\n    overall_quality = calculate_overall_quality(confidence_scores)\n\n    # 7. Construire l'objet quality avec tous les d√©tails\n    extraction_reasoning = {}\n    for field in expected_fields:\n        if field in normalized_meta:\n            method = normalized_meta[field]['method']\n            extraction_reasoning[field] = f\"Detected via {method}\"\n        else:\n            extraction_reasoning[field] = \"Not detected\"\n\n    final_metadata['quality'] = {\n        'extraction_rate': overall_quality,\n        'field_scores': confidence_scores,\n        'extraction_reasoning': extraction_reasoning,\n        'extracted_fields': sum(1 for v in final_metadata.values() if v and v != \"\"),\n        'total_fields': len(expected_fields),\n        'llm_powered': False\n    }\n\n    print(f\"\\n{'=' * 80}\")\n    print(f\"‚úÖ EXTRACTION TERMIN√âE\")\n    print(f\"üìä Taux de remplissage: {final_metadata['quality']['extraction_rate']}%\")\n    print(\n        f\"üìà Champs extraits: {final_metadata['quality']['extracted_fields']}/{final_metadata['quality']['total_fields']}\")\n    print(f\"{'=' * 80}\\n\")\n\n    return final_metadata\n\n\ndef call_mistral_with_confidence(text_chunk, document_url=\"\", filename=\"\"):\n    \"\"\"\n    Get extraction + confidence scores from Mistral in one call.\n    La cl√É¬© API est lue depuis la variable d'environnement MISTRAL_API_KEY.\n    \"\"\"\n    api_key = os.getenv(\n        \"MISTRAL_API_KEY\") or \"j2wOKpM86nlZhhlvkXXG7rFd4bhM4PN5\"  # Fallback √É  la cl√É¬© hardcod√É¬©e si .env non disponible\n\n    try:\n        url = \"https://api.mistral.ai/v1/chat/completions\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n\n        prompt = f\"\"\"\n        You are an expert document analyzer. Analyze this document and extract metadata with confidence scores.\n\n        DOCUMENT TEXT (first 5000 chars):\n        {text_chunk[:5000]}\n\n        SOURCE URL: {document_url}\n        FILENAME: {filename}\n\n        HINT: Look for version numbers in headers like \"DTC_S006597_PF_R_FAB_17319_EN_15.0\" where 15.0 is the version\n\n        TASK: Return ONLY a JSON object with extracted metadata AND your confidence scores:\n\n        {{\n            \"metadata\": {{\n                \"title\": \"the MAIN document title (not technical headers)\",\n                \"type\": \"guideline|regulation|directive|report|procedure|standard|manufacturer|certificate|authorization|specifications|other\",\n                \"publication_date\": \"exact date (DD Month YYYY format)\",\n                \"version\": \"document version/reference number\",\n                \"source\": \"organization name\",\n                \"context\": \"main domain (pharmaceutical, medical, legal, etc.)\",\n                \"country\": \"country code (EU, US, FR, IE, PL, etc.)\",\n                \"language\": \"language code (en, fr, etc.)\"\n            }},\n            \"confidence_scores\": {{\n                \"title\": 85,\n                \"type\": 90,\n                \"publication_date\": 95,\n                \"version\": 20,\n                \"source\": 90,\n                \"context\": 80,\n                \"country\": 95,\n                \"language\": 98\n            }},\n            \"extraction_reasoning\": {{\n                \"title\": \"Found clear title in document header\",\n                \"type\": \"Document explicitly mentions 'guideline' in title\",\n                \"publication_date\": \"Detected header date indicating publication\",\n                \"version\": \"No clear version number found\",\n                \"source\": \"URL or internal org name used\",\n                \"context\": \"Multiple pharmaceutical terms detected\",\n                \"country\": \"URL domain or regulatory hints used\",\n                \"language\": \"Detected by language model\"\n            }}\n        }}\n\n        INSTRUCTIONS FOR DOCUMENT TYPE DETECTION:\n        - \"manufacturer\": Documents listing manufacturing sites, manufacturer information, production details\n        - \"specifications\": Documents detailing product specifications, technical specifications, drug product specs\n        - \"guideline\": Official guidance documents from regulatory agencies\n        - \"regulation\": Legal regulatory texts, laws, regulations\n        - \"directive\": EU directives and similar regulatory directives  \n        - \"report\": Assessment reports, evaluation reports, inspection reports\n        - \"procedure\": Standard operating procedures, process descriptions\n        - \"standard\": Technical standards, quality standards\n        - \"certificate\": Certificates, authorizations, approvals\n        - \"authorization\": Marketing authorizations, permits, licenses\n        - \"other\": Any other document type\n\n\n        RULES FOR PUBLICATION DATE DETECTION:\n\n        Use the date that appears:\n        - At the top near the agency name (e.g. \"EMA/CHMP/ICH/24235/2006 Corr.2 23 January 2025\" √¢‚Ä†‚Äô use 23 January 2025)\n        - In the format \"Published: ...\" or \"Publication date: ...\"\n        - Next to \"Official Journal of the European Union\" (this is official)\n\n        DO NOT use:\n        - \"Adopted by CHMP on √¢‚Ç¨¬¶\"\n        - \"Date of coming into effect √¢‚Ç¨¬¶\"\n        - \"Finalization date\"\n        - References to old laws like \"Directive 2008/57/EC of 15 July 2008\"\n\n        Examples:\n        √¢≈ì‚Ä¶ GOOD √¢‚Ä†‚Äô \"23 January 2025\" near header  \n        √¢≈ì‚Ä¶ GOOD √¢‚Ä†‚Äô \"Published in Official Journal: 2 August 2013\"  \n        √¢¬ù≈í BAD √¢‚Ä†‚Äô \"Directive 2008/123/EC of 24 November 2008\"  \n        √¢¬ù≈í BAD √¢‚Ä†‚Äô \"Date for coming into effect: 26 July 2023\"\n\n        Additional Notes:\n        - Prioritize clarity over completeness\n        - Return no extra text outside JSON\n\n        TYPE DETECTION KEYWORDS:\n        - \"SPECIFICATIONS OF\" √¢‚Ä†‚Äô type: \"specifications\"\n        - \"MANUFACTURERS\" √¢‚Ä†‚Äô type: \"manufacturer\"  \n        - \"GUIDELINE\" √¢‚Ä†‚Äô type: \"guideline\"\n        - \"PROCEDURE\" √¢‚Ä†‚Äô type: \"procedure\"\n        - Look for explicit type indicators in document titles and headers\n\n        INSTRUCTIONS FOR TITLE EXTRACTION:\n        - Extract the MAIN document title from the body content, not technical headers\n        - Ignore technical codes like \"DTC_S006597_PF_R_FAB_17319_EN_15.0\"\n        - Look for the primary subject/topic in the document body\n        - For manufacturer docs: \"MANUFACTURERS\" + product name + description\n        - Combine logical title elements: \"MANUFACTURERS S 6597 Film-coated tablet...\"\n        - Avoid including addresses, company names, or metadata in the title\n\n        INSTRUCTIONS FOR VERSION DETECTION:\n\n\n        1. If a regulatory code is present like:\n        - EMA/CHMP/ICH/xxxxx/yyyy\n        - ICH/yyyy/stepX\n        √¢‚Ä†‚Äô extract that full code as version\n\n        2. If \"Corr.2\", \"Rev.1\", \"(R1)\", etc. appears after the code or title √¢‚Ä†‚Äô append it\n\n        3. If \"Step 5\", \"Step 2b\", etc. appears near the header √¢‚Ä†‚Äô append it\n\n        4. Accept also formats like:\n        - M4Q(R2) √¢‚Ä†‚Äô version = \"M4Q(R2)\"\n        - CTD_QR_Rev.2 √¢‚Ä†‚Äô version = \"Rev.2\"\n        - EN_15.0 √¢‚Ä†‚Äô version = \"15.0\"\n\n        √¢‚Ç¨‚Äù EXAMPLES √¢‚Ç¨‚Äù\n        √¢≈ì‚Ä¶ EMA/CHMP/ICH/214732/2007 Step 5\n        √¢≈ì‚Ä¶ EMA/CHMP/ICH/24235/2006 Corr.2 Step 5\n        √¢≈ì‚Ä¶ M4Q(R2)\n        √¢≈ì‚Ä¶ Rev. 1\n        √¢≈ì‚Ä¶ Version 3.0\n\n        √¢‚Ç¨‚Äù DO NOT EXTRACT √¢‚Ç¨‚Äù\n        √¢¬ù≈í Directive 2008/57/EC\n        √¢¬ù≈í IDs not linked to versioning\n\n\n        If no clear version is found, return an empty string: `\"\"`\n\n\n        INSTRUCTIONS FOR SOURCE DETECTION:\n        - Look for explicit organization names in the document\n        - Check for regulatory agency indicators (EMA, FDA, ANSM, MHRA, etc.)\n        - For manufacturer documents, use the company name mentioned\n        - For specifications documents, look for regulatory authority context\n        - Check for \"European Pharmacopoeia\" √¢‚Ä†‚Äô likely EMA/European regulatory context\n        - If from URL domain: .ema.europa.eu = \"EMA\", .fda.gov = \"FDA\", etc.\n        - If unclear, analyze content to determine likely source organization\n        - Do NOT default to EMA unless clearly an EMA document\n\n        INSTRUCTIONS FOR COUNTRY DETECTION:\n        - Look for country-specific indicators in text\n        - Check addresses, phone numbers, regulatory codes\n        - Analyze URL domain for country hints\n        - For manufacturer docs, use country where facilities are located\n        - Use specific country codes (FR, IE, PL) not just EU when possible\n\n        CONFIDENCE SCORING:\n        - 90-100: Explicitly stated in document\n        - 70-89: Strong indicators/evidence  \n        - 50-69: Reasonable inference from context\n        - 30-49: Weak indicators, uncertain\n        - 0-29: Not found or very unclear\n\n        TITLE EXTRACTION EXAMPLES:\n        √¢≈ì‚Ä¶ GOOD: \"MANUFACTURERS S 6597 Film-coated tablet containing 10 mg of perindopril arginine and 2.5 mg of indapamide\"\n        √¢≈ì‚Ä¶ GOOD: \"SPECIFICATIONS OF THE DRUG PRODUCT S 20098 Film-coated tablets containing 25 mg of drug substance\"\n        √¢¬ù≈í BAD: \"DTC_S006597_PF_R_FAB_17319_EN_15.0 MANUFACTURERS S 6597...\"\n        √¢¬ù≈í BAD: Including addresses or company details in title\n\n        Return ONLY the JSON, no other text.\n        \"\"\"\n\n        data = {\n            \"model\": \"mistral-large-latest\",\n            \"messages\": [{\"role\": \"user\",\n                          \"content\": prompt + \"\\n\\nReturn ONLY one JSON object or array. No markdown, no code fences, no explanations, no ellipses in values.\"}],\n            \"temperature\": 0.1,\n            \"max_tokens\": 1200\n        }\n\n        response = requests.post(url, headers=headers, json=data, timeout=30)\n        response.raise_for_status()\n\n        if response.status_code == 200:\n            result = response.json()\n            response_text = result['choices'][0]['message']['content']\n\n            # Try to robustly parse JSON from the response\n            def _sanitize_and_parse(candidate: str):\n                original = candidate\n                # Normalize smart quotes\n                candidate = candidate.replace('\\u201c', '\"').replace('\\u201d', '\"').replace('\\u2019', \"'\")\n                # Remove trailing commas before } or ]\n                candidate = re.sub(r',\\s*([}\\]])', r'\\1', candidate)\n                # Fix single-quoted keys: 'key': value -> \"key\": value\n                candidate = re.sub(r\"(?<=\\{|,)\\s*'([A-Za-z0-9_]+)'\\s*:\\s*\", r'\"\\1\": ', candidate)\n                # Fix single-quoted string values: key: 'value' -> key: \"value\"\n                candidate = re.sub(r\":\\s*'([^'\\\\]*(?:\\\\.[^'\\\\]*)*)'\", r': \"\\1\"', candidate)\n                # Fix unquoted keys only when at object boundaries\n                candidate = re.sub(r'(?<=\\{|,)\\s*([A-Za-z0-9_]+)\\s*:', r'\"\\1\":', candidate)\n                try:\n                    return json.loads(candidate)\n                except Exception:\n                    return None\n\n            def _extract_balanced_block(text: str):\n                # Support either object {..} or array [..]\n                start_obj = text.find('{')\n                start_arr = text.find('[')\n                starts = [s for s in [start_obj, start_arr] if s != -1]\n                if not starts:\n                    return None\n                start = min(starts)\n                opener = text[start]\n                closer = '}' if opener == '{' else ']'\n                i = start\n                depth = 0\n                in_str = False\n                esc = False\n                while i < len(text):\n                    ch = text[i]\n                    if in_str:\n                        if esc:\n                            esc = False\n                        elif ch == '\\\\':\n                            esc = True\n                        elif ch == '\"':\n                            in_str = False\n                    else:\n                        if ch == '\"':\n                            in_str = True\n                        elif ch == opener:\n                            depth += 1\n                        elif ch == closer:\n                            depth -= 1\n                            if depth == 0:\n                                return text[start:i + 1]\n                    i += 1\n                return None\n\n            def _try_parse_json(text: str):\n                # 1) Direct parse\n                try:\n                    return json.loads(text)\n                except Exception:\n                    pass\n                # 2) Fenced code block ```json:disable-run\n                m = re.search(r\"```(?:json|JSON)?\\s*([\\[{][\\s\\S]*?[\\]}])\\s*```\", text)\n                if m:\n                    fragment = m.group(1)\n                    try:\n                        return json.loads(fragment)\n                    except Exception:\n                        parsed = _sanitize_and_parse(fragment)\n                        if parsed is not None:\n                            return parsed\n                # 3) Balanced block extraction (object or array)\n                candidate = _extract_balanced_block(text)\n                if candidate:\n                    try:\n                        return json.loads(candidate)\n                    except Exception:\n                        parsed = _sanitize_and_parse(candidate)\n                        if parsed is not None:\n                            return parsed\n                return None\n\n            full_result = _try_parse_json(response_text)\n            if not full_result:\n                print(\"√¢¬ù≈í No valid JSON found or parse failed\")\n                return None\n\n            # Add quality metrics to metadata\n            if 'metadata' in full_result and 'confidence_scores' in full_result:\n                confidence_scores = full_result.get('confidence_scores', {})\n                overall_quality = calculate_overall_quality(confidence_scores)\n\n                full_result['metadata']['quality'] = {\n                    'extraction_rate': overall_quality,\n                    'field_scores': confidence_scores,\n                    'extraction_reasoning': full_result.get('extraction_reasoning', {}),\n                    'extracted_fields': len([v for v in full_result['metadata'].values() if v]),\n                    'total_fields': len(full_result['metadata']),\n                    'llm_powered': True\n                }\n\n            print(\"√¢≈ì‚Ä¶ Mistral extraction with confidence successful!\")\n            return full_result\n        else:\n            print(f\"√¢¬ù≈í Mistral API error: {response.status_code}\")\n            return None\n\n    except Exception as e:\n        print(f\"√¢¬ù≈í Mistral API error: {e}\")\n        return None\n\n\ndef calculate_overall_quality(confidence_scores):\n    \"\"\"\n    Calculate overall extraction quality from LLM confidence scores\n    \"\"\"\n    if not confidence_scores:\n        return 0\n\n    weights = {\n        'title': 1.5,\n        'type': 1.2,\n        'publication_date': 1.3,\n        'source': 1.2,\n        'context': 1.0,\n        'language': 0.8,\n        'country': 0.8,\n        'version': 0.7\n    }\n\n    total_weighted_score = 0\n    total_weight = 0\n    for field, score in confidence_scores.items():\n        w = weights.get(field, 1.0)\n        total_weighted_score += score * w\n        total_weight += w\n\n    return int(total_weighted_score / total_weight) if total_weight else 0\n\n\ndef extract_tables_from_pdf(file_path):\n    \"\"\"\n    Extrait tous les tableaux du PDF avec pdfplumber\n    \"\"\"\n    tables_data = []\n\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            for page_num, page in enumerate(pdf.pages, 1):\n                # Extraire les tableaux de la page\n                tables = page.extract_tables()\n\n                for table_num, table in enumerate(tables, 1):\n                    if table and len(table) > 1:  # Au moins un header + une ligne\n                        # Nettoyer le tableau (supprimer les cellules vides)\n                        cleaned_table = []\n                        for row in table:\n                            cleaned_row = [cell.strip() if cell else \"\" for cell in row]\n                            if any(cleaned_row):  # Garder seulement les lignes non vides\n                                cleaned_table.append(cleaned_row)\n\n                        if len(cleaned_table) > 1:\n                            tables_data.append({\n                                'page': page_num,\n                                'table_id': f\"table_{page_num}_{table_num}\",\n                                'headers': cleaned_table[0],\n                                'rows': cleaned_table[1:],\n                                'total_rows': len(cleaned_table) - 1,\n                                'total_columns': len(cleaned_table[0]) if cleaned_table else 0\n                            })\n\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur lors de l'extraction des tableaux: {e}\")\n\n    return tables_data\n\n\ndef call_llm_with_learned_prompt(prompt):\n    \"\"\"Call LLM with the adaptive prompt\"\"\"\n    try:\n        api_key = os.getenv(\"GROQ_API_KEY\") or \"your_actual_groq_api_key_here\"\n        client = Groq(api_key=api_key)\n\n        response = client.chat.completions.create(\n            messages=[{\n                \"role\": \"user\",\n                \"content\": prompt + \"\\n\\nReturn ONLY one JSON object or array. No markdown, no code fences, no explanations, no ellipses in values.\"\n            }],\n            model=\"llama3-8b-8192\",\n            temperature=0.1,\n            max_tokens=1500\n        )\n\n        result = response.choices[0].message.content\n\n        # Robust JSON parsing similar to Mistral path\n        def _sanitize_and_parse(candidate: str):\n            # Normalize smart quotes\n            candidate = candidate.replace('\\u201c', '\"').replace('\\u201d', '\"').replace('\\u2019', \"'\")\n            # Remove trailing commas before } or ]\n            candidate = re.sub(r',\\s*([}\\]])', r'\\1', candidate)\n            # Fix single-quoted keys: 'key': value -> \"key\": value\n            candidate = re.sub(r\"(?<=\\{|,)\\s*'([A-Za-z0-9_]+)'\\s*:\\s*\", r'\"\\1\": ', candidate)\n            # Fix single-quoted string values: key: 'value' -> key: \"value\"\n            candidate = re.sub(r\":\\s*'([^'\\\\]*(?:\\\\.[^'\\\\]*)*)'\", r': \"\\1\"', candidate)\n            # Fix unquoted keys only when at object boundaries\n            candidate = re.sub(r'(?<=\\{|,)\\s*([A-Za-z0-9_]+)\\s*:', r'\"\\1\":', candidate)\n            try:\n                return json.loads(candidate)\n            except Exception:\n                return None\n\n        def _extract_balanced_json(text: str):\n            start = text.find('{')\n            if start == -1:\n                return None\n            i = start\n            depth = 0\n            in_str = False\n            esc = False\n            while i < len(text):\n                ch = text[i]\n                if in_str:\n                    if esc:\n                        esc = False\n                    elif ch == '\\\\':\n                        esc = True\n                    elif ch == '\"':\n                        in_str = False\n                else:\n                    if ch == '\"':\n                        in_str = True\n                    elif ch == '{':\n                        depth += 1\n                    elif ch == '}':\n                        depth -= 1\n                        if depth == 0:\n                            return text[start:i + 1]\n                i += 1\n            return None\n\n        def _try_parse_json(text: str):\n            # 1) Direct parse\n            try:\n                return json.loads(text)\n            except Exception:\n                pass\n            # 2) Fenced code block ```json ... ``` or ``` ... ```\n            m = re.search(r\"```(?:json|JSON)?\\s*(\\{[\\s\\S]*?\\})\\s*```\", text)\n            if m:\n                try:\n                    return json.loads(m.group(1))\n                except Exception:\n                    parsed = _sanitize_and_parse(m.group(1))\n                    if parsed is not None:\n                        return parsed\n            # 3) Balanced-brace extraction\n            candidate = _extract_balanced_json(text)\n            if candidate:\n                try:\n                    return json.loads(candidate)\n                except Exception:\n                    parsed = _sanitize_and_parse(candidate)\n                    if parsed is not None:\n                        return parsed\n            return {}\n\n        return _try_parse_json(result)\n\n    except Exception as e:\n        print(f\"LLM call error: {e}\")\n        # Fallback to your existing Mistral extraction\n        return call_mistral_basic_extraction(prompt)\n\n\ndef call_mistral_basic_extraction(prompt):\n    \"\"\"Fallback to basic extraction if Groq fails\"\"\"\n    # Use your existing call_mistral_with_confidence logic here\n    # or return empty dict to use fallback\n    return {}\n\n\ndef extract_images_from_pdf(file_path):\n    \"\"\"\n    Extrait toutes les images du PDF avec PyMuPDF\n    \"\"\"\n    images_data = []\n\n    try:\n        pdf_document = fitz.open(file_path)\n\n        for page_num in range(len(pdf_document)):\n            page = pdf_document[page_num]\n            image_list = page.get_images()\n\n            for img_index, img in enumerate(image_list):\n                try:\n                    # R√É¬©cup√É¬©rer l'image\n                    xref = img[0]\n                    pix = fitz.Pixmap(pdf_document, xref)\n\n                    # Convertir en format utilisable\n                    if pix.n - pix.alpha < 4:  # GRAY ou RGB\n                        img_data = pix.tobytes(\"png\")\n\n                        # Cr√É¬©er un objet PIL pour obtenir les dimensions\n                        pil_image = Image.open(io.BytesIO(img_data))\n                        width, height = pil_image.size\n\n                        # Encoder en base64 pour stockage/affichage\n                        img_base64 = base64.b64encode(img_data).decode()\n\n                        # Cr√É¬©er une version preview (plus petite pour l'affichage dans la liste)\n                        preview_image = pil_image.copy()\n                        preview_image.thumbnail((300, 200), Image.Resampling.LANCZOS)\n                        preview_buffer = io.BytesIO()\n                        preview_image.save(preview_buffer, format='PNG')\n                        preview_base64 = base64.b64encode(preview_buffer.getvalue()).decode()\n\n                        images_data.append({\n                            'page': page_num + 1,\n                            'image_id': f\"img_{page_num + 1}_{img_index + 1}\",\n                            'width': width,\n                            'height': height,\n                            'format': 'PNG',\n                            'size_bytes': len(img_data),\n                            'base64_data': img_base64[:100] + \"...\" if len(img_base64) > 100 else img_base64,\n                            # Tronquer pour l'affichage\n                            'preview_base64': preview_base64,  # Version preview pour l'affichage\n                            'full_base64': img_base64  # Donn√É¬©es compl√É¬®tes\n                        })\n\n                    pix = None  # Lib√É¬©rer la m√É¬©moire\n\n                except Exception as e:\n                    print(f\"√¢¬ù≈í Erreur lors de l'extraction de l'image {img_index} page {page_num + 1}: {e}\")\n                    continue\n\n        pdf_document.close()\n\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur lors de l'extraction des images: {e}\")\n\n    return images_data\n\n\ndef extract_full_text(file_path):\n    \"\"\"\n    Extraction de texte utilisant directement pdfplumber\n    \"\"\"\n    # Utiliser directement le fallback pdfplumber qui est maintenant la m√É¬©thode principale\n    return _extract_full_text_fallback(file_path)\n\n\ndef _extract_full_text_fallback(file_path):\n    \"\"\"Fallback vers l'ancienne m√É¬©thode pdfplumber\"\"\"\n    text = \"\"\n    with pdfplumber.open(file_path) as pdf:\n        # extraire en priorit√É¬© les 1√É¬®res pages\n        for page in pdf.pages[:2]:\n            page_text = page.extract_text() or \"\"\n            text += \"\\n\" + page_text\n    # ensuite le reste si besoin\n    with pdfplumber.open(file_path) as pdf:\n        for page in pdf.pages[2:]:\n            text += \"\\n\" + (page.extract_text() or \"\")\n    return text\n\n\ndef extract_metadonnees(pdf_path, url=\"\"):\n    \"\"\"Extract metadata with fallback to working system\"\"\"\n\n    if url:\n        print(f\"√∞≈∏‚Äú¬° Utilisation du prompt LLM pour l'extraction des m√É¬©tadonn√É¬©es (URL: {url})\")\n        # First try the learning-enhanced extraction\n        try:\n            # Get learning insights\n            learning_prompts = get_learned_field_improvements()\n\n            # If we have learning data, use adaptive extraction\n            if learning_prompts:\n                text = extract_full_text(pdf_path)\n                if text:\n                    instructions = build_adaptive_field_instructions(learning_prompts)\n                    mistakes_prompt = get_common_mistakes_prompt(learning_prompts)\n\n                    prompt = f\"\"\"Extract metadata from this document:\n        {text[:2000]}\n\n        Fields to extract: {json.dumps(instructions, indent=2)}\n\n        Common mistakes to avoid: {mistakes_prompt}\n\n        Return ONLY JSON with keys: title, type, publication_date, version, source, context, country, language, url_source\"\"\"\n\n                    result = call_llm_with_learned_prompt(prompt)\n                    if result and isinstance(result, dict):\n                        return result\n\n        except Exception as e:\n            print(f\"Learning extraction failed: {e}\")\n\n        # Fallback to your working Mistral extraction\n        try:\n            result = call_mistral_with_confidence(extract_full_text(pdf_path), url, pdf_path)\n            if result and 'metadata' in result:\n                return result['metadata']\n        except Exception as e:\n            print(f\"Mistral extraction failed: {e}\")\n\n        # Final fallback\n        return extract_basic_fallback(pdf_path, url)\n    else:\n        return extract_metadata_local_pipeline(pdf_path)\n\n\ndef get_learned_field_improvements():\n    \"\"\"Get field-specific improvements from feedback data\"\"\"\n    try:\n        from .models import MetadataFeedback\n\n        field_insights = {}\n\n        for feedback in MetadataFeedback.objects.all():\n            corrections = feedback.corrections_made\n\n            # Analyze wrong fields\n            for wrong in corrections.get('corrected_fields', []):\n                field = wrong.get('field')\n                ai_value = wrong.get('ai_value', '')\n                correct_value = wrong.get('human_value', '')\n\n                if field not in field_insights:\n                    field_insights[field] = {'mistakes': [], 'patterns': []}\n\n                field_insights[field]['mistakes'].append({\n                    'wrong': ai_value,\n                    'correct': correct_value\n                })\n\n        return field_insights\n\n    except Exception as e:\n        print(f\"Learning insights error: {e}\")\n        return {}\n\n\ndef build_adaptive_field_instructions(learning_prompts):\n    \"\"\"Build field instructions that incorporate learning\"\"\"\n    instructions = {}\n\n    base_fields = {\n        'title': \"Document title\",\n        'type': \"Document type (guide, report, etc)\",\n        'publication_date': \"Publication date\",\n        'source': \"Source organization\",\n        'language': \"Document language code (en, fr, etc.)\",\n        'context': \"The field of the document content (pharmaceutical, legal, etc.) \",\n    }\n\n    for field, base_desc in base_fields.items():\n        if field in learning_prompts:\n            mistakes = learning_prompts[field]['mistakes']\n            if mistakes:\n                # Add specific guidance based on past mistakes\n                common_errors = [m['wrong'] for m in mistakes[-3:]]  # Last 3 mistakes\n                instructions[field] = f\"{base_desc}. AVOID these patterns: {', '.join(common_errors)}\"\n            else:\n                instructions[field] = base_desc\n        else:\n            instructions[field] = base_desc\n\n    return instructions\n\n\ndef get_common_mistakes_prompt(learning_prompts):\n    \"\"\"Generate prompt section about common mistakes\"\"\"\n    mistakes = []\n\n    for field, data in learning_prompts.items():\n        if data['mistakes']:\n            latest_mistakes = data['mistakes'][-2:]  # Last 2 mistakes\n            for mistake in latest_mistakes:\n                mistakes.append(f\"Don't confuse {field}: '{mistake['wrong']}' should be '{mistake['correct']}'\")\n\n    return \"\\n\".join(mistakes[:5])\n\n\ndef extract_basic_fallback(file_path: str, source_url: str) -> dict:\n    \"\"\"Basic fallback with honest low confidence scores\"\"\"\n    reader = PdfReader(file_path)\n    info = reader.metadata or {}\n    title = info.title or Path(file_path).stem\n    full_text = extract_full_text(file_path)\n\n    try:\n        lang = detect(full_text)\n    except:\n        lang = \"en\"\n\n    basic_meta = {\n        \"title\": title,\n        \"type\": \"unknown\",\n        \"publication_date\": \"\",\n        \"version\": \"\",\n        \"source\": \"\",\n        \"context\": \"\",\n        \"country\": \"\",\n        \"language\": lang,\n        \"url_source\": source_url\n    }\n\n    conf_scores = {\n        'title': 30 if title else 0,\n        'type': 0,\n        'publication_date': 0,\n        'version': 0,\n        'source': 0,\n        'context': 0,\n        'country': 0,\n        'language': 80 if lang else 0\n    }\n\n    overall_quality = calculate_overall_quality(conf_scores)\n\n    basic_meta['quality'] = {\n        'extraction_rate': overall_quality,\n        'field_scores': conf_scores,\n        'extraction_reasoning': {\n            'title': 'Basic PDF metadata extraction',\n            'type': 'Could not determine document type',\n            'source': 'No source identification possible'\n        },\n        'extracted_fields': 1,\n        'total_fields': len(conf_scores),\n        'llm_powered': False\n    }\n    return basic_meta",
                    "language": "python"
                }
            },
            "createdAt": 1760837923976,
            "isInternal": false,
            "rawContent": {
                "type": "doc",
                "content": [
                    {
                        "type": "paragraph",
                        "content": [
                            {
                                "type": "text",
                                "text": "En fait quand vous consultez utils py et que vous regardez l extraction Exiftool pour les documents upload√© pourquoi le titre du document s affiche pas et comment regler ca pas que le titre il y a d autres champs qui s affiche pas aussi "
                            }
                        ]
                    }
                ]
            },
            "autoAppliedRules": []
        },
        {
            "id": "fc5b0174-6954-4a4a-a45a-9efb2d10476a",
            "role": "assistant",
            "content": [
                {
                    "type": "thought",
                    "text": " to**Preparing check repository rules**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_BMp7tvPzGSFO6s6fcNVwWCKB",
                    "statusText": "Viewed directory",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "/",
                                "fsPath": "C:\\Users\\lasss\\PycharmProjects\\MyProjectIABlcokLabs"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_WFsDGApyslQbRFIiILQAv5FD",
                    "title": "",
                    "statusText": "File not found",
                    "status": "WARNING",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_92DL1uGdP5piUCvs8IZl0vtO",
                    "statusText": "Requirements collected",
                    "status": "SUCCESS",
                    "isHidden": false,
                    "localResult": "{\"answers\":{\"Je peux cr√©er automatiquement le fichier de r√®gles du d√©p√¥t pour am√©liorer l‚Äôassistance. Souhaitez-vous que je le g√©n√®re¬†?\":\"Non\"}}"
                },
                {
                    "type": "requirementsTool",
                    "id": "9a0dda0d-5da2-45a7-9dad-1f5c4c6047c7",
                    "questions": [
                        {
                            "question": "Je peux cr√©er automatiquement le fichier de r√®gles du d√©p√¥t pour am√©liorer l‚Äôassistance. Souhaitez-vous que je le g√©n√®re¬†?",
                            "options": [
                                "Oui",
                                "Non"
                            ]
                        }
                    ],
                    "toolCallId": "call_92DL1uGdP5piUCvs8IZl0vtO",
                    "answers": {
                        "Je peux cr√©er automatiquement le fichier de r√®gles du d√©p√¥t pour am√©liorer l‚Äôassistance. Souhaitez-vous que je le g√©n√®re¬†?": "Non"
                    },
                    "status": "SUCCESS"
                }
            ],
            "context": {
                "author": "Code agent",
                "operationId": "2fc77092-b2ae-4dd6-8562-ea79125b2869"
            },
            "createdAt": 1760838075018,
            "isInternal": false
        },
        {
            "id": "b1ba9471-8201-402c-9fd6-d5c73f734246",
            "role": "assistant",
            "content": [
                {
                    "type": "toolCall",
                    "toolCallId": "call_WEtHHliLIIL04RCz3hG3gG6o",
                    "statusText": "Viewed file (from the beginning up to line 400)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/utils.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/utils.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Investigating EXIF metadata handling**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_WkA0UN58JNZPds0jRiFMEDjK",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_5dj2LdmokgrMZH6hkiSDn5El",
                    "title": "File search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "text",
                    "text": "# Analyse du souci d‚Äôextraction EXIF\n\n## Constats √† partir de `rawdocs/utils.py`\n\n1. **Extraction brute**  \n   La fonction `extract_exif_metadata(file_path)` ex√©cute simplement :\n   ```python\n   result = subprocess.run(\n       [\"exiftool\", \"-json\", file_path],\n       stdout=subprocess.PIPE,\n       stderr=subprocess.PIPE,\n       text=True\n   )\n   metadata = json.loads(result.stdout)[0]\n   ```\n   Elle renvoie un dictionnaire brut. Aucun champ n‚Äôest filtr√© ni renomm√©.\n\n2. **Traitement en aval**  \n   Les champs EXIF ne sont exploit√©s nulle part ensuite. En particulier, aucune logique ne transf√®re `metadata.get(\"Title\")`, `metadata.get(\"DocumentName\")`, etc., vers les structures affich√©es par la page Upload/Edit.  \n   Sans mapping, le titre (et d‚Äôautres champs) restent `None` dans la base ou remplac√©s par des fallbacks (nom de fichier, `title_structured`, etc.).\n\n3. **Champs d√©pendants du type de fichier**  \n   - PDF‚ÄØ: ExifTool expose souvent `Title`, `Subject`, `Author`, `Keywords`, `CreateDate`, `ModifyDate`.  \n   - DOCX‚ÄØ: les propri√©t√©s sont plut√¥t `Title`, `Subject`, `Creator`, `LastModifiedBy`, `Created`, `Modified`.  \n   Des fichiers ‚Äúvierges‚Äù n‚Äô√©crivent tout simplement pas ces m√©tadonn√©es.\n\n## Pourquoi le titre ne remonte pas\n\n- **Absence de mapping** : aucune fonction ne copie le champ `Title` EXIF vers votre mod√®le `RawDocument` ou la r√©ponse JSON envoy√©e √† l‚ÄôUI.\n- **Nom de champ diff√©rent** : ExifTool peut fournir `Title` ou `DocumentName`. Si le mod√®le attend `title`, il faut un mapping explicite.\n- **M√©tadonn√©es inexistantes** : si le PDF n‚Äôa jamais eu de `Title`, le champ est vide malgr√© ExifTool.\n\n## Plan de correction\n\n1. **Cr√©er une fonction utilitaire pour normaliser les donn√©es**  \n   Exemple √† ajouter dans `rawdocs/utils.py` :\n\n   ```python\n   def normalize_exif_metadata(raw_meta):\n       \"\"\"Mappe les champs ExifTool vers des cl√©s normalis√©es\"\"\"\n       if not raw_meta:\n           return {}\n\n       normalized = {}\n\n       title = raw_meta.get(\"Title\") or raw_meta.get(\"DocumentName\")\n       if title:\n           normalized[\"title\"] = title.strip()\n\n       author = raw_meta.get(\"Author\") or raw_meta.get(\"Creator\")\n       if author:\n           normalized[\"author\"] = author.strip()\n\n       subject = raw_meta.get(\"Subject\")\n       if subject:\n           normalized[\"subject\"] = subject.strip()\n\n       keywords = raw_meta.get(\"Keywords\")\n       if keywords:\n           normalized[\"keywords\"] = [\n               kw.strip() for kw in keywords.split(\",\") if kw.strip()\n           ] if isinstance(keywords, str) else keywords\n\n       created = raw_meta.get(\"CreateDate\") or raw_meta.get(\"CreationDate\")\n       if created:\n           normalized[\"created\"] = created\n\n       modified = raw_meta.get(\"ModifyDate\") or raw_meta.get(\"ModDate\")\n       if modified:\n           normalized[\"modified\"] = modified\n\n       return normalized\n   ```\n\n2. **Appeler cette fonction l√† o√π l‚Äôon sauvegarde les docs**  \n   Dans la vue (probablement `rawdocs/views.py`), apr√®s `extract_exif_metadata` :\n   ```python\n   raw_exif = extract_exif_metadata(file_path)\n   exif_meta = normalize_exif_metadata(raw_exif)\n\n   document.title = exif_meta.get(\"title\") or document.title or fallback\n   document.author = exif_meta.get(\"author\") or document.author\n   # etc.\n   ```\n\n   Vous pouvez aussi stocker `raw_exif` pour audit, mais c‚Äôest `exif_meta` qui doit alimenter les champs affich√©s √† l‚Äôutilisateur.\n\n3. **Adapter l‚Äôinterface**  \n   V√©rifier que la r√©ponse JSON envoy√©e √† la page Upload inclut ces champs normalis√©s (ex. `data.doc.metadata.title`) pour qu‚Äôils apparaissent automatiquement.\n\n4. **Pr√©voir des fallbacks**  \n   Si `title` reste vide :\n   - Premier essai : `exif_meta.get(\"title\")`\n   - Sinon : `extract_structured_metadata(file_path)[\"title_structured\"]`\n   - Sinon : nom de fichier sans extension.\n\n## Tests recommand√©s\n\n1. Uploader un PDF contenant bien les m√©tadonn√©es (via Adobe ou ExifTool).\n2. V√©rifier dans la console Django ce qui est logg√© par `print(f\"‚úÖ ExifTool a extrait ‚Ä¶\")`.\n3. Confirmer que `normalize_exif_metadata` renvoie les champs attendus.\n4. Contr√¥ler que l‚ÄôUI affiche d√©sormais le titre/author.\n\n---\n\nSouhaitez-vous que je pr√©pare les modifications (normalisation + int√©gration dans la vue)‚ÄØ?"
                }
            ],
            "context": {
                "author": "Code agent",
                "operationId": "159a5805-dec8-45f4-a139-11dda153acaf",
                "usageStatistics": {
                    "llmCallsUsedForAnswer": 7.0,
                    "llmCallsUsedTotal": 7.0,
                    "llmCallsLimit": 30.0,
                    "limitRenewalMillisecondTimestamp": 1760924472438
                }
            },
            "createdAt": 1760838125742,
            "isInternal": false
        },
        {
            "id": "2ebd542c-200f-48b3-a1e4-ce307092bbd8",
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Oui faites les preparations necessaires pour regler ce probleme de mapping pour que les autres champs s affichent"
                }
            ],
            "context": {
                "currentFile": {
                    "path": "rawdocs/utils.py",
                    "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/utils.py",
                    "sources": [
                        "opened_tab"
                    ],
                    "content": "# rawdocs/utils.py\nimport os\nimport re\nimport json\nimport requests\nfrom pathlib import Path\nfrom datetime import datetime\nfrom urllib.parse import urlparse\nimport json\nimport requests\nfrom groq import Groq\nimport dateparser\n\nimport spacy\nfrom PyPDF2 import PdfReader\nfrom langdetect import detect\nfrom dotenv import load_dotenv\n\nimport subprocess\nimport json\nfrom pdfminer.high_level import extract_text\nfrom langdetect import detect\nimport spacy\nfrom pathlib import Path\nfrom docx import Document\nimport pdfplumber\nimport fitz  # PyMuPDF pour l'extraction d'images\nfrom PIL import Image\nimport io\nimport base64\n\n# Charger les variables d'environnement depuis .env\nload_dotenv()\n\n# Chargez une fois les mod√É¬®les spaCy :\nNLP_FR = spacy.load(\"fr_core_news_sm\")\nNLP_EN = spacy.load(\"en_core_web_sm\")\n\n# Stopwords FR/EN basiques pour nettoyage\nSTOPWORDS = {\n    \"le\", \"la\", \"les\", \"de\", \"des\", \"du\", \"un\", \"une\", \"et\", \"en\", \"√É \", \"dans\", \"que\", \"qui\",\n    \"pour\", \"par\", \"sur\", \"avec\", \"au\", \"aux\", \"ce\", \"ces\", \"se\", \"ses\", \"est\",\n    \"the\", \"and\", \"of\", \"to\", \"in\", \"that\", \"it\", \"is\", \"was\", \"for\", \"on\", \"are\", \"with\",\n    \"as\", \"i\", \"at\", \"be\", \"by\", \"this\"\n}\n\n\ndef extract_exif_metadata(file_path):\n    \"\"\"Extraction brute avec ExifTool\"\"\"\n    print(f\"√∞≈∏‚Äù¬ç Tentative d'extraction ExifTool pour {file_path}\")\n    try:\n        result = subprocess.run(\n            [\"exiftool\", \"-json\", file_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        if result.returncode != 0:\n            print(f\"√¢¬ù≈í Erreur ExifTool: {result.stderr}\")\n            return {}\n        metadata = json.loads(result.stdout)[0]\n        print(f\"√¢≈ì‚Ä¶ ExifTool a extrait {len(metadata)} champs: {list(metadata.keys())}\")\n        return metadata\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur ExifTool: {e}\")\n        print(f\"PATH actuel: {os.environ['PATH']}\")\n        return {}\n\n\ndef extract_text_content(file_path):\n    \"\"\"Extraction du texte brut selon le type de fichier\"\"\"\n    ext = Path(file_path).suffix.lower()\n    text = \"\"\n    print(f\"√∞≈∏‚Äù¬ç D√É¬©but de l'extraction texte pour {file_path} (type: {ext})\")\n    if ext == \".pdf\":\n        try:\n            text = extract_text(file_path)\n            print(f\"√¢≈ì‚Ä¶ Texte extrait avec succ√É¬®s ({len(text)} caract√É¬®res): {text[:100]}...\")\n        except Exception as e:\n            print(f\"√¢¬ù≈í Erreur extraction texte PDF: {e}\")\n    elif ext in [\".docx\"]:\n        try:\n            doc = Document(file_path)\n            text = \"\\n\".join([p.text for p in doc.paragraphs])\n            print(f\"√¢≈ì‚Ä¶ Texte extrait avec succ√É¬®s ({len(text)} caract√É¬®res)\")\n        except Exception as e:\n            print(f\"√¢¬ù≈í Erreur extraction texte DOCX: {e}\")\n    return text\n\n\ndef enhanced_nlp_analysis(text):\n    \"\"\"Analyse NLP renforc√©e sans LLM\"\"\"\n    meta = {}\n    if not text.strip():\n        print(\"‚ö†Ô∏è Aucun texte √† analyser pour NLP\")\n        return meta\n\n    print(\"üîç Lancement de l'analyse NLP renforc√©e\")\n\n    # D√âTECTION DU TYPE DE DOCUMENT\n    doc_types = {\n        'guideline': ['guideline', 'guidance', 'recommendations', 'best practices', 'guide', 'notice', 'instructions',\n                      'ligne directrice', 'recommandations'],\n        'regulation': ['regulation', 'directive', 'law', 'legal', 'article', 'decree', 'r√®glement', 'directive'],\n        'report': ['report', 'assessment', 'evaluation', 'analysis', 'study', 'rapport', '√©valuation'],\n        'procedure': ['procedure', 'protocol', 'method', 'process', 'operating', 'utilisation', 'pr√©cautions',\n                      'proc√©dure', 'protocole'],\n        'specifications': ['specifications', 'specs', 'requirements', 'criteria', 'specification', 'sp√©cifications',\n                           'exigences'],\n        'manufacturer': ['manufacturer', 'manufacturing', 'production', 'facility', 'plant', 'fabricant',\n                         'manufacturier'],\n        'certificate': ['certificate', 'certification', 'approval', 'authorization', 'certificat', 'approbation'],\n        'standard': ['standard', 'norm', 'iso', 'quality standard', 'norme', 'standard qualit√©']\n    }\n\n    text_lower = text.lower()\n    for doc_type, keywords in doc_types.items():\n        if any(keyword in text_lower for keyword in keywords):\n            meta['type'] = {\n                'value': doc_type,\n                'confidence': 85,\n                'method': 'nlp_keyword_match'\n            }\n            print(f\"‚úÖ Type d√©tect√©: {doc_type}\")\n            break\n\n    # EXTRACTION DE DATES AM√âLIOR√âE AVEC PRIORIT√â SUR LES LABELS EXPLICITES\n    date_candidates = []\n\n    # D√©tection de langue\n    language_info = detect_document_language(text)\n    lang = language_info.get('value', 'en') if language_info else 'en'\n    print(f\"Langue d√©tect√©e pour dates: {lang}\")\n\n    # D√©finir les noms de mois par langue\n    month_names_by_lang = {\n        'en': \"(?:January|February|March|April|May|June|July|August|September|October|November|December)\",\n        'fr': \"(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\"\n    }\n    month_names = month_names_by_lang.get(lang, month_names_by_lang['en'])\n\n    # Pattern 1: Label explicite \"Date de publication:\" (priorit√© maximale)\n    pub_label_pattern = rf'(?:Date\\s+de\\s+publication|Published\\s+on)[:\\s\\|]+(\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}})'\n    for match in re.finditer(pub_label_pattern, text, re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date:\n            normalized_date = parsed_date.strftime('%d %B %Y')\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': 99,  # Priorit√© maximale pour labels explicites\n                'context': 'publication_date_label',\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e (Publication label): {date_str} ‚Üí {normalized_date}\")\n\n    # Pattern 2: Date dans en-t√™te avec mois/ann√©e (format EMA typique)\n    header_date_pattern = rf'^([A-Z][a-z]+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})'\n    for match in re.finditer(header_date_pattern, text, re.MULTILINE | re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date and not any(\n                c['context'] == 'publication_date_label' for c in date_candidates):  # √âviter si label trouv√©\n            normalized_date = parsed_date.strftime('1 %B %Y')\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': 93,\n                'context': 'header_date',\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e (en-t√™te): {date_str} ‚Üí {normalized_date}\")\n\n    # Pattern 3: \"Step X dated DD Month YYYY\" (priorit√© √©lev√©e)\n    step_dated_pattern = rf'Step\\s+\\d+[a-z]?\\s+(?:dated\\s+)?(\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}})'\n    for match in re.finditer(step_dated_pattern, text, re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date and not any(c['context'] == 'publication_date_label' for c in date_candidates):\n            normalized_date = parsed_date.strftime('%d %B %Y')\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': 98,\n                'context': 'step_dated',\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e (Step dated): {date_str}\")\n\n    # Pattern 4: \"Final adoption by CHMP\" (priorit√© haute)\n    adoption_patterns = [\n        (rf'Final\\s+adoption\\s+by\\s+CHMP[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 92),\n        (rf'Final\\s+adoption[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 90),\n        (rf'Adopted\\s+by\\s+CHMP[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 85),\n        (rf'Adoption\\s+finale\\s+par\\s+CHMP[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 92)\n    ]\n    for pattern, priority in adoption_patterns:\n        for match in re.finditer(pattern, text, re.IGNORECASE):\n            date_str = match.group(1)\n            parsed_date = dateparser.parse(date_str, languages=[lang])\n            if parsed_date and not any(c['context'] == 'publication_date_label' for c in date_candidates):\n                normalized_date = parsed_date.strftime('1 %B %Y')\n                date_candidates.append({\n                    'date': normalized_date,\n                    'priority': priority,\n                    'context': 'final_adoption',\n                    'raw': date_str\n                })\n                print(f\"üìÖ Date trouv√©e (Final adoption): {date_str} ‚Üí {normalized_date}\")\n\n    # Pattern 5: Format DD Month YYYY (g√©n√©rique)\n    generic_date_pattern = rf'\\b(\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}})\\b'\n    for match in re.finditer(generic_date_pattern, text, re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date and not any(c['context'] == 'publication_date_label' for c in date_candidates):\n            normalized_date = parsed_date.strftime('%d %B %Y')\n            start_pos = max(0, match.start() - 100)\n            end_pos = min(len(text), match.end() + 100)\n            context = text[start_pos:end_pos].lower()\n            if 'coming into effect' in context or 'entr√©e en vigueur' in context or 'deadline' in context:\n                priority = 50\n                ctx_type = 'effective_date'\n            elif 'published' in context or 'publication' in context or 'publi√©' in context:\n                priority = 94\n                ctx_type = 'publication_date'\n            elif 'adopted' in context or 'adoption' in context or 'adopt√©' in context:\n                priority = 90\n                ctx_type = 'adoption_date'\n            else:\n                priority = 75\n                ctx_type = 'generic_date'\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': priority,\n                'context': ctx_type,\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e ({ctx_type}): {date_str}\")\n\n    # Fallback avec dateparser sur candidats bruts\n    raw_candidates = re.findall(rf'\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}}|\\d{{1,2}}\\.\\d{{1,2}}\\.\\d{{4}}', text,\n                                re.IGNORECASE)\n    for raw in raw_candidates:\n        parsed = dateparser.parse(raw, languages=[lang, 'en'])\n        if parsed and not any(c['raw'] == raw for c in date_candidates):\n            normalized = parsed.strftime('%d %B %Y')\n            if 'directive' in text_lower or 'law' in text_lower or re.search(r'\\d{4}/\\d+/EC', text):\n                priority = 40\n            else:\n                priority = 70\n            date_candidates.append({\n                'date': normalized,\n                'priority': priority,\n                'context': 'fallback_dateparser',\n                'raw': raw\n            })\n            print(f\"üìÖ Date fallback (dateparser): {raw} ‚Üí {normalized}\")\n\n    # S√©lection de la meilleure date (priorit√© sur labels explicites)\n    if date_candidates:\n        date_candidates.sort(key=lambda x: (-x['priority'], datetime.strptime(x['date'], '%d %B %Y') if '%d' in x[\n            'date'] else datetime.min), reverse=True)\n        selected_date = date_candidates[0]['date']\n        meta['publication_date'] = {\n            'value': selected_date,\n            'confidence': date_candidates[0]['priority'],\n            'context': date_candidates[0]['context'],\n            'method': 'nlp_pattern_match_multilingual'\n        }\n        print(\n            f\"‚úÖ Date de publication s√©lectionn√©e: {selected_date} (priorit√© {date_candidates[0]['priority']}, contexte: {date_candidates[0]['context']})\")\n\n    # EXTRACTION DE VERSION\n    version_patterns = [\n        r'version\\s*[:\\-]?\\s*(\\d+\\.?\\d*)',\n        r'v\\.?\\s*(\\d+\\.?\\d*)',\n        r'rev\\.?\\s*(\\d+)',\n        r'r(\\d+)',\n        r'step\\s*(\\d+)',\n        r'corr\\.?\\s*(\\d+)',\n        r'EN_(\\d+\\.?\\d*)',\n        r'_(\\d+\\.?\\d*)\\.pdf'\n    ]\n    for pattern in version_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            meta['version'] = {\n                'value': match.group(1),\n                'confidence': 80,\n                'method': 'regex_version'\n            }\n            print(f\"‚úÖ Version d√©tect√©e: {match.group(1)}\")\n            break\n\n    return meta\n\n\ndef extract_structured_metadata(file_path):\n    \"\"\"Extraction bas√É¬©e sur la structure du document\"\"\"\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            if not pdf.pages:\n                print(\"√¢≈° √Ø¬∏¬è Aucune page trouv√É¬©e dans le PDF\")\n                return {'title_structured': '', 'header_info': ''}\n\n            first_page = pdf.pages[0]\n\n            # Extraire le texte avec position\n            text_objects = first_page.extract_words()\n\n            if not text_objects:\n                print(\"√¢≈° √Ø¬∏¬è Aucun texte extrait de la premi√É¬®re page\")\n                # Essayer un fallback avec pdfminer\n                try:\n                    text = extract_text(file_path)  # Utiliser pdfminer comme secours\n                    first_line = text.split('\\n')[0].strip() if text else ''\n                    return {\n                        'title_structured': first_line,\n                        'header_info': first_line\n                    }\n                except Exception as e:\n                    print(f\"√¢¬ù≈í Erreur fallback pdfminer: {e}\")\n                    return {'title_structured': '', 'header_info': ''}\n\n            # Analyser les en-t√É¬™tes (texte en haut de page)\n            header_text = []\n            for obj in text_objects:\n                if obj.get('top', float('inf')) < 100:  # 100 pixels du haut\n                    header_text.append(obj['text'])\n\n            header_string = ' '.join(header_text) if header_text else ''\n\n            # Extraire titre (g√É¬©n√É¬©ralement le texte le plus gros)\n            title_candidates = []\n            sizes = [obj.get('size', 0) for obj in text_objects if obj.get('size') is not None]\n\n            if not sizes:\n                print(\"√¢≈° √Ø¬∏¬è Aucune taille de texte trouv√É¬©e\")\n                return {\n                    'title_structured': header_string or Path(file_path).stem,\n                    'header_info': header_string\n                }\n\n            max_size = max(sizes)\n            for obj in text_objects:\n                if obj.get('size', 0) >= max_size - 2:  # Texte de taille importante\n                    title_candidates.append(obj['text'])\n\n            title = ' '.join(title_candidates[:15]) if title_candidates else header_string or Path(file_path).stem\n\n            return {\n                'title_structured': title,\n                'header_info': header_string\n            }\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur extraction structur√É¬©e: {e}\")\n        return {'title_structured': '', 'header_info': ''}\n\n\ndef extract_extended_pdf_properties(file_path):\n    \"\"\"Extraction propri√É¬©t√É¬©s PDF √É¬©tendues\"\"\"\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            metadata = pdf.metadata or {}\n\n            # Analyser les m√É¬©tadonn√É¬©es XMP si disponibles\n            xmp_metadata = {}\n            try:\n                if hasattr(pdf, 'doc') and hasattr(pdf.doc, 'xmp_metadata'):\n                    xmp = pdf.doc.xmp_metadata\n                    if xmp:\n                        xmp_metadata = {\n                            'creator': getattr(xmp, 'creator', ''),\n                            'description': getattr(xmp, 'description', ''),\n                            'subject': getattr(xmp, 'subject', ''),\n                            'keywords': getattr(xmp, 'keywords', '')\n                        }\n            except Exception:\n                pass\n\n            return {**metadata, **xmp_metadata}\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur propri√É¬©t√É¬©s PDF: {e}\")\n        return {}\n\n\ndef regex_metadata_extraction(text):\n    \"\"\"Extraction par regex sp√É¬©cialis√É¬©es pour documents r√É¬©glementaires\"\"\"\n    patterns = {\n        'ema_reference': r'EMA/[A-Z]+/[A-Z]+/\\d+/\\d+',\n        'ich_reference': r'ICH/[A-Z0-9]+/\\d+',\n        'step_version': r'Step\\s+(\\d+[a-z]?)',\n        'correction': r'Corr\\.?\\s*(\\d+)',\n        'european_directive': r'Directive\\s+(\\d{4}/\\d+/EC)',\n        'organization_regex': r'(European Medicines Agency|EMA|FDA|ANSM|MHRA)',\n        'effective_date': r'effective\\s+(?:from\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n        'published_date': r'published\\s+(?:on\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n        'adoption_date': r'adopted\\s+(?:on\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})'\n    }\n\n    extracted = {}\n    for key, pattern in patterns.items():\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            extracted[key] = match.group(1) if match.groups() else match.group(0)\n\n    return extracted\n\n\ndef merge_metadata_with_priority(metadata_sources):\n    \"\"\"Fusionner les m√É¬©tadonn√É¬©es avec priorit√É¬©\"\"\"\n    merged = {}\n\n    field_mapping = {\n        'title': ['Title', 'title', 'title_inferred', 'title_structured', 'Subject'],\n        'type': ['type', 'DM_Category', 'doc_type'],\n        'source': ['Author', 'Creator', 'organization', 'source_organization', 'ema_reference', 'organization_regex'],\n        'version': ['version', 'DM_Version', 'step_version', 'correction'],\n        'publication_date': ['CreateDate', 'publication_date', 'effective_date', 'published_date', 'date'],\n        'language': ['Language', 'language', 'DM_Language'],\n        'country': ['country'],\n        'context': ['DM_Category', 'DM_Path', 'context']\n    }\n\n    for field, possible_keys in field_mapping.items():\n        for source in metadata_sources:\n            for key in possible_keys:\n                if key in source and source[key] and not merged.get(field):\n                    merged[field] = source[key]\n                    break\n            if merged.get(field):\n                break\n\n    return merged\n\n\ndef infer_additional_metadata(text):\n    \"\"\"Analyse NLP du contenu pour d√É¬©duire d'autres m√É¬©tadonn√É¬©es (version am√É¬©lior√É¬©e)\"\"\"\n    return enhanced_nlp_analysis(text)\n\n\ndef detect_document_language(text, file_path=\"\"):\n    \"\"\"\n    D√©tecte la langue du document avec plusieurs m√©thodes et retourne un score de confiance\n    \"\"\"\n    if not text or len(text.strip()) < 50:\n        print(\"‚ö†Ô∏è Texte trop court pour d√©tecter la langue\")\n        return None\n\n    detected_langs = []\n\n    # M√©thode 1 : langdetect sur le texte complet\n    try:\n        lang_full = detect(text[:5000])  # Analyse des premiers 5000 caract√®res\n        detected_langs.append(('langdetect_full', lang_full, 90))\n        print(f\"‚úÖ Langue d√©tect√©e (texte complet): {lang_full}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Erreur langdetect (texte complet): {e}\")\n\n    # M√©thode 2 : langdetect sur plusieurs √©chantillons (plus fiable)\n    try:\n        samples = [\n            text[:1000],\n            text[len(text) // 2:len(text) // 2 + 1000] if len(text) > 2000 else text,\n            text[-1000:] if len(text) > 1000 else text\n        ]\n        lang_counts = {}\n        for sample in samples:\n            if len(sample.strip()) > 50:\n                try:\n                    lang = detect(sample)\n                    lang_counts[lang] = lang_counts.get(lang, 0) + 1\n                except:\n                    pass\n\n        if lang_counts:\n            # Langue la plus fr√©quente\n            most_common_lang = max(lang_counts, key=lang_counts.get)\n            confidence = 85 + (lang_counts[most_common_lang] * 5)  # Max 100\n            detected_langs.append(('langdetect_samples', most_common_lang, min(confidence, 98)))\n            print(\n                f\"‚úÖ Langue d√©tect√©e (√©chantillons multiples): {most_common_lang} (consensus: {lang_counts[most_common_lang]}/3)\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Erreur langdetect (√©chantillons): {e}\")\n\n    # M√©thode 3 : Analyse du nom de fichier\n    if file_path:\n        filename = Path(file_path).stem.upper()\n        lang_indicators = {\n            'EN': ['_EN_', '_EN.', 'ENGLISH', '_ENG_'],\n            'FR': ['_FR_', '_FR.', 'FRENCH', '_FRA_'],\n            'DE': ['_DE_', '_DE.', 'GERMAN', '_DEU_'],\n            'ES': ['_ES_', '_ES.', 'SPANISH', '_ESP_'],\n            'IT': ['_IT_', '_IT.', 'ITALIAN', '_ITA_'],\n            'PL': ['_PL_', '_PL.', 'POLISH', '_POL_']\n        }\n\n        for lang_code, indicators in lang_indicators.items():\n            if any(ind in filename for ind in indicators):\n                detected_langs.append(('filename', lang_code.lower(), 80))\n                print(f\"‚úÖ Langue d√©tect√©e (nom de fichier): {lang_code.lower()}\")\n                break\n\n    # M√©thode 4 : Analyse des mots-cl√©s de langue\n    text_sample = text[:3000].lower()\n    keyword_scores = {\n        'en': sum([\n            text_sample.count(' the ') * 2,\n            text_sample.count(' and '),\n            text_sample.count(' of '),\n            text_sample.count(' to '),\n            text_sample.count(' in '),\n            text_sample.count('guideline'),\n            text_sample.count('regulation')\n        ]),\n        'fr': sum([\n            text_sample.count(' le ') * 2,\n            text_sample.count(' la ') * 2,\n            text_sample.count(' les ') * 2,\n            text_sample.count(' de '),\n            text_sample.count(' du '),\n            text_sample.count(' des '),\n            text_sample.count('r√®glement'),\n            text_sample.count('directive')\n        ]),\n        'de': sum([\n            text_sample.count(' der ') * 2,\n            text_sample.count(' die ') * 2,\n            text_sample.count(' das ') * 2,\n            text_sample.count(' und '),\n            text_sample.count(' ist '),\n            text_sample.count('richtlinie')\n        ])\n    }\n\n    if keyword_scores:\n        best_lang = max(keyword_scores, key=keyword_scores.get)\n        if keyword_scores[best_lang] > 10:  # Seuil minimal\n            confidence = min(70 + (keyword_scores[best_lang] // 5), 85)\n            detected_langs.append(('keywords', best_lang, confidence))\n            print(f\"‚úÖ Langue d√©tect√©e (mots-cl√©s): {best_lang} (score: {keyword_scores[best_lang]})\")\n\n    # Consolidation : choisir la meilleure d√©tection\n    if not detected_langs:\n        print(\"‚ùå Aucune langue d√©tect√©e\")\n        return None\n\n    # Trier par confiance et choisir la meilleure\n    detected_langs.sort(key=lambda x: x[2], reverse=True)\n    best_method, best_lang, best_confidence = detected_langs[0]\n\n    # V√©rifier la coh√©rence entre m√©thodes\n    lang_votes = {}\n    for method, lang, conf in detected_langs:\n        lang_votes[lang] = lang_votes.get(lang, 0) + 1\n\n    # Si plusieurs m√©thodes sont d'accord, augmenter la confiance\n    if lang_votes.get(best_lang, 0) >= 2:\n        best_confidence = min(best_confidence + 10, 98)\n        print(f\"üéØ Consensus sur la langue: {best_lang} ({lang_votes[best_lang]} m√©thodes concordantes)\")\n\n    return {\n        'value': best_lang,\n        'confidence': best_confidence,\n        'method': f'language_detection_{best_method}'\n    }\n\n\ndef extract_metadata_local_pipeline(file_path):\n    \"\"\"Point d'entr√©e principal pour l'extraction locale\"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"üöÄ EXTRACTION DYNAMIQUE PROFESSIONNELLE\")\n    print(f\"üìÑ Fichier: {Path(file_path).name}\")\n    print(f\"{'=' * 80}\\n\")\n\n    # 1. Extraction du texte\n    full_text = extract_text_content(file_path)\n\n    # 1b. D√©tection de la langue (NOUVEAU)\n    detected_language = detect_document_language(full_text, file_path)\n\n    # 2. Analyse NLP renforc√©e comme source principale\n    nlp_meta = enhanced_nlp_analysis(full_text)\n\n    # 3. Extraction compl√©mentaire via Exif (si disponible)\n    exif_meta = extract_exif_metadata(file_path)\n\n    # 4. Normalisation : convertir toutes les donn√©es en format uniforme\n    normalized_meta = {}\n\n    # Traiter les donn√©es NLP\n    for field, value in nlp_meta.items():\n        if isinstance(value, dict) and 'value' in value:\n            # D√©j√† au bon format\n            normalized_meta[field] = value\n        elif value:  # Valeur simple (comme type='guideline')\n            # Convertir en format structur√©\n            normalized_meta[field] = {\n                'value': value,\n                'confidence': 80,  # Confidence par d√©faut pour d√©tection NLP\n                'method': 'nlp_analysis'\n            }\n\n    # Ajouter les donn√©es Exif si elles am√©liorent la qualit√©\n    if exif_meta:\n        # Title depuis Exif (si pas d√©j√† trouv√© avec meilleure confidence)\n        if 'Title' in exif_meta and exif_meta['Title']:\n            if 'title' not in normalized_meta or normalized_meta['title']['confidence'] < 75:\n                normalized_meta['title'] = {\n                    'value': exif_meta['Title'],\n                    'confidence': 75,\n                    'method': 'exif'\n                }\n\n        # Source depuis Author\n        if 'Author' in exif_meta and exif_meta['Author']:\n            if 'source' not in normalized_meta or normalized_meta['source']['confidence'] < 70:\n                normalized_meta['source'] = {\n                    'value': exif_meta['Author'],\n                    'confidence': 70,\n                    'method': 'exif'\n                }\n\n        # Language depuis Exif (priorit√© moyenne)\n        if 'Language' in exif_meta and exif_meta['Language']:\n            if 'language' not in normalized_meta or normalized_meta['language']['confidence'] < 75:\n                normalized_meta['language'] = {\n                    'value': exif_meta['Language'],\n                    'confidence': 75,\n                    'method': 'exif'\n                }\n\n    # Ajouter la langue d√©tect√©e (si pas de meilleure source)\n    if detected_language:\n        if 'language' not in normalized_meta or normalized_meta['language']['confidence'] < detected_language[\n            'confidence']:\n            normalized_meta['language'] = detected_language\n\n    # 5. Consolidation finale : extraire les valeurs et calculer les scores\n    final_metadata = {}\n    confidence_scores = {}\n\n    # Liste des champs attendus\n    expected_fields = ['title', 'type', 'publication_date', 'version', 'source', 'context', 'country', 'language',\n                       'url_source']\n\n    for field in expected_fields:\n        if field in normalized_meta:\n            meta_obj = normalized_meta[field]\n            final_metadata[field] = meta_obj['value']\n            confidence_scores[field] = meta_obj['confidence']\n            print(f\"   ‚Ä¢ {field}: {meta_obj['value']} (confiance: {meta_obj['confidence']}% - {meta_obj['method']})\")\n        else:\n            final_metadata[field] = \"\"\n            confidence_scores[field] = 0\n            print(f\"   ‚Ä¢ {field}: (non d√©tect√©)\")\n\n    # 6. Calcul de qualit√© globale\n    overall_quality = calculate_overall_quality(confidence_scores)\n\n    # 7. Construire l'objet quality avec tous les d√©tails\n    extraction_reasoning = {}\n    for field in expected_fields:\n        if field in normalized_meta:\n            method = normalized_meta[field]['method']\n            extraction_reasoning[field] = f\"Detected via {method}\"\n        else:\n            extraction_reasoning[field] = \"Not detected\"\n\n    final_metadata['quality'] = {\n        'extraction_rate': overall_quality,\n        'field_scores': confidence_scores,\n        'extraction_reasoning': extraction_reasoning,\n        'extracted_fields': sum(1 for v in final_metadata.values() if v and v != \"\"),\n        'total_fields': len(expected_fields),\n        'llm_powered': False\n    }\n\n    print(f\"\\n{'=' * 80}\")\n    print(f\"‚úÖ EXTRACTION TERMIN√âE\")\n    print(f\"üìä Taux de remplissage: {final_metadata['quality']['extraction_rate']}%\")\n    print(\n        f\"üìà Champs extraits: {final_metadata['quality']['extracted_fields']}/{final_metadata['quality']['total_fields']}\")\n    print(f\"{'=' * 80}\\n\")\n\n    return final_metadata\n\n\ndef call_mistral_with_confidence(text_chunk, document_url=\"\", filename=\"\"):\n    \"\"\"\n    Get extraction + confidence scores from Mistral in one call.\n    La cl√É¬© API est lue depuis la variable d'environnement MISTRAL_API_KEY.\n    \"\"\"\n    api_key = os.getenv(\n        \"MISTRAL_API_KEY\") or \"j2wOKpM86nlZhhlvkXXG7rFd4bhM4PN5\"  # Fallback √É  la cl√É¬© hardcod√É¬©e si .env non disponible\n\n    try:\n        url = \"https://api.mistral.ai/v1/chat/completions\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n\n        prompt = f\"\"\"\n        You are an expert document analyzer. Analyze this document and extract metadata with confidence scores.\n\n        DOCUMENT TEXT (first 5000 chars):\n        {text_chunk[:5000]}\n\n        SOURCE URL: {document_url}\n        FILENAME: {filename}\n\n        HINT: Look for version numbers in headers like \"DTC_S006597_PF_R_FAB_17319_EN_15.0\" where 15.0 is the version\n\n        TASK: Return ONLY a JSON object with extracted metadata AND your confidence scores:\n\n        {{\n            \"metadata\": {{\n                \"title\": \"the MAIN document title (not technical headers)\",\n                \"type\": \"guideline|regulation|directive|report|procedure|standard|manufacturer|certificate|authorization|specifications|other\",\n                \"publication_date\": \"exact date (DD Month YYYY format)\",\n                \"version\": \"document version/reference number\",\n                \"source\": \"organization name\",\n                \"context\": \"main domain (pharmaceutical, medical, legal, etc.)\",\n                \"country\": \"country code (EU, US, FR, IE, PL, etc.)\",\n                \"language\": \"language code (en, fr, etc.)\"\n            }},\n            \"confidence_scores\": {{\n                \"title\": 85,\n                \"type\": 90,\n                \"publication_date\": 95,\n                \"version\": 20,\n                \"source\": 90,\n                \"context\": 80,\n                \"country\": 95,\n                \"language\": 98\n            }},\n            \"extraction_reasoning\": {{\n                \"title\": \"Found clear title in document header\",\n                \"type\": \"Document explicitly mentions 'guideline' in title\",\n                \"publication_date\": \"Detected header date indicating publication\",\n                \"version\": \"No clear version number found\",\n                \"source\": \"URL or internal org name used\",\n                \"context\": \"Multiple pharmaceutical terms detected\",\n                \"country\": \"URL domain or regulatory hints used\",\n                \"language\": \"Detected by language model\"\n            }}\n        }}\n\n        INSTRUCTIONS FOR DOCUMENT TYPE DETECTION:\n        - \"manufacturer\": Documents listing manufacturing sites, manufacturer information, production details\n        - \"specifications\": Documents detailing product specifications, technical specifications, drug product specs\n        - \"guideline\": Official guidance documents from regulatory agencies\n        - \"regulation\": Legal regulatory texts, laws, regulations\n        - \"directive\": EU directives and similar regulatory directives  \n        - \"report\": Assessment reports, evaluation reports, inspection reports\n        - \"procedure\": Standard operating procedures, process descriptions\n        - \"standard\": Technical standards, quality standards\n        - \"certificate\": Certificates, authorizations, approvals\n        - \"authorization\": Marketing authorizations, permits, licenses\n        - \"other\": Any other document type\n\n\n        RULES FOR PUBLICATION DATE DETECTION:\n\n        Use the date that appears:\n        - At the top near the agency name (e.g. \"EMA/CHMP/ICH/24235/2006 Corr.2 23 January 2025\" √¢‚Ä†‚Äô use 23 January 2025)\n        - In the format \"Published: ...\" or \"Publication date: ...\"\n        - Next to \"Official Journal of the European Union\" (this is official)\n\n        DO NOT use:\n        - \"Adopted by CHMP on √¢‚Ç¨¬¶\"\n        - \"Date of coming into effect √¢‚Ç¨¬¶\"\n        - \"Finalization date\"\n        - References to old laws like \"Directive 2008/57/EC of 15 July 2008\"\n\n        Examples:\n        √¢≈ì‚Ä¶ GOOD √¢‚Ä†‚Äô \"23 January 2025\" near header  \n        √¢≈ì‚Ä¶ GOOD √¢‚Ä†‚Äô \"Published in Official Journal: 2 August 2013\"  \n        √¢¬ù≈í BAD √¢‚Ä†‚Äô \"Directive 2008/123/EC of 24 November 2008\"  \n        √¢¬ù≈í BAD √¢‚Ä†‚Äô \"Date for coming into effect: 26 July 2023\"\n\n        Additional Notes:\n        - Prioritize clarity over completeness\n        - Return no extra text outside JSON\n\n        TYPE DETECTION KEYWORDS:\n        - \"SPECIFICATIONS OF\" √¢‚Ä†‚Äô type: \"specifications\"\n        - \"MANUFACTURERS\" √¢‚Ä†‚Äô type: \"manufacturer\"  \n        - \"GUIDELINE\" √¢‚Ä†‚Äô type: \"guideline\"\n        - \"PROCEDURE\" √¢‚Ä†‚Äô type: \"procedure\"\n        - Look for explicit type indicators in document titles and headers\n\n        INSTRUCTIONS FOR TITLE EXTRACTION:\n        - Extract the MAIN document title from the body content, not technical headers\n        - Ignore technical codes like \"DTC_S006597_PF_R_FAB_17319_EN_15.0\"\n        - Look for the primary subject/topic in the document body\n        - For manufacturer docs: \"MANUFACTURERS\" + product name + description\n        - Combine logical title elements: \"MANUFACTURERS S 6597 Film-coated tablet...\"\n        - Avoid including addresses, company names, or metadata in the title\n\n        INSTRUCTIONS FOR VERSION DETECTION:\n\n\n        1. If a regulatory code is present like:\n        - EMA/CHMP/ICH/xxxxx/yyyy\n        - ICH/yyyy/stepX\n        √¢‚Ä†‚Äô extract that full code as version\n\n        2. If \"Corr.2\", \"Rev.1\", \"(R1)\", etc. appears after the code or title √¢‚Ä†‚Äô append it\n\n        3. If \"Step 5\", \"Step 2b\", etc. appears near the header √¢‚Ä†‚Äô append it\n\n        4. Accept also formats like:\n        - M4Q(R2) √¢‚Ä†‚Äô version = \"M4Q(R2)\"\n        - CTD_QR_Rev.2 √¢‚Ä†‚Äô version = \"Rev.2\"\n        - EN_15.0 √¢‚Ä†‚Äô version = \"15.0\"\n\n        √¢‚Ç¨‚Äù EXAMPLES √¢‚Ç¨‚Äù\n        √¢≈ì‚Ä¶ EMA/CHMP/ICH/214732/2007 Step 5\n        √¢≈ì‚Ä¶ EMA/CHMP/ICH/24235/2006 Corr.2 Step 5\n        √¢≈ì‚Ä¶ M4Q(R2)\n        √¢≈ì‚Ä¶ Rev. 1\n        √¢≈ì‚Ä¶ Version 3.0\n\n        √¢‚Ç¨‚Äù DO NOT EXTRACT √¢‚Ç¨‚Äù\n        √¢¬ù≈í Directive 2008/57/EC\n        √¢¬ù≈í IDs not linked to versioning\n\n\n        If no clear version is found, return an empty string: `\"\"`\n\n\n        INSTRUCTIONS FOR SOURCE DETECTION:\n        - Look for explicit organization names in the document\n        - Check for regulatory agency indicators (EMA, FDA, ANSM, MHRA, etc.)\n        - For manufacturer documents, use the company name mentioned\n        - For specifications documents, look for regulatory authority context\n        - Check for \"European Pharmacopoeia\" √¢‚Ä†‚Äô likely EMA/European regulatory context\n        - If from URL domain: .ema.europa.eu = \"EMA\", .fda.gov = \"FDA\", etc.\n        - If unclear, analyze content to determine likely source organization\n        - Do NOT default to EMA unless clearly an EMA document\n\n        INSTRUCTIONS FOR COUNTRY DETECTION:\n        - Look for country-specific indicators in text\n        - Check addresses, phone numbers, regulatory codes\n        - Analyze URL domain for country hints\n        - For manufacturer docs, use country where facilities are located\n        - Use specific country codes (FR, IE, PL) not just EU when possible\n\n        CONFIDENCE SCORING:\n        - 90-100: Explicitly stated in document\n        - 70-89: Strong indicators/evidence  \n        - 50-69: Reasonable inference from context\n        - 30-49: Weak indicators, uncertain\n        - 0-29: Not found or very unclear\n\n        TITLE EXTRACTION EXAMPLES:\n        √¢≈ì‚Ä¶ GOOD: \"MANUFACTURERS S 6597 Film-coated tablet containing 10 mg of perindopril arginine and 2.5 mg of indapamide\"\n        √¢≈ì‚Ä¶ GOOD: \"SPECIFICATIONS OF THE DRUG PRODUCT S 20098 Film-coated tablets containing 25 mg of drug substance\"\n        √¢¬ù≈í BAD: \"DTC_S006597_PF_R_FAB_17319_EN_15.0 MANUFACTURERS S 6597...\"\n        √¢¬ù≈í BAD: Including addresses or company details in title\n\n        Return ONLY the JSON, no other text.\n        \"\"\"\n\n        data = {\n            \"model\": \"mistral-large-latest\",\n            \"messages\": [{\"role\": \"user\",\n                          \"content\": prompt + \"\\n\\nReturn ONLY one JSON object or array. No markdown, no code fences, no explanations, no ellipses in values.\"}],\n            \"temperature\": 0.1,\n            \"max_tokens\": 1200\n        }\n\n        response = requests.post(url, headers=headers, json=data, timeout=30)\n        response.raise_for_status()\n\n        if response.status_code == 200:\n            result = response.json()\n            response_text = result['choices'][0]['message']['content']\n\n            # Try to robustly parse JSON from the response\n            def _sanitize_and_parse(candidate: str):\n                original = candidate\n                # Normalize smart quotes\n                candidate = candidate.replace('\\u201c', '\"').replace('\\u201d', '\"').replace('\\u2019', \"'\")\n                # Remove trailing commas before } or ]\n                candidate = re.sub(r',\\s*([}\\]])', r'\\1', candidate)\n                # Fix single-quoted keys: 'key': value -> \"key\": value\n                candidate = re.sub(r\"(?<=\\{|,)\\s*'([A-Za-z0-9_]+)'\\s*:\\s*\", r'\"\\1\": ', candidate)\n                # Fix single-quoted string values: key: 'value' -> key: \"value\"\n                candidate = re.sub(r\":\\s*'([^'\\\\]*(?:\\\\.[^'\\\\]*)*)'\", r': \"\\1\"', candidate)\n                # Fix unquoted keys only when at object boundaries\n                candidate = re.sub(r'(?<=\\{|,)\\s*([A-Za-z0-9_]+)\\s*:', r'\"\\1\":', candidate)\n                try:\n                    return json.loads(candidate)\n                except Exception:\n                    return None\n\n            def _extract_balanced_block(text: str):\n                # Support either object {..} or array [..]\n                start_obj = text.find('{')\n                start_arr = text.find('[')\n                starts = [s for s in [start_obj, start_arr] if s != -1]\n                if not starts:\n                    return None\n                start = min(starts)\n                opener = text[start]\n                closer = '}' if opener == '{' else ']'\n                i = start\n                depth = 0\n                in_str = False\n                esc = False\n                while i < len(text):\n                    ch = text[i]\n                    if in_str:\n                        if esc:\n                            esc = False\n                        elif ch == '\\\\':\n                            esc = True\n                        elif ch == '\"':\n                            in_str = False\n                    else:\n                        if ch == '\"':\n                            in_str = True\n                        elif ch == opener:\n                            depth += 1\n                        elif ch == closer:\n                            depth -= 1\n                            if depth == 0:\n                                return text[start:i + 1]\n                    i += 1\n                return None\n\n            def _try_parse_json(text: str):\n                # 1) Direct parse\n                try:\n                    return json.loads(text)\n                except Exception:\n                    pass\n                # 2) Fenced code block ```json:disable-run\n                m = re.search(r\"```(?:json|JSON)?\\s*([\\[{][\\s\\S]*?[\\]}])\\s*```\", text)\n                if m:\n                    fragment = m.group(1)\n                    try:\n                        return json.loads(fragment)\n                    except Exception:\n                        parsed = _sanitize_and_parse(fragment)\n                        if parsed is not None:\n                            return parsed\n                # 3) Balanced block extraction (object or array)\n                candidate = _extract_balanced_block(text)\n                if candidate:\n                    try:\n                        return json.loads(candidate)\n                    except Exception:\n                        parsed = _sanitize_and_parse(candidate)\n                        if parsed is not None:\n                            return parsed\n                return None\n\n            full_result = _try_parse_json(response_text)\n            if not full_result:\n                print(\"√¢¬ù≈í No valid JSON found or parse failed\")\n                return None\n\n            # Add quality metrics to metadata\n            if 'metadata' in full_result and 'confidence_scores' in full_result:\n                confidence_scores = full_result.get('confidence_scores', {})\n                overall_quality = calculate_overall_quality(confidence_scores)\n\n                full_result['metadata']['quality'] = {\n                    'extraction_rate': overall_quality,\n                    'field_scores': confidence_scores,\n                    'extraction_reasoning': full_result.get('extraction_reasoning', {}),\n                    'extracted_fields': len([v for v in full_result['metadata'].values() if v]),\n                    'total_fields': len(full_result['metadata']),\n                    'llm_powered': True\n                }\n\n            print(\"√¢≈ì‚Ä¶ Mistral extraction with confidence successful!\")\n            return full_result\n        else:\n            print(f\"√¢¬ù≈í Mistral API error: {response.status_code}\")\n            return None\n\n    except Exception as e:\n        print(f\"√¢¬ù≈í Mistral API error: {e}\")\n        return None\n\n\ndef calculate_overall_quality(confidence_scores):\n    \"\"\"\n    Calculate overall extraction quality from LLM confidence scores\n    \"\"\"\n    if not confidence_scores:\n        return 0\n\n    weights = {\n        'title': 1.5,\n        'type': 1.2,\n        'publication_date': 1.3,\n        'source': 1.2,\n        'context': 1.0,\n        'language': 0.8,\n        'country': 0.8,\n        'version': 0.7\n    }\n\n    total_weighted_score = 0\n    total_weight = 0\n    for field, score in confidence_scores.items():\n        w = weights.get(field, 1.0)\n        total_weighted_score += score * w\n        total_weight += w\n\n    return int(total_weighted_score / total_weight) if total_weight else 0\n\n\ndef extract_tables_from_pdf(file_path):\n    \"\"\"\n    Extrait tous les tableaux du PDF avec pdfplumber\n    \"\"\"\n    tables_data = []\n\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            for page_num, page in enumerate(pdf.pages, 1):\n                # Extraire les tableaux de la page\n                tables = page.extract_tables()\n\n                for table_num, table in enumerate(tables, 1):\n                    if table and len(table) > 1:  # Au moins un header + une ligne\n                        # Nettoyer le tableau (supprimer les cellules vides)\n                        cleaned_table = []\n                        for row in table:\n                            cleaned_row = [cell.strip() if cell else \"\" for cell in row]\n                            if any(cleaned_row):  # Garder seulement les lignes non vides\n                                cleaned_table.append(cleaned_row)\n\n                        if len(cleaned_table) > 1:\n                            tables_data.append({\n                                'page': page_num,\n                                'table_id': f\"table_{page_num}_{table_num}\",\n                                'headers': cleaned_table[0],\n                                'rows': cleaned_table[1:],\n                                'total_rows': len(cleaned_table) - 1,\n                                'total_columns': len(cleaned_table[0]) if cleaned_table else 0\n                            })\n\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur lors de l'extraction des tableaux: {e}\")\n\n    return tables_data\n\n\ndef call_llm_with_learned_prompt(prompt):\n    \"\"\"Call LLM with the adaptive prompt\"\"\"\n    try:\n        api_key = os.getenv(\"GROQ_API_KEY\") or \"your_actual_groq_api_key_here\"\n        client = Groq(api_key=api_key)\n\n        response = client.chat.completions.create(\n            messages=[{\n                \"role\": \"user\",\n                \"content\": prompt + \"\\n\\nReturn ONLY one JSON object or array. No markdown, no code fences, no explanations, no ellipses in values.\"\n            }],\n            model=\"llama3-8b-8192\",\n            temperature=0.1,\n            max_tokens=1500\n        )\n\n        result = response.choices[0].message.content\n\n        # Robust JSON parsing similar to Mistral path\n        def _sanitize_and_parse(candidate: str):\n            # Normalize smart quotes\n            candidate = candidate.replace('\\u201c', '\"').replace('\\u201d', '\"').replace('\\u2019', \"'\")\n            # Remove trailing commas before } or ]\n            candidate = re.sub(r',\\s*([}\\]])', r'\\1', candidate)\n            # Fix single-quoted keys: 'key': value -> \"key\": value\n            candidate = re.sub(r\"(?<=\\{|,)\\s*'([A-Za-z0-9_]+)'\\s*:\\s*\", r'\"\\1\": ', candidate)\n            # Fix single-quoted string values: key: 'value' -> key: \"value\"\n            candidate = re.sub(r\":\\s*'([^'\\\\]*(?:\\\\.[^'\\\\]*)*)'\", r': \"\\1\"', candidate)\n            # Fix unquoted keys only when at object boundaries\n            candidate = re.sub(r'(?<=\\{|,)\\s*([A-Za-z0-9_]+)\\s*:', r'\"\\1\":', candidate)\n            try:\n                return json.loads(candidate)\n            except Exception:\n                return None\n\n        def _extract_balanced_json(text: str):\n            start = text.find('{')\n            if start == -1:\n                return None\n            i = start\n            depth = 0\n            in_str = False\n            esc = False\n            while i < len(text):\n                ch = text[i]\n                if in_str:\n                    if esc:\n                        esc = False\n                    elif ch == '\\\\':\n                        esc = True\n                    elif ch == '\"':\n                        in_str = False\n                else:\n                    if ch == '\"':\n                        in_str = True\n                    elif ch == '{':\n                        depth += 1\n                    elif ch == '}':\n                        depth -= 1\n                        if depth == 0:\n                            return text[start:i + 1]\n                i += 1\n            return None\n\n        def _try_parse_json(text: str):\n            # 1) Direct parse\n            try:\n                return json.loads(text)\n            except Exception:\n                pass\n            # 2) Fenced code block ```json ... ``` or ``` ... ```\n            m = re.search(r\"```(?:json|JSON)?\\s*(\\{[\\s\\S]*?\\})\\s*```\", text)\n            if m:\n                try:\n                    return json.loads(m.group(1))\n                except Exception:\n                    parsed = _sanitize_and_parse(m.group(1))\n                    if parsed is not None:\n                        return parsed\n            # 3) Balanced-brace extraction\n            candidate = _extract_balanced_json(text)\n            if candidate:\n                try:\n                    return json.loads(candidate)\n                except Exception:\n                    parsed = _sanitize_and_parse(candidate)\n                    if parsed is not None:\n                        return parsed\n            return {}\n\n        return _try_parse_json(result)\n\n    except Exception as e:\n        print(f\"LLM call error: {e}\")\n        # Fallback to your existing Mistral extraction\n        return call_mistral_basic_extraction(prompt)\n\n\ndef call_mistral_basic_extraction(prompt):\n    \"\"\"Fallback to basic extraction if Groq fails\"\"\"\n    # Use your existing call_mistral_with_confidence logic here\n    # or return empty dict to use fallback\n    return {}\n\n\ndef extract_images_from_pdf(file_path):\n    \"\"\"\n    Extrait toutes les images du PDF avec PyMuPDF\n    \"\"\"\n    images_data = []\n\n    try:\n        pdf_document = fitz.open(file_path)\n\n        for page_num in range(len(pdf_document)):\n            page = pdf_document[page_num]\n            image_list = page.get_images()\n\n            for img_index, img in enumerate(image_list):\n                try:\n                    # R√É¬©cup√É¬©rer l'image\n                    xref = img[0]\n                    pix = fitz.Pixmap(pdf_document, xref)\n\n                    # Convertir en format utilisable\n                    if pix.n - pix.alpha < 4:  # GRAY ou RGB\n                        img_data = pix.tobytes(\"png\")\n\n                        # Cr√É¬©er un objet PIL pour obtenir les dimensions\n                        pil_image = Image.open(io.BytesIO(img_data))\n                        width, height = pil_image.size\n\n                        # Encoder en base64 pour stockage/affichage\n                        img_base64 = base64.b64encode(img_data).decode()\n\n                        # Cr√É¬©er une version preview (plus petite pour l'affichage dans la liste)\n                        preview_image = pil_image.copy()\n                        preview_image.thumbnail((300, 200), Image.Resampling.LANCZOS)\n                        preview_buffer = io.BytesIO()\n                        preview_image.save(preview_buffer, format='PNG')\n                        preview_base64 = base64.b64encode(preview_buffer.getvalue()).decode()\n\n                        images_data.append({\n                            'page': page_num + 1,\n                            'image_id': f\"img_{page_num + 1}_{img_index + 1}\",\n                            'width': width,\n                            'height': height,\n                            'format': 'PNG',\n                            'size_bytes': len(img_data),\n                            'base64_data': img_base64[:100] + \"...\" if len(img_base64) > 100 else img_base64,\n                            # Tronquer pour l'affichage\n                            'preview_base64': preview_base64,  # Version preview pour l'affichage\n                            'full_base64': img_base64  # Donn√É¬©es compl√É¬®tes\n                        })\n\n                    pix = None  # Lib√É¬©rer la m√É¬©moire\n\n                except Exception as e:\n                    print(f\"√¢¬ù≈í Erreur lors de l'extraction de l'image {img_index} page {page_num + 1}: {e}\")\n                    continue\n\n        pdf_document.close()\n\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur lors de l'extraction des images: {e}\")\n\n    return images_data\n\n\ndef extract_full_text(file_path):\n    \"\"\"\n    Extraction de texte utilisant directement pdfplumber\n    \"\"\"\n    # Utiliser directement le fallback pdfplumber qui est maintenant la m√É¬©thode principale\n    return _extract_full_text_fallback(file_path)\n\n\ndef _extract_full_text_fallback(file_path):\n    \"\"\"Fallback vers l'ancienne m√É¬©thode pdfplumber\"\"\"\n    text = \"\"\n    with pdfplumber.open(file_path) as pdf:\n        # extraire en priorit√É¬© les 1√É¬®res pages\n        for page in pdf.pages[:2]:\n            page_text = page.extract_text() or \"\"\n            text += \"\\n\" + page_text\n    # ensuite le reste si besoin\n    with pdfplumber.open(file_path) as pdf:\n        for page in pdf.pages[2:]:\n            text += \"\\n\" + (page.extract_text() or \"\")\n    return text\n\n\ndef extract_metadonnees(pdf_path, url=\"\"):\n    \"\"\"Extract metadata with fallback to working system\"\"\"\n\n    if url:\n        print(f\"√∞≈∏‚Äú¬° Utilisation du prompt LLM pour l'extraction des m√É¬©tadonn√É¬©es (URL: {url})\")\n        # First try the learning-enhanced extraction\n        try:\n            # Get learning insights\n            learning_prompts = get_learned_field_improvements()\n\n            # If we have learning data, use adaptive extraction\n            if learning_prompts:\n                text = extract_full_text(pdf_path)\n                if text:\n                    instructions = build_adaptive_field_instructions(learning_prompts)\n                    mistakes_prompt = get_common_mistakes_prompt(learning_prompts)\n\n                    prompt = f\"\"\"Extract metadata from this document:\n        {text[:2000]}\n\n        Fields to extract: {json.dumps(instructions, indent=2)}\n\n        Common mistakes to avoid: {mistakes_prompt}\n\n        Return ONLY JSON with keys: title, type, publication_date, version, source, context, country, language, url_source\"\"\"\n\n                    result = call_llm_with_learned_prompt(prompt)\n                    if result and isinstance(result, dict):\n                        return result\n\n        except Exception as e:\n            print(f\"Learning extraction failed: {e}\")\n\n        # Fallback to your working Mistral extraction\n        try:\n            result = call_mistral_with_confidence(extract_full_text(pdf_path), url, pdf_path)\n            if result and 'metadata' in result:\n                return result['metadata']\n        except Exception as e:\n            print(f\"Mistral extraction failed: {e}\")\n\n        # Final fallback\n        return extract_basic_fallback(pdf_path, url)\n    else:\n        return extract_metadata_local_pipeline(pdf_path)\n\n\ndef get_learned_field_improvements():\n    \"\"\"Get field-specific improvements from feedback data\"\"\"\n    try:\n        from .models import MetadataFeedback\n\n        field_insights = {}\n\n        for feedback in MetadataFeedback.objects.all():\n            corrections = feedback.corrections_made\n\n            # Analyze wrong fields\n            for wrong in corrections.get('corrected_fields', []):\n                field = wrong.get('field')\n                ai_value = wrong.get('ai_value', '')\n                correct_value = wrong.get('human_value', '')\n\n                if field not in field_insights:\n                    field_insights[field] = {'mistakes': [], 'patterns': []}\n\n                field_insights[field]['mistakes'].append({\n                    'wrong': ai_value,\n                    'correct': correct_value\n                })\n\n        return field_insights\n\n    except Exception as e:\n        print(f\"Learning insights error: {e}\")\n        return {}\n\n\ndef build_adaptive_field_instructions(learning_prompts):\n    \"\"\"Build field instructions that incorporate learning\"\"\"\n    instructions = {}\n\n    base_fields = {\n        'title': \"Document title\",\n        'type': \"Document type (guide, report, etc)\",\n        'publication_date': \"Publication date\",\n        'source': \"Source organization\",\n        'language': \"Document language code (en, fr, etc.)\",\n        'context': \"The field of the document content (pharmaceutical, legal, etc.) \",\n    }\n\n    for field, base_desc in base_fields.items():\n        if field in learning_prompts:\n            mistakes = learning_prompts[field]['mistakes']\n            if mistakes:\n                # Add specific guidance based on past mistakes\n                common_errors = [m['wrong'] for m in mistakes[-3:]]  # Last 3 mistakes\n                instructions[field] = f\"{base_desc}. AVOID these patterns: {', '.join(common_errors)}\"\n            else:\n                instructions[field] = base_desc\n        else:\n            instructions[field] = base_desc\n\n    return instructions\n\n\ndef get_common_mistakes_prompt(learning_prompts):\n    \"\"\"Generate prompt section about common mistakes\"\"\"\n    mistakes = []\n\n    for field, data in learning_prompts.items():\n        if data['mistakes']:\n            latest_mistakes = data['mistakes'][-2:]  # Last 2 mistakes\n            for mistake in latest_mistakes:\n                mistakes.append(f\"Don't confuse {field}: '{mistake['wrong']}' should be '{mistake['correct']}'\")\n\n    return \"\\n\".join(mistakes[:5])\n\n\ndef extract_basic_fallback(file_path: str, source_url: str) -> dict:\n    \"\"\"Basic fallback with honest low confidence scores\"\"\"\n    reader = PdfReader(file_path)\n    info = reader.metadata or {}\n    title = info.title or Path(file_path).stem\n    full_text = extract_full_text(file_path)\n\n    try:\n        lang = detect(full_text)\n    except:\n        lang = \"en\"\n\n    basic_meta = {\n        \"title\": title,\n        \"type\": \"unknown\",\n        \"publication_date\": \"\",\n        \"version\": \"\",\n        \"source\": \"\",\n        \"context\": \"\",\n        \"country\": \"\",\n        \"language\": lang,\n        \"url_source\": source_url\n    }\n\n    conf_scores = {\n        'title': 30 if title else 0,\n        'type': 0,\n        'publication_date': 0,\n        'version': 0,\n        'source': 0,\n        'context': 0,\n        'country': 0,\n        'language': 80 if lang else 0\n    }\n\n    overall_quality = calculate_overall_quality(conf_scores)\n\n    basic_meta['quality'] = {\n        'extraction_rate': overall_quality,\n        'field_scores': conf_scores,\n        'extraction_reasoning': {\n            'title': 'Basic PDF metadata extraction',\n            'type': 'Could not determine document type',\n            'source': 'No source identification possible'\n        },\n        'extracted_fields': 1,\n        'total_fields': len(conf_scores),\n        'llm_powered': False\n    }\n    return basic_meta",
                    "language": "python"
                }
            },
            "createdAt": 1760838438391,
            "isInternal": false,
            "rawContent": {
                "type": "doc",
                "content": [
                    {
                        "type": "paragraph",
                        "content": [
                            {
                                "type": "text",
                                "text": "Oui faites les preparations necessaires pour regler ce probleme de mapping pour que les autres champs s affichent "
                            }
                        ]
                    }
                ]
            },
            "autoAppliedRules": []
        },
        {
            "id": "889e3b54-5411-442f-bda3-d968fbfab4ef",
            "role": "assistant",
            "content": [
                {
                    "type": "thought",
                    "text": " modifications**Planning and searching usage**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_8mi9l84ErY30UTS8FE36kcds",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Reviewing code around line 585**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_fHPEv4xiZwCZuwGrDrMSjf88",
                    "statusText": "Viewed file (lines 560:679)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/utils.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/utils.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Designing EXIF metadata normalization**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_Xrax9uDWbt0WqVkLj0UnOaES",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_fh4FtolHTpO6UyzZMN3xQoIz",
                    "statusText": "Viewed file (from the beginning up to line 400)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/views.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/views.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_n4XGg5JfWp488VPDcRPPjdNp",
                    "statusText": "Viewed file (lines 400:599)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/views.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/views.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_EKn6aTNf4W9MCGuIOFhS8Fyq",
                    "statusText": "Requirements collected",
                    "status": "SUCCESS",
                    "isHidden": false,
                    "localResult": "{\"answers\":{\"Votre mod√®le `RawDocument` poss√®de-t-il d√©j√† des champs pour stocker le titre, auteur, sujet, mots-cl√©s, dates, etc. (par exemple `title`, `doc_type`, `source`, `keywords`, `created_date`...) ?\":\"Je ne sais pas\",\"Souhaitez-vous que nous gardions toutes les m√©tadonn√©es Exif brutes en plus des champs normalis√©s (par exemple dans un champ JSON) ?\":\"Oui\",\"Le formulaire / front doit-il afficher imm√©diatement les champs suppl√©mentaires (auteur, keywords...) ?\":\"Pas n√©cessaire pour l‚Äôinstant\"}}"
                },
                {
                    "type": "requirementsTool",
                    "id": "52be867a-2fb6-4284-aa3a-5839a1f32aca",
                    "questions": [
                        {
                            "question": "Votre mod√®le `RawDocument` poss√®de-t-il d√©j√† des champs pour stocker le titre, auteur, sujet, mots-cl√©s, dates, etc. (par exemple `title`, `doc_type`, `source`, `keywords`, `created_date`...) ?",
                            "options": [
                                "Oui, ils existent d√©j√†",
                                "Non, certains manquent",
                                "Je ne sais pas"
                            ]
                        },
                        {
                            "question": "Souhaitez-vous que nous gardions toutes les m√©tadonn√©es Exif brutes en plus des champs normalis√©s (par exemple dans un champ JSON) ?",
                            "options": [
                                "Oui",
                                "Non",
                                "Je ne sais pas"
                            ]
                        },
                        {
                            "question": "Le formulaire / front doit-il afficher imm√©diatement les champs suppl√©mentaires (auteur, keywords...) ?",
                            "options": [
                                "Oui",
                                "Non",
                                "Pas n√©cessaire pour l‚Äôinstant"
                            ]
                        }
                    ],
                    "toolCallId": "call_EKn6aTNf4W9MCGuIOFhS8Fyq",
                    "answers": {
                        "Votre mod√®le `RawDocument` poss√®de-t-il d√©j√† des champs pour stocker le titre, auteur, sujet, mots-cl√©s, dates, etc. (par exemple `title`, `doc_type`, `source`, `keywords`, `created_date`...) ?": "Je ne sais pas",
                        "Souhaitez-vous que nous gardions toutes les m√©tadonn√©es Exif brutes en plus des champs normalis√©s (par exemple dans un champ JSON) ?": "Oui",
                        "Le formulaire / front doit-il afficher imm√©diatement les champs suppl√©mentaires (auteur, keywords...) ?": "Pas n√©cessaire pour l‚Äôinstant"
                    },
                    "status": "SUCCESS"
                }
            ],
            "context": {
                "author": "Code agent",
                "operationId": "bcd7f2d8-0131-4657-8b6a-3b0b48d1f40a"
            },
            "createdAt": 1760838444460,
            "isInternal": false
        },
        {
            "id": "910f575e-4e9b-420b-a500-aa6c49d3e155",
            "role": "assistant",
            "content": [
                {
                    "type": "toolCall",
                    "toolCallId": "call_VkoUHM5P317bq8WDEhrNaxc8",
                    "title": "File search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_4NMYnxMap26QIiQxW1zr4eLJ",
                    "statusText": "Viewed file (from the beginning up to line 400)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/models.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/models.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_v9cYyJSzB9sPl96ooAYA91KL",
                    "statusText": "Viewed file (lines 600:799)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/views.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/views.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Planning metadata normalization**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_SOIeDA6YRPjcppOdTXDOXwDQ",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_M6lcyQwx33gZmpDp9ITtpya4",
                    "statusText": "Viewed file (lines 1200:1399)",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": "rawdocs/utils.py",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/utils.py"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Planning metadata normalization and integration**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_B6BZ5PAURtD5ALSdOkNjXKOG",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_ve4fz4O4KvnaojA5dQ6H5QBe",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_ufpu6QM9ce8od0iiE4OWWo7O",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_RdYu8lc82OVTz1hfoulwE5lj",
                    "statusText": "Requirements collected",
                    "status": "SUCCESS",
                    "isHidden": false,
                    "localResult": "{\"answers\":{\"Dans le mod√®le `RawDocument`, disposez-vous d√©j√† d‚Äôun champ JSON permettant de stocker les m√©tadonn√©es brutes (par exemple `original_exif_metadata`) ?\":\"Je ne sais pas\",\"Souhaitez-vous ajouter un tel champ JSON pour conserver l‚Äôint√©gralit√© des m√©tadonn√©es Exif ?\":\"Oui\"}}"
                },
                {
                    "type": "requirementsTool",
                    "id": "d7167e6e-7c0a-44c1-84c6-79e7acfe0e29",
                    "questions": [
                        {
                            "question": "Dans le mod√®le `RawDocument`, disposez-vous d√©j√† d‚Äôun champ JSON permettant de stocker les m√©tadonn√©es brutes (par exemple `original_exif_metadata`) ?",
                            "options": [
                                "Oui",
                                "Non",
                                "Je ne sais pas"
                            ]
                        },
                        {
                            "question": "Souhaitez-vous ajouter un tel champ JSON pour conserver l‚Äôint√©gralit√© des m√©tadonn√©es Exif ?",
                            "options": [
                                "Oui",
                                "Non",
                                "Je ne sais pas"
                            ]
                        }
                    ],
                    "toolCallId": "call_RdYu8lc82OVTz1hfoulwE5lj",
                    "answers": {
                        "Dans le mod√®le `RawDocument`, disposez-vous d√©j√† d‚Äôun champ JSON permettant de stocker les m√©tadonn√©es brutes (par exemple `original_exif_metadata`) ?": "Je ne sais pas",
                        "Souhaitez-vous ajouter un tel champ JSON pour conserver l‚Äôint√©gralit√© des m√©tadonn√©es Exif ?": "Oui"
                    },
                    "status": "SUCCESS"
                }
            ],
            "context": {
                "author": "Code agent",
                "operationId": "f791dc67-080a-4b18-94be-893170a46274"
            },
            "createdAt": 1760838571604,
            "isInternal": false
        },
        {
            "id": "50d55926-15f9-4f28-a43f-5eeffa0e45fa",
            "role": "assistant",
            "content": [
                {
                    "type": "thought",
                    "text": "**Searching for original_exif_metadata usage**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_FJuf1AL0UsiyqMoeCAoUXi7N",
                    "title": "Fulltext search",
                    "statusText": "Done",
                    "status": "SUCCESS",
                    "isHidden": false
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_sBRrluTjNaAoaDL0gESIA9jh",
                    "statusText": "Requirements collected",
                    "status": "SUCCESS",
                    "isHidden": false,
                    "localResult": "{\"answers\":{\"Souhaitez-vous que les champs normalis√©s incluent : titre, auteur, sujet, mots-cl√©s, dates (cr√©ation/modification), producteur ?\":\"Non\",\"Si vous choisissez d‚Äôajouter seulement certains, lesquels ?\":\"Date de cr√©ation\"}}"
                },
                {
                    "type": "requirementsTool",
                    "id": "920fbf3c-1e15-4d28-8590-540fe49a6c65",
                    "questions": [
                        {
                            "question": "Souhaitez-vous que les champs normalis√©s incluent : titre, auteur, sujet, mots-cl√©s, dates (cr√©ation/modification), producteur ?",
                            "options": [
                                "Oui",
                                "Non",
                                "Ajouter seulement certains"
                            ]
                        },
                        {
                            "question": "Si vous choisissez d‚Äôajouter seulement certains, lesquels ?",
                            "options": [
                                "Titre",
                                "Auteur",
                                "Sujet",
                                "Mots-cl√©s",
                                "Date de cr√©ation",
                                "Date de modification",
                                "Producteur"
                            ]
                        }
                    ],
                    "toolCallId": "call_sBRrluTjNaAoaDL0gESIA9jh",
                    "answers": {
                        "Souhaitez-vous que les champs normalis√©s incluent : titre, auteur, sujet, mots-cl√©s, dates (cr√©ation/modification), producteur ?": "Non",
                        "Si vous choisissez d‚Äôajouter seulement certains, lesquels ?": "Date de cr√©ation"
                    },
                    "status": "SUCCESS"
                }
            ],
            "context": {
                "author": "Code agent",
                "operationId": "879d4432-e24f-4371-ac7b-1ec6ee31da45"
            },
            "createdAt": 1760838662118,
            "isInternal": false
        },
        {
            "id": "dcab7c40-e075-4a6f-881b-c2c5438b69d1",
            "role": "assistant",
            "content": [
                {
                    "type": "toolCall",
                    "toolCallId": "call_pBNuNtEiAouBk7bR0qjGUAE9",
                    "statusText": "Requirements collected",
                    "status": "SUCCESS",
                    "isHidden": false,
                    "localResult": "{\"answers\":{\"Le code qui exploite ces m√©tadonn√©es attend-il que la date soit une cha√Æne (`CharField`) ou un objet `datetime` ?\":\"Je ne sais pas\"}}"
                },
                {
                    "type": "requirementsTool",
                    "id": "26b13ba9-a660-4142-8aca-d191136ea100",
                    "questions": [
                        {
                            "question": "Le code qui exploite ces m√©tadonn√©es attend-il que la date soit une cha√Æne (`CharField`) ou un objet `datetime` ?",
                            "options": [
                                "Cha√Æne de caract√®res",
                                "Objet datetime",
                                "Je ne sais pas"
                            ]
                        }
                    ],
                    "toolCallId": "call_pBNuNtEiAouBk7bR0qjGUAE9",
                    "answers": {
                        "Le code qui exploite ces m√©tadonn√©es attend-il que la date soit une cha√Æne (`CharField`) ou un objet `datetime` ?": "Je ne sais pas"
                    },
                    "status": "SUCCESS"
                }
            ],
            "context": {
                "author": "Code agent",
                "operationId": "64e01f1b-cfae-4332-8acf-c05294e1103c"
            },
            "createdAt": 1760838774534,
            "isInternal": false
        },
        {
            "id": "852dbbb7-2f4e-42bb-82bd-d2852b6455d2",
            "role": "assistant",
            "content": [
                {
                    "type": "text",
                    "text": "I‚Äôve reviewed the situation:\n\n- **A normalisation helper** like `normalize_exif_metadata` isn‚Äôt present yet.\n- The view currently stores only a limited subset of Exif data (title, author, language), leaving out other useful fields such as `CreateDate`.\n- The pipeline keeps only the final metadata; Exif output is not normalized or saved separately.\n- The front-end only shows the existing fields already stored in `RawDocument`.\n\nTo resolve this, we should:\n1. **Add a `normalize_exif_metadata` helper** in `rawdocs/utils.py` to map raw Exif fields (`Title`, `Author`, `CreateDate`, etc.) into normalized keys (`title`, `author`, `created_date`, etc.) and keep a cleaned `keywords` list.\n2. **Enhance the metadata pipeline** (`extract_metadata_local_pipeline`) to:\n   - Call `normalize_exif_metadata`.\n   - Merge normalized Exif fields into `final_metadata` when they add new information.\n   - Store a new JSON field (`raw_exif_metadata`) in `RawDocument` to preserve all raw Exif data (this field isn‚Äôt in the model yet‚Äîif you want it, we‚Äôll need to add it via a migration).\n3. **Update the upload view** so that the normalized Exif fields (title, source, language, created date) are saved together with the existing metadata.\n4. Optional: in the template, display `created_date` (or other new fields) if desired once the back-end stores them.\n\nLet me know if you‚Äôd like me to implement these changes step by step (adding the helper, updating the model, adjusting the view, etc.)."
                }
            ],
            "context": {
                "author": "Code agent",
                "operationId": "3e5de849-81de-4459-8b7c-89c447e78f4f",
                "usageStatistics": {
                    "llmCallsUsedForAnswer": 19.0,
                    "llmCallsUsedTotal": 26.0,
                    "llmCallsLimit": 30.0,
                    "limitRenewalMillisecondTimestamp": 1760924472502
                }
            },
            "createdAt": 1760838801076,
            "isInternal": false
        },
        {
            "id": "99d2e149-f962-4fd0-8a1d-6478312deee3",
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Yes"
                }
            ],
            "context": {
                "currentFile": {
                    "path": "rawdocs/utils.py",
                    "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/rawdocs/utils.py",
                    "sources": [
                        "opened_tab"
                    ],
                    "content": "# rawdocs/utils.py\nimport os\nimport re\nimport json\nimport requests\nfrom pathlib import Path\nfrom datetime import datetime\nfrom urllib.parse import urlparse\nimport json\nimport requests\nfrom groq import Groq\nimport dateparser\n\nimport spacy\nfrom PyPDF2 import PdfReader\nfrom langdetect import detect\nfrom dotenv import load_dotenv\n\nimport subprocess\nimport json\nfrom pdfminer.high_level import extract_text\nfrom langdetect import detect\nimport spacy\nfrom pathlib import Path\nfrom docx import Document\nimport pdfplumber\nimport fitz  # PyMuPDF pour l'extraction d'images\nfrom PIL import Image\nimport io\nimport base64\n\n# Charger les variables d'environnement depuis .env\nload_dotenv()\n\n# Chargez une fois les mod√É¬®les spaCy :\nNLP_FR = spacy.load(\"fr_core_news_sm\")\nNLP_EN = spacy.load(\"en_core_web_sm\")\n\n# Stopwords FR/EN basiques pour nettoyage\nSTOPWORDS = {\n    \"le\", \"la\", \"les\", \"de\", \"des\", \"du\", \"un\", \"une\", \"et\", \"en\", \"√É \", \"dans\", \"que\", \"qui\",\n    \"pour\", \"par\", \"sur\", \"avec\", \"au\", \"aux\", \"ce\", \"ces\", \"se\", \"ses\", \"est\",\n    \"the\", \"and\", \"of\", \"to\", \"in\", \"that\", \"it\", \"is\", \"was\", \"for\", \"on\", \"are\", \"with\",\n    \"as\", \"i\", \"at\", \"be\", \"by\", \"this\"\n}\n\n\ndef extract_exif_metadata(file_path):\n    \"\"\"Extraction brute avec ExifTool\"\"\"\n    print(f\"√∞≈∏‚Äù¬ç Tentative d'extraction ExifTool pour {file_path}\")\n    try:\n        result = subprocess.run(\n            [\"exiftool\", \"-json\", file_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        if result.returncode != 0:\n            print(f\"√¢¬ù≈í Erreur ExifTool: {result.stderr}\")\n            return {}\n        metadata = json.loads(result.stdout)[0]\n        print(f\"√¢≈ì‚Ä¶ ExifTool a extrait {len(metadata)} champs: {list(metadata.keys())}\")\n        return metadata\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur ExifTool: {e}\")\n        print(f\"PATH actuel: {os.environ['PATH']}\")\n        return {}\n\n\ndef extract_text_content(file_path):\n    \"\"\"Extraction du texte brut selon le type de fichier\"\"\"\n    ext = Path(file_path).suffix.lower()\n    text = \"\"\n    print(f\"√∞≈∏‚Äù¬ç D√É¬©but de l'extraction texte pour {file_path} (type: {ext})\")\n    if ext == \".pdf\":\n        try:\n            text = extract_text(file_path)\n            print(f\"√¢≈ì‚Ä¶ Texte extrait avec succ√É¬®s ({len(text)} caract√É¬®res): {text[:100]}...\")\n        except Exception as e:\n            print(f\"√¢¬ù≈í Erreur extraction texte PDF: {e}\")\n    elif ext in [\".docx\"]:\n        try:\n            doc = Document(file_path)\n            text = \"\\n\".join([p.text for p in doc.paragraphs])\n            print(f\"√¢≈ì‚Ä¶ Texte extrait avec succ√É¬®s ({len(text)} caract√É¬®res)\")\n        except Exception as e:\n            print(f\"√¢¬ù≈í Erreur extraction texte DOCX: {e}\")\n    return text\n\n\ndef enhanced_nlp_analysis(text):\n    \"\"\"Analyse NLP renforc√©e sans LLM\"\"\"\n    meta = {}\n    if not text.strip():\n        print(\"‚ö†Ô∏è Aucun texte √† analyser pour NLP\")\n        return meta\n\n    print(\"üîç Lancement de l'analyse NLP renforc√©e\")\n\n    # D√âTECTION DU TYPE DE DOCUMENT\n    doc_types = {\n        'guideline': ['guideline', 'guidance', 'recommendations', 'best practices', 'guide', 'notice', 'instructions',\n                      'ligne directrice', 'recommandations'],\n        'regulation': ['regulation', 'directive', 'law', 'legal', 'article', 'decree', 'r√®glement', 'directive'],\n        'report': ['report', 'assessment', 'evaluation', 'analysis', 'study', 'rapport', '√©valuation'],\n        'procedure': ['procedure', 'protocol', 'method', 'process', 'operating', 'utilisation', 'pr√©cautions',\n                      'proc√©dure', 'protocole'],\n        'specifications': ['specifications', 'specs', 'requirements', 'criteria', 'specification', 'sp√©cifications',\n                           'exigences'],\n        'manufacturer': ['manufacturer', 'manufacturing', 'production', 'facility', 'plant', 'fabricant',\n                         'manufacturier'],\n        'certificate': ['certificate', 'certification', 'approval', 'authorization', 'certificat', 'approbation'],\n        'standard': ['standard', 'norm', 'iso', 'quality standard', 'norme', 'standard qualit√©']\n    }\n\n    text_lower = text.lower()\n    for doc_type, keywords in doc_types.items():\n        if any(keyword in text_lower for keyword in keywords):\n            meta['type'] = {\n                'value': doc_type,\n                'confidence': 85,\n                'method': 'nlp_keyword_match'\n            }\n            print(f\"‚úÖ Type d√©tect√©: {doc_type}\")\n            break\n\n    # EXTRACTION DE DATES AM√âLIOR√âE AVEC PRIORIT√â SUR LES LABELS EXPLICITES\n    date_candidates = []\n\n    # D√©tection de langue\n    language_info = detect_document_language(text)\n    lang = language_info.get('value', 'en') if language_info else 'en'\n    print(f\"Langue d√©tect√©e pour dates: {lang}\")\n\n    # D√©finir les noms de mois par langue\n    month_names_by_lang = {\n        'en': \"(?:January|February|March|April|May|June|July|August|September|October|November|December)\",\n        'fr': \"(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\"\n    }\n    month_names = month_names_by_lang.get(lang, month_names_by_lang['en'])\n\n    # Pattern 1: Label explicite \"Date de publication:\" (priorit√© maximale)\n    pub_label_pattern = rf'(?:Date\\s+de\\s+publication|Published\\s+on)[:\\s\\|]+(\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}})'\n    for match in re.finditer(pub_label_pattern, text, re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date:\n            normalized_date = parsed_date.strftime('%d %B %Y')\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': 99,  # Priorit√© maximale pour labels explicites\n                'context': 'publication_date_label',\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e (Publication label): {date_str} ‚Üí {normalized_date}\")\n\n    # Pattern 2: Date dans en-t√™te avec mois/ann√©e (format EMA typique)\n    header_date_pattern = rf'^([A-Z][a-z]+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})'\n    for match in re.finditer(header_date_pattern, text, re.MULTILINE | re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date and not any(\n                c['context'] == 'publication_date_label' for c in date_candidates):  # √âviter si label trouv√©\n            normalized_date = parsed_date.strftime('1 %B %Y')\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': 93,\n                'context': 'header_date',\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e (en-t√™te): {date_str} ‚Üí {normalized_date}\")\n\n    # Pattern 3: \"Step X dated DD Month YYYY\" (priorit√© √©lev√©e)\n    step_dated_pattern = rf'Step\\s+\\d+[a-z]?\\s+(?:dated\\s+)?(\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}})'\n    for match in re.finditer(step_dated_pattern, text, re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date and not any(c['context'] == 'publication_date_label' for c in date_candidates):\n            normalized_date = parsed_date.strftime('%d %B %Y')\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': 98,\n                'context': 'step_dated',\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e (Step dated): {date_str}\")\n\n    # Pattern 4: \"Final adoption by CHMP\" (priorit√© haute)\n    adoption_patterns = [\n        (rf'Final\\s+adoption\\s+by\\s+CHMP[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 92),\n        (rf'Final\\s+adoption[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 90),\n        (rf'Adopted\\s+by\\s+CHMP[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 85),\n        (rf'Adoption\\s+finale\\s+par\\s+CHMP[:\\s]+(\\w+\\s+\\d{{4}}|{month_names}\\s+\\d{{4}})', 92)\n    ]\n    for pattern, priority in adoption_patterns:\n        for match in re.finditer(pattern, text, re.IGNORECASE):\n            date_str = match.group(1)\n            parsed_date = dateparser.parse(date_str, languages=[lang])\n            if parsed_date and not any(c['context'] == 'publication_date_label' for c in date_candidates):\n                normalized_date = parsed_date.strftime('1 %B %Y')\n                date_candidates.append({\n                    'date': normalized_date,\n                    'priority': priority,\n                    'context': 'final_adoption',\n                    'raw': date_str\n                })\n                print(f\"üìÖ Date trouv√©e (Final adoption): {date_str} ‚Üí {normalized_date}\")\n\n    # Pattern 5: Format DD Month YYYY (g√©n√©rique)\n    generic_date_pattern = rf'\\b(\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}})\\b'\n    for match in re.finditer(generic_date_pattern, text, re.IGNORECASE):\n        date_str = match.group(1)\n        parsed_date = dateparser.parse(date_str, languages=[lang])\n        if parsed_date and not any(c['context'] == 'publication_date_label' for c in date_candidates):\n            normalized_date = parsed_date.strftime('%d %B %Y')\n            start_pos = max(0, match.start() - 100)\n            end_pos = min(len(text), match.end() + 100)\n            context = text[start_pos:end_pos].lower()\n            if 'coming into effect' in context or 'entr√©e en vigueur' in context or 'deadline' in context:\n                priority = 50\n                ctx_type = 'effective_date'\n            elif 'published' in context or 'publication' in context or 'publi√©' in context:\n                priority = 94\n                ctx_type = 'publication_date'\n            elif 'adopted' in context or 'adoption' in context or 'adopt√©' in context:\n                priority = 90\n                ctx_type = 'adoption_date'\n            else:\n                priority = 75\n                ctx_type = 'generic_date'\n            date_candidates.append({\n                'date': normalized_date,\n                'priority': priority,\n                'context': ctx_type,\n                'raw': date_str\n            })\n            print(f\"üìÖ Date trouv√©e ({ctx_type}): {date_str}\")\n\n    # Fallback avec dateparser sur candidats bruts\n    raw_candidates = re.findall(rf'\\d{{1,2}}\\s+{month_names}\\s+\\d{{4}}|\\d{{1,2}}\\.\\d{{1,2}}\\.\\d{{4}}', text,\n                                re.IGNORECASE)\n    for raw in raw_candidates:\n        parsed = dateparser.parse(raw, languages=[lang, 'en'])\n        if parsed and not any(c['raw'] == raw for c in date_candidates):\n            normalized = parsed.strftime('%d %B %Y')\n            if 'directive' in text_lower or 'law' in text_lower or re.search(r'\\d{4}/\\d+/EC', text):\n                priority = 40\n            else:\n                priority = 70\n            date_candidates.append({\n                'date': normalized,\n                'priority': priority,\n                'context': 'fallback_dateparser',\n                'raw': raw\n            })\n            print(f\"üìÖ Date fallback (dateparser): {raw} ‚Üí {normalized}\")\n\n    # S√©lection de la meilleure date (priorit√© sur labels explicites)\n    if date_candidates:\n        date_candidates.sort(key=lambda x: (-x['priority'], datetime.strptime(x['date'], '%d %B %Y') if '%d' in x[\n            'date'] else datetime.min), reverse=True)\n        selected_date = date_candidates[0]['date']\n        meta['publication_date'] = {\n            'value': selected_date,\n            'confidence': date_candidates[0]['priority'],\n            'context': date_candidates[0]['context'],\n            'method': 'nlp_pattern_match_multilingual'\n        }\n        print(\n            f\"‚úÖ Date de publication s√©lectionn√©e: {selected_date} (priorit√© {date_candidates[0]['priority']}, contexte: {date_candidates[0]['context']})\")\n\n    # EXTRACTION DE VERSION\n    version_patterns = [\n        r'version\\s*[:\\-]?\\s*(\\d+\\.?\\d*)',\n        r'v\\.?\\s*(\\d+\\.?\\d*)',\n        r'rev\\.?\\s*(\\d+)',\n        r'r(\\d+)',\n        r'step\\s*(\\d+)',\n        r'corr\\.?\\s*(\\d+)',\n        r'EN_(\\d+\\.?\\d*)',\n        r'_(\\d+\\.?\\d*)\\.pdf'\n    ]\n    for pattern in version_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            meta['version'] = {\n                'value': match.group(1),\n                'confidence': 80,\n                'method': 'regex_version'\n            }\n            print(f\"‚úÖ Version d√©tect√©e: {match.group(1)}\")\n            break\n\n    return meta\n\n\ndef extract_structured_metadata(file_path):\n    \"\"\"Extraction bas√É¬©e sur la structure du document\"\"\"\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            if not pdf.pages:\n                print(\"√¢≈° √Ø¬∏¬è Aucune page trouv√É¬©e dans le PDF\")\n                return {'title_structured': '', 'header_info': ''}\n\n            first_page = pdf.pages[0]\n\n            # Extraire le texte avec position\n            text_objects = first_page.extract_words()\n\n            if not text_objects:\n                print(\"√¢≈° √Ø¬∏¬è Aucun texte extrait de la premi√É¬®re page\")\n                # Essayer un fallback avec pdfminer\n                try:\n                    text = extract_text(file_path)  # Utiliser pdfminer comme secours\n                    first_line = text.split('\\n')[0].strip() if text else ''\n                    return {\n                        'title_structured': first_line,\n                        'header_info': first_line\n                    }\n                except Exception as e:\n                    print(f\"√¢¬ù≈í Erreur fallback pdfminer: {e}\")\n                    return {'title_structured': '', 'header_info': ''}\n\n            # Analyser les en-t√É¬™tes (texte en haut de page)\n            header_text = []\n            for obj in text_objects:\n                if obj.get('top', float('inf')) < 100:  # 100 pixels du haut\n                    header_text.append(obj['text'])\n\n            header_string = ' '.join(header_text) if header_text else ''\n\n            # Extraire titre (g√É¬©n√É¬©ralement le texte le plus gros)\n            title_candidates = []\n            sizes = [obj.get('size', 0) for obj in text_objects if obj.get('size') is not None]\n\n            if not sizes:\n                print(\"√¢≈° √Ø¬∏¬è Aucune taille de texte trouv√É¬©e\")\n                return {\n                    'title_structured': header_string or Path(file_path).stem,\n                    'header_info': header_string\n                }\n\n            max_size = max(sizes)\n            for obj in text_objects:\n                if obj.get('size', 0) >= max_size - 2:  # Texte de taille importante\n                    title_candidates.append(obj['text'])\n\n            title = ' '.join(title_candidates[:15]) if title_candidates else header_string or Path(file_path).stem\n\n            return {\n                'title_structured': title,\n                'header_info': header_string\n            }\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur extraction structur√É¬©e: {e}\")\n        return {'title_structured': '', 'header_info': ''}\n\n\ndef extract_extended_pdf_properties(file_path):\n    \"\"\"Extraction propri√É¬©t√É¬©s PDF √É¬©tendues\"\"\"\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            metadata = pdf.metadata or {}\n\n            # Analyser les m√É¬©tadonn√É¬©es XMP si disponibles\n            xmp_metadata = {}\n            try:\n                if hasattr(pdf, 'doc') and hasattr(pdf.doc, 'xmp_metadata'):\n                    xmp = pdf.doc.xmp_metadata\n                    if xmp:\n                        xmp_metadata = {\n                            'creator': getattr(xmp, 'creator', ''),\n                            'description': getattr(xmp, 'description', ''),\n                            'subject': getattr(xmp, 'subject', ''),\n                            'keywords': getattr(xmp, 'keywords', '')\n                        }\n            except Exception:\n                pass\n\n            return {**metadata, **xmp_metadata}\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur propri√É¬©t√É¬©s PDF: {e}\")\n        return {}\n\n\ndef regex_metadata_extraction(text):\n    \"\"\"Extraction par regex sp√É¬©cialis√É¬©es pour documents r√É¬©glementaires\"\"\"\n    patterns = {\n        'ema_reference': r'EMA/[A-Z]+/[A-Z]+/\\d+/\\d+',\n        'ich_reference': r'ICH/[A-Z0-9]+/\\d+',\n        'step_version': r'Step\\s+(\\d+[a-z]?)',\n        'correction': r'Corr\\.?\\s*(\\d+)',\n        'european_directive': r'Directive\\s+(\\d{4}/\\d+/EC)',\n        'organization_regex': r'(European Medicines Agency|EMA|FDA|ANSM|MHRA)',\n        'effective_date': r'effective\\s+(?:from\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n        'published_date': r'published\\s+(?:on\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n        'adoption_date': r'adopted\\s+(?:on\\s+)?(\\d{1,2}\\s+\\w+\\s+\\d{4})'\n    }\n\n    extracted = {}\n    for key, pattern in patterns.items():\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            extracted[key] = match.group(1) if match.groups() else match.group(0)\n\n    return extracted\n\n\ndef merge_metadata_with_priority(metadata_sources):\n    \"\"\"Fusionner les m√É¬©tadonn√É¬©es avec priorit√É¬©\"\"\"\n    merged = {}\n\n    field_mapping = {\n        'title': ['Title', 'title', 'title_inferred', 'title_structured', 'Subject'],\n        'type': ['type', 'DM_Category', 'doc_type'],\n        'source': ['Author', 'Creator', 'organization', 'source_organization', 'ema_reference', 'organization_regex'],\n        'version': ['version', 'DM_Version', 'step_version', 'correction'],\n        'publication_date': ['CreateDate', 'publication_date', 'effective_date', 'published_date', 'date'],\n        'language': ['Language', 'language', 'DM_Language'],\n        'country': ['country'],\n        'context': ['DM_Category', 'DM_Path', 'context']\n    }\n\n    for field, possible_keys in field_mapping.items():\n        for source in metadata_sources:\n            for key in possible_keys:\n                if key in source and source[key] and not merged.get(field):\n                    merged[field] = source[key]\n                    break\n            if merged.get(field):\n                break\n\n    return merged\n\n\ndef infer_additional_metadata(text):\n    \"\"\"Analyse NLP du contenu pour d√É¬©duire d'autres m√É¬©tadonn√É¬©es (version am√É¬©lior√É¬©e)\"\"\"\n    return enhanced_nlp_analysis(text)\n\n\ndef detect_document_language(text, file_path=\"\"):\n    \"\"\"\n    D√©tecte la langue du document avec plusieurs m√©thodes et retourne un score de confiance\n    \"\"\"\n    if not text or len(text.strip()) < 50:\n        print(\"‚ö†Ô∏è Texte trop court pour d√©tecter la langue\")\n        return None\n\n    detected_langs = []\n\n    # M√©thode 1 : langdetect sur le texte complet\n    try:\n        lang_full = detect(text[:5000])  # Analyse des premiers 5000 caract√®res\n        detected_langs.append(('langdetect_full', lang_full, 90))\n        print(f\"‚úÖ Langue d√©tect√©e (texte complet): {lang_full}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Erreur langdetect (texte complet): {e}\")\n\n    # M√©thode 2 : langdetect sur plusieurs √©chantillons (plus fiable)\n    try:\n        samples = [\n            text[:1000],\n            text[len(text) // 2:len(text) // 2 + 1000] if len(text) > 2000 else text,\n            text[-1000:] if len(text) > 1000 else text\n        ]\n        lang_counts = {}\n        for sample in samples:\n            if len(sample.strip()) > 50:\n                try:\n                    lang = detect(sample)\n                    lang_counts[lang] = lang_counts.get(lang, 0) + 1\n                except:\n                    pass\n\n        if lang_counts:\n            # Langue la plus fr√©quente\n            most_common_lang = max(lang_counts, key=lang_counts.get)\n            confidence = 85 + (lang_counts[most_common_lang] * 5)  # Max 100\n            detected_langs.append(('langdetect_samples', most_common_lang, min(confidence, 98)))\n            print(\n                f\"‚úÖ Langue d√©tect√©e (√©chantillons multiples): {most_common_lang} (consensus: {lang_counts[most_common_lang]}/3)\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Erreur langdetect (√©chantillons): {e}\")\n\n    # M√©thode 3 : Analyse du nom de fichier\n    if file_path:\n        filename = Path(file_path).stem.upper()\n        lang_indicators = {\n            'EN': ['_EN_', '_EN.', 'ENGLISH', '_ENG_'],\n            'FR': ['_FR_', '_FR.', 'FRENCH', '_FRA_'],\n            'DE': ['_DE_', '_DE.', 'GERMAN', '_DEU_'],\n            'ES': ['_ES_', '_ES.', 'SPANISH', '_ESP_'],\n            'IT': ['_IT_', '_IT.', 'ITALIAN', '_ITA_'],\n            'PL': ['_PL_', '_PL.', 'POLISH', '_POL_']\n        }\n\n        for lang_code, indicators in lang_indicators.items():\n            if any(ind in filename for ind in indicators):\n                detected_langs.append(('filename', lang_code.lower(), 80))\n                print(f\"‚úÖ Langue d√©tect√©e (nom de fichier): {lang_code.lower()}\")\n                break\n\n    # M√©thode 4 : Analyse des mots-cl√©s de langue\n    text_sample = text[:3000].lower()\n    keyword_scores = {\n        'en': sum([\n            text_sample.count(' the ') * 2,\n            text_sample.count(' and '),\n            text_sample.count(' of '),\n            text_sample.count(' to '),\n            text_sample.count(' in '),\n            text_sample.count('guideline'),\n            text_sample.count('regulation')\n        ]),\n        'fr': sum([\n            text_sample.count(' le ') * 2,\n            text_sample.count(' la ') * 2,\n            text_sample.count(' les ') * 2,\n            text_sample.count(' de '),\n            text_sample.count(' du '),\n            text_sample.count(' des '),\n            text_sample.count('r√®glement'),\n            text_sample.count('directive')\n        ]),\n        'de': sum([\n            text_sample.count(' der ') * 2,\n            text_sample.count(' die ') * 2,\n            text_sample.count(' das ') * 2,\n            text_sample.count(' und '),\n            text_sample.count(' ist '),\n            text_sample.count('richtlinie')\n        ])\n    }\n\n    if keyword_scores:\n        best_lang = max(keyword_scores, key=keyword_scores.get)\n        if keyword_scores[best_lang] > 10:  # Seuil minimal\n            confidence = min(70 + (keyword_scores[best_lang] // 5), 85)\n            detected_langs.append(('keywords', best_lang, confidence))\n            print(f\"‚úÖ Langue d√©tect√©e (mots-cl√©s): {best_lang} (score: {keyword_scores[best_lang]})\")\n\n    # Consolidation : choisir la meilleure d√©tection\n    if not detected_langs:\n        print(\"‚ùå Aucune langue d√©tect√©e\")\n        return None\n\n    # Trier par confiance et choisir la meilleure\n    detected_langs.sort(key=lambda x: x[2], reverse=True)\n    best_method, best_lang, best_confidence = detected_langs[0]\n\n    # V√©rifier la coh√©rence entre m√©thodes\n    lang_votes = {}\n    for method, lang, conf in detected_langs:\n        lang_votes[lang] = lang_votes.get(lang, 0) + 1\n\n    # Si plusieurs m√©thodes sont d'accord, augmenter la confiance\n    if lang_votes.get(best_lang, 0) >= 2:\n        best_confidence = min(best_confidence + 10, 98)\n        print(f\"üéØ Consensus sur la langue: {best_lang} ({lang_votes[best_lang]} m√©thodes concordantes)\")\n\n    return {\n        'value': best_lang,\n        'confidence': best_confidence,\n        'method': f'language_detection_{best_method}'\n    }\n\n\ndef extract_metadata_local_pipeline(file_path):\n    \"\"\"Point d'entr√©e principal pour l'extraction locale\"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"üöÄ EXTRACTION DYNAMIQUE PROFESSIONNELLE\")\n    print(f\"üìÑ Fichier: {Path(file_path).name}\")\n    print(f\"{'=' * 80}\\n\")\n\n    # 1. Extraction du texte\n    full_text = extract_text_content(file_path)\n\n    # 1b. D√©tection de la langue (NOUVEAU)\n    detected_language = detect_document_language(full_text, file_path)\n\n    # 2. Analyse NLP renforc√©e comme source principale\n    nlp_meta = enhanced_nlp_analysis(full_text)\n\n    # 3. Extraction compl√©mentaire via Exif (si disponible)\n    exif_meta = extract_exif_metadata(file_path)\n\n    # 4. Normalisation : convertir toutes les donn√©es en format uniforme\n    normalized_meta = {}\n\n    # Traiter les donn√©es NLP\n    for field, value in nlp_meta.items():\n        if isinstance(value, dict) and 'value' in value:\n            # D√©j√† au bon format\n            normalized_meta[field] = value\n        elif value:  # Valeur simple (comme type='guideline')\n            # Convertir en format structur√©\n            normalized_meta[field] = {\n                'value': value,\n                'confidence': 80,  # Confidence par d√©faut pour d√©tection NLP\n                'method': 'nlp_analysis'\n            }\n\n    # Ajouter les donn√©es Exif si elles am√©liorent la qualit√©\n    if exif_meta:\n        # Title depuis Exif (si pas d√©j√† trouv√© avec meilleure confidence)\n        if 'Title' in exif_meta and exif_meta['Title']:\n            if 'title' not in normalized_meta or normalized_meta['title']['confidence'] < 75:\n                normalized_meta['title'] = {\n                    'value': exif_meta['Title'],\n                    'confidence': 75,\n                    'method': 'exif'\n                }\n\n        # Source depuis Author\n        if 'Author' in exif_meta and exif_meta['Author']:\n            if 'source' not in normalized_meta or normalized_meta['source']['confidence'] < 70:\n                normalized_meta['source'] = {\n                    'value': exif_meta['Author'],\n                    'confidence': 70,\n                    'method': 'exif'\n                }\n\n        # Language depuis Exif (priorit√© moyenne)\n        if 'Language' in exif_meta and exif_meta['Language']:\n            if 'language' not in normalized_meta or normalized_meta['language']['confidence'] < 75:\n                normalized_meta['language'] = {\n                    'value': exif_meta['Language'],\n                    'confidence': 75,\n                    'method': 'exif'\n                }\n\n    # Ajouter la langue d√©tect√©e (si pas de meilleure source)\n    if detected_language:\n        if 'language' not in normalized_meta or normalized_meta['language']['confidence'] < detected_language[\n            'confidence']:\n            normalized_meta['language'] = detected_language\n\n    # 5. Consolidation finale : extraire les valeurs et calculer les scores\n    final_metadata = {}\n    confidence_scores = {}\n\n    # Liste des champs attendus\n    expected_fields = ['title', 'type', 'publication_date', 'version', 'source', 'context', 'country', 'language',\n                       'url_source']\n\n    for field in expected_fields:\n        if field in normalized_meta:\n            meta_obj = normalized_meta[field]\n            final_metadata[field] = meta_obj['value']\n            confidence_scores[field] = meta_obj['confidence']\n            print(f\"   ‚Ä¢ {field}: {meta_obj['value']} (confiance: {meta_obj['confidence']}% - {meta_obj['method']})\")\n        else:\n            final_metadata[field] = \"\"\n            confidence_scores[field] = 0\n            print(f\"   ‚Ä¢ {field}: (non d√©tect√©)\")\n\n    # 6. Calcul de qualit√© globale\n    overall_quality = calculate_overall_quality(confidence_scores)\n\n    # 7. Construire l'objet quality avec tous les d√©tails\n    extraction_reasoning = {}\n    for field in expected_fields:\n        if field in normalized_meta:\n            method = normalized_meta[field]['method']\n            extraction_reasoning[field] = f\"Detected via {method}\"\n        else:\n            extraction_reasoning[field] = \"Not detected\"\n\n    final_metadata['quality'] = {\n        'extraction_rate': overall_quality,\n        'field_scores': confidence_scores,\n        'extraction_reasoning': extraction_reasoning,\n        'extracted_fields': sum(1 for v in final_metadata.values() if v and v != \"\"),\n        'total_fields': len(expected_fields),\n        'llm_powered': False\n    }\n\n    print(f\"\\n{'=' * 80}\")\n    print(f\"‚úÖ EXTRACTION TERMIN√âE\")\n    print(f\"üìä Taux de remplissage: {final_metadata['quality']['extraction_rate']}%\")\n    print(\n        f\"üìà Champs extraits: {final_metadata['quality']['extracted_fields']}/{final_metadata['quality']['total_fields']}\")\n    print(f\"{'=' * 80}\\n\")\n\n    return final_metadata\n\n\ndef call_mistral_with_confidence(text_chunk, document_url=\"\", filename=\"\"):\n    \"\"\"\n    Get extraction + confidence scores from Mistral in one call.\n    La cl√É¬© API est lue depuis la variable d'environnement MISTRAL_API_KEY.\n    \"\"\"\n    api_key = os.getenv(\n        \"MISTRAL_API_KEY\") or \"j2wOKpM86nlZhhlvkXXG7rFd4bhM4PN5\"  # Fallback √É  la cl√É¬© hardcod√É¬©e si .env non disponible\n\n    try:\n        url = \"https://api.mistral.ai/v1/chat/completions\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n\n        prompt = f\"\"\"\n        You are an expert document analyzer. Analyze this document and extract metadata with confidence scores.\n\n        DOCUMENT TEXT (first 5000 chars):\n        {text_chunk[:5000]}\n\n        SOURCE URL: {document_url}\n        FILENAME: {filename}\n\n        HINT: Look for version numbers in headers like \"DTC_S006597_PF_R_FAB_17319_EN_15.0\" where 15.0 is the version\n\n        TASK: Return ONLY a JSON object with extracted metadata AND your confidence scores:\n\n        {{\n            \"metadata\": {{\n                \"title\": \"the MAIN document title (not technical headers)\",\n                \"type\": \"guideline|regulation|directive|report|procedure|standard|manufacturer|certificate|authorization|specifications|other\",\n                \"publication_date\": \"exact date (DD Month YYYY format)\",\n                \"version\": \"document version/reference number\",\n                \"source\": \"organization name\",\n                \"context\": \"main domain (pharmaceutical, medical, legal, etc.)\",\n                \"country\": \"country code (EU, US, FR, IE, PL, etc.)\",\n                \"language\": \"language code (en, fr, etc.)\"\n            }},\n            \"confidence_scores\": {{\n                \"title\": 85,\n                \"type\": 90,\n                \"publication_date\": 95,\n                \"version\": 20,\n                \"source\": 90,\n                \"context\": 80,\n                \"country\": 95,\n                \"language\": 98\n            }},\n            \"extraction_reasoning\": {{\n                \"title\": \"Found clear title in document header\",\n                \"type\": \"Document explicitly mentions 'guideline' in title\",\n                \"publication_date\": \"Detected header date indicating publication\",\n                \"version\": \"No clear version number found\",\n                \"source\": \"URL or internal org name used\",\n                \"context\": \"Multiple pharmaceutical terms detected\",\n                \"country\": \"URL domain or regulatory hints used\",\n                \"language\": \"Detected by language model\"\n            }}\n        }}\n\n        INSTRUCTIONS FOR DOCUMENT TYPE DETECTION:\n        - \"manufacturer\": Documents listing manufacturing sites, manufacturer information, production details\n        - \"specifications\": Documents detailing product specifications, technical specifications, drug product specs\n        - \"guideline\": Official guidance documents from regulatory agencies\n        - \"regulation\": Legal regulatory texts, laws, regulations\n        - \"directive\": EU directives and similar regulatory directives  \n        - \"report\": Assessment reports, evaluation reports, inspection reports\n        - \"procedure\": Standard operating procedures, process descriptions\n        - \"standard\": Technical standards, quality standards\n        - \"certificate\": Certificates, authorizations, approvals\n        - \"authorization\": Marketing authorizations, permits, licenses\n        - \"other\": Any other document type\n\n\n        RULES FOR PUBLICATION DATE DETECTION:\n\n        Use the date that appears:\n        - At the top near the agency name (e.g. \"EMA/CHMP/ICH/24235/2006 Corr.2 23 January 2025\" √¢‚Ä†‚Äô use 23 January 2025)\n        - In the format \"Published: ...\" or \"Publication date: ...\"\n        - Next to \"Official Journal of the European Union\" (this is official)\n\n        DO NOT use:\n        - \"Adopted by CHMP on √¢‚Ç¨¬¶\"\n        - \"Date of coming into effect √¢‚Ç¨¬¶\"\n        - \"Finalization date\"\n        - References to old laws like \"Directive 2008/57/EC of 15 July 2008\"\n\n        Examples:\n        √¢≈ì‚Ä¶ GOOD √¢‚Ä†‚Äô \"23 January 2025\" near header  \n        √¢≈ì‚Ä¶ GOOD √¢‚Ä†‚Äô \"Published in Official Journal: 2 August 2013\"  \n        √¢¬ù≈í BAD √¢‚Ä†‚Äô \"Directive 2008/123/EC of 24 November 2008\"  \n        √¢¬ù≈í BAD √¢‚Ä†‚Äô \"Date for coming into effect: 26 July 2023\"\n\n        Additional Notes:\n        - Prioritize clarity over completeness\n        - Return no extra text outside JSON\n\n        TYPE DETECTION KEYWORDS:\n        - \"SPECIFICATIONS OF\" √¢‚Ä†‚Äô type: \"specifications\"\n        - \"MANUFACTURERS\" √¢‚Ä†‚Äô type: \"manufacturer\"  \n        - \"GUIDELINE\" √¢‚Ä†‚Äô type: \"guideline\"\n        - \"PROCEDURE\" √¢‚Ä†‚Äô type: \"procedure\"\n        - Look for explicit type indicators in document titles and headers\n\n        INSTRUCTIONS FOR TITLE EXTRACTION:\n        - Extract the MAIN document title from the body content, not technical headers\n        - Ignore technical codes like \"DTC_S006597_PF_R_FAB_17319_EN_15.0\"\n        - Look for the primary subject/topic in the document body\n        - For manufacturer docs: \"MANUFACTURERS\" + product name + description\n        - Combine logical title elements: \"MANUFACTURERS S 6597 Film-coated tablet...\"\n        - Avoid including addresses, company names, or metadata in the title\n\n        INSTRUCTIONS FOR VERSION DETECTION:\n\n\n        1. If a regulatory code is present like:\n        - EMA/CHMP/ICH/xxxxx/yyyy\n        - ICH/yyyy/stepX\n        √¢‚Ä†‚Äô extract that full code as version\n\n        2. If \"Corr.2\", \"Rev.1\", \"(R1)\", etc. appears after the code or title √¢‚Ä†‚Äô append it\n\n        3. If \"Step 5\", \"Step 2b\", etc. appears near the header √¢‚Ä†‚Äô append it\n\n        4. Accept also formats like:\n        - M4Q(R2) √¢‚Ä†‚Äô version = \"M4Q(R2)\"\n        - CTD_QR_Rev.2 √¢‚Ä†‚Äô version = \"Rev.2\"\n        - EN_15.0 √¢‚Ä†‚Äô version = \"15.0\"\n\n        √¢‚Ç¨‚Äù EXAMPLES √¢‚Ç¨‚Äù\n        √¢≈ì‚Ä¶ EMA/CHMP/ICH/214732/2007 Step 5\n        √¢≈ì‚Ä¶ EMA/CHMP/ICH/24235/2006 Corr.2 Step 5\n        √¢≈ì‚Ä¶ M4Q(R2)\n        √¢≈ì‚Ä¶ Rev. 1\n        √¢≈ì‚Ä¶ Version 3.0\n\n        √¢‚Ç¨‚Äù DO NOT EXTRACT √¢‚Ç¨‚Äù\n        √¢¬ù≈í Directive 2008/57/EC\n        √¢¬ù≈í IDs not linked to versioning\n\n\n        If no clear version is found, return an empty string: `\"\"`\n\n\n        INSTRUCTIONS FOR SOURCE DETECTION:\n        - Look for explicit organization names in the document\n        - Check for regulatory agency indicators (EMA, FDA, ANSM, MHRA, etc.)\n        - For manufacturer documents, use the company name mentioned\n        - For specifications documents, look for regulatory authority context\n        - Check for \"European Pharmacopoeia\" √¢‚Ä†‚Äô likely EMA/European regulatory context\n        - If from URL domain: .ema.europa.eu = \"EMA\", .fda.gov = \"FDA\", etc.\n        - If unclear, analyze content to determine likely source organization\n        - Do NOT default to EMA unless clearly an EMA document\n\n        INSTRUCTIONS FOR COUNTRY DETECTION:\n        - Look for country-specific indicators in text\n        - Check addresses, phone numbers, regulatory codes\n        - Analyze URL domain for country hints\n        - For manufacturer docs, use country where facilities are located\n        - Use specific country codes (FR, IE, PL) not just EU when possible\n\n        CONFIDENCE SCORING:\n        - 90-100: Explicitly stated in document\n        - 70-89: Strong indicators/evidence  \n        - 50-69: Reasonable inference from context\n        - 30-49: Weak indicators, uncertain\n        - 0-29: Not found or very unclear\n\n        TITLE EXTRACTION EXAMPLES:\n        √¢≈ì‚Ä¶ GOOD: \"MANUFACTURERS S 6597 Film-coated tablet containing 10 mg of perindopril arginine and 2.5 mg of indapamide\"\n        √¢≈ì‚Ä¶ GOOD: \"SPECIFICATIONS OF THE DRUG PRODUCT S 20098 Film-coated tablets containing 25 mg of drug substance\"\n        √¢¬ù≈í BAD: \"DTC_S006597_PF_R_FAB_17319_EN_15.0 MANUFACTURERS S 6597...\"\n        √¢¬ù≈í BAD: Including addresses or company details in title\n\n        Return ONLY the JSON, no other text.\n        \"\"\"\n\n        data = {\n            \"model\": \"mistral-large-latest\",\n            \"messages\": [{\"role\": \"user\",\n                          \"content\": prompt + \"\\n\\nReturn ONLY one JSON object or array. No markdown, no code fences, no explanations, no ellipses in values.\"}],\n            \"temperature\": 0.1,\n            \"max_tokens\": 1200\n        }\n\n        response = requests.post(url, headers=headers, json=data, timeout=30)\n        response.raise_for_status()\n\n        if response.status_code == 200:\n            result = response.json()\n            response_text = result['choices'][0]['message']['content']\n\n            # Try to robustly parse JSON from the response\n            def _sanitize_and_parse(candidate: str):\n                original = candidate\n                # Normalize smart quotes\n                candidate = candidate.replace('\\u201c', '\"').replace('\\u201d', '\"').replace('\\u2019', \"'\")\n                # Remove trailing commas before } or ]\n                candidate = re.sub(r',\\s*([}\\]])', r'\\1', candidate)\n                # Fix single-quoted keys: 'key': value -> \"key\": value\n                candidate = re.sub(r\"(?<=\\{|,)\\s*'([A-Za-z0-9_]+)'\\s*:\\s*\", r'\"\\1\": ', candidate)\n                # Fix single-quoted string values: key: 'value' -> key: \"value\"\n                candidate = re.sub(r\":\\s*'([^'\\\\]*(?:\\\\.[^'\\\\]*)*)'\", r': \"\\1\"', candidate)\n                # Fix unquoted keys only when at object boundaries\n                candidate = re.sub(r'(?<=\\{|,)\\s*([A-Za-z0-9_]+)\\s*:', r'\"\\1\":', candidate)\n                try:\n                    return json.loads(candidate)\n                except Exception:\n                    return None\n\n            def _extract_balanced_block(text: str):\n                # Support either object {..} or array [..]\n                start_obj = text.find('{')\n                start_arr = text.find('[')\n                starts = [s for s in [start_obj, start_arr] if s != -1]\n                if not starts:\n                    return None\n                start = min(starts)\n                opener = text[start]\n                closer = '}' if opener == '{' else ']'\n                i = start\n                depth = 0\n                in_str = False\n                esc = False\n                while i < len(text):\n                    ch = text[i]\n                    if in_str:\n                        if esc:\n                            esc = False\n                        elif ch == '\\\\':\n                            esc = True\n                        elif ch == '\"':\n                            in_str = False\n                    else:\n                        if ch == '\"':\n                            in_str = True\n                        elif ch == opener:\n                            depth += 1\n                        elif ch == closer:\n                            depth -= 1\n                            if depth == 0:\n                                return text[start:i + 1]\n                    i += 1\n                return None\n\n            def _try_parse_json(text: str):\n                # 1) Direct parse\n                try:\n                    return json.loads(text)\n                except Exception:\n                    pass\n                # 2) Fenced code block ```json:disable-run\n                m = re.search(r\"```(?:json|JSON)?\\s*([\\[{][\\s\\S]*?[\\]}])\\s*```\", text)\n                if m:\n                    fragment = m.group(1)\n                    try:\n                        return json.loads(fragment)\n                    except Exception:\n                        parsed = _sanitize_and_parse(fragment)\n                        if parsed is not None:\n                            return parsed\n                # 3) Balanced block extraction (object or array)\n                candidate = _extract_balanced_block(text)\n                if candidate:\n                    try:\n                        return json.loads(candidate)\n                    except Exception:\n                        parsed = _sanitize_and_parse(candidate)\n                        if parsed is not None:\n                            return parsed\n                return None\n\n            full_result = _try_parse_json(response_text)\n            if not full_result:\n                print(\"√¢¬ù≈í No valid JSON found or parse failed\")\n                return None\n\n            # Add quality metrics to metadata\n            if 'metadata' in full_result and 'confidence_scores' in full_result:\n                confidence_scores = full_result.get('confidence_scores', {})\n                overall_quality = calculate_overall_quality(confidence_scores)\n\n                full_result['metadata']['quality'] = {\n                    'extraction_rate': overall_quality,\n                    'field_scores': confidence_scores,\n                    'extraction_reasoning': full_result.get('extraction_reasoning', {}),\n                    'extracted_fields': len([v for v in full_result['metadata'].values() if v]),\n                    'total_fields': len(full_result['metadata']),\n                    'llm_powered': True\n                }\n\n            print(\"√¢≈ì‚Ä¶ Mistral extraction with confidence successful!\")\n            return full_result\n        else:\n            print(f\"√¢¬ù≈í Mistral API error: {response.status_code}\")\n            return None\n\n    except Exception as e:\n        print(f\"√¢¬ù≈í Mistral API error: {e}\")\n        return None\n\n\ndef calculate_overall_quality(confidence_scores):\n    \"\"\"\n    Calculate overall extraction quality from LLM confidence scores\n    \"\"\"\n    if not confidence_scores:\n        return 0\n\n    weights = {\n        'title': 1.5,\n        'type': 1.2,\n        'publication_date': 1.3,\n        'source': 1.2,\n        'context': 1.0,\n        'language': 0.8,\n        'country': 0.8,\n        'version': 0.7\n    }\n\n    total_weighted_score = 0\n    total_weight = 0\n    for field, score in confidence_scores.items():\n        w = weights.get(field, 1.0)\n        total_weighted_score += score * w\n        total_weight += w\n\n    return int(total_weighted_score / total_weight) if total_weight else 0\n\n\ndef extract_tables_from_pdf(file_path):\n    \"\"\"\n    Extrait tous les tableaux du PDF avec pdfplumber\n    \"\"\"\n    tables_data = []\n\n    try:\n        with pdfplumber.open(file_path) as pdf:\n            for page_num, page in enumerate(pdf.pages, 1):\n                # Extraire les tableaux de la page\n                tables = page.extract_tables()\n\n                for table_num, table in enumerate(tables, 1):\n                    if table and len(table) > 1:  # Au moins un header + une ligne\n                        # Nettoyer le tableau (supprimer les cellules vides)\n                        cleaned_table = []\n                        for row in table:\n                            cleaned_row = [cell.strip() if cell else \"\" for cell in row]\n                            if any(cleaned_row):  # Garder seulement les lignes non vides\n                                cleaned_table.append(cleaned_row)\n\n                        if len(cleaned_table) > 1:\n                            tables_data.append({\n                                'page': page_num,\n                                'table_id': f\"table_{page_num}_{table_num}\",\n                                'headers': cleaned_table[0],\n                                'rows': cleaned_table[1:],\n                                'total_rows': len(cleaned_table) - 1,\n                                'total_columns': len(cleaned_table[0]) if cleaned_table else 0\n                            })\n\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur lors de l'extraction des tableaux: {e}\")\n\n    return tables_data\n\n\ndef call_llm_with_learned_prompt(prompt):\n    \"\"\"Call LLM with the adaptive prompt\"\"\"\n    try:\n        api_key = os.getenv(\"GROQ_API_KEY\") or \"your_actual_groq_api_key_here\"\n        client = Groq(api_key=api_key)\n\n        response = client.chat.completions.create(\n            messages=[{\n                \"role\": \"user\",\n                \"content\": prompt + \"\\n\\nReturn ONLY one JSON object or array. No markdown, no code fences, no explanations, no ellipses in values.\"\n            }],\n            model=\"llama3-8b-8192\",\n            temperature=0.1,\n            max_tokens=1500\n        )\n\n        result = response.choices[0].message.content\n\n        # Robust JSON parsing similar to Mistral path\n        def _sanitize_and_parse(candidate: str):\n            # Normalize smart quotes\n            candidate = candidate.replace('\\u201c', '\"').replace('\\u201d', '\"').replace('\\u2019', \"'\")\n            # Remove trailing commas before } or ]\n            candidate = re.sub(r',\\s*([}\\]])', r'\\1', candidate)\n            # Fix single-quoted keys: 'key': value -> \"key\": value\n            candidate = re.sub(r\"(?<=\\{|,)\\s*'([A-Za-z0-9_]+)'\\s*:\\s*\", r'\"\\1\": ', candidate)\n            # Fix single-quoted string values: key: 'value' -> key: \"value\"\n            candidate = re.sub(r\":\\s*'([^'\\\\]*(?:\\\\.[^'\\\\]*)*)'\", r': \"\\1\"', candidate)\n            # Fix unquoted keys only when at object boundaries\n            candidate = re.sub(r'(?<=\\{|,)\\s*([A-Za-z0-9_]+)\\s*:', r'\"\\1\":', candidate)\n            try:\n                return json.loads(candidate)\n            except Exception:\n                return None\n\n        def _extract_balanced_json(text: str):\n            start = text.find('{')\n            if start == -1:\n                return None\n            i = start\n            depth = 0\n            in_str = False\n            esc = False\n            while i < len(text):\n                ch = text[i]\n                if in_str:\n                    if esc:\n                        esc = False\n                    elif ch == '\\\\':\n                        esc = True\n                    elif ch == '\"':\n                        in_str = False\n                else:\n                    if ch == '\"':\n                        in_str = True\n                    elif ch == '{':\n                        depth += 1\n                    elif ch == '}':\n                        depth -= 1\n                        if depth == 0:\n                            return text[start:i + 1]\n                i += 1\n            return None\n\n        def _try_parse_json(text: str):\n            # 1) Direct parse\n            try:\n                return json.loads(text)\n            except Exception:\n                pass\n            # 2) Fenced code block ```json ... ``` or ``` ... ```\n            m = re.search(r\"```(?:json|JSON)?\\s*(\\{[\\s\\S]*?\\})\\s*```\", text)\n            if m:\n                try:\n                    return json.loads(m.group(1))\n                except Exception:\n                    parsed = _sanitize_and_parse(m.group(1))\n                    if parsed is not None:\n                        return parsed\n            # 3) Balanced-brace extraction\n            candidate = _extract_balanced_json(text)\n            if candidate:\n                try:\n                    return json.loads(candidate)\n                except Exception:\n                    parsed = _sanitize_and_parse(candidate)\n                    if parsed is not None:\n                        return parsed\n            return {}\n\n        return _try_parse_json(result)\n\n    except Exception as e:\n        print(f\"LLM call error: {e}\")\n        # Fallback to your existing Mistral extraction\n        return call_mistral_basic_extraction(prompt)\n\n\ndef call_mistral_basic_extraction(prompt):\n    \"\"\"Fallback to basic extraction if Groq fails\"\"\"\n    # Use your existing call_mistral_with_confidence logic here\n    # or return empty dict to use fallback\n    return {}\n\n\ndef extract_images_from_pdf(file_path):\n    \"\"\"\n    Extrait toutes les images du PDF avec PyMuPDF\n    \"\"\"\n    images_data = []\n\n    try:\n        pdf_document = fitz.open(file_path)\n\n        for page_num in range(len(pdf_document)):\n            page = pdf_document[page_num]\n            image_list = page.get_images()\n\n            for img_index, img in enumerate(image_list):\n                try:\n                    # R√É¬©cup√É¬©rer l'image\n                    xref = img[0]\n                    pix = fitz.Pixmap(pdf_document, xref)\n\n                    # Convertir en format utilisable\n                    if pix.n - pix.alpha < 4:  # GRAY ou RGB\n                        img_data = pix.tobytes(\"png\")\n\n                        # Cr√É¬©er un objet PIL pour obtenir les dimensions\n                        pil_image = Image.open(io.BytesIO(img_data))\n                        width, height = pil_image.size\n\n                        # Encoder en base64 pour stockage/affichage\n                        img_base64 = base64.b64encode(img_data).decode()\n\n                        # Cr√É¬©er une version preview (plus petite pour l'affichage dans la liste)\n                        preview_image = pil_image.copy()\n                        preview_image.thumbnail((300, 200), Image.Resampling.LANCZOS)\n                        preview_buffer = io.BytesIO()\n                        preview_image.save(preview_buffer, format='PNG')\n                        preview_base64 = base64.b64encode(preview_buffer.getvalue()).decode()\n\n                        images_data.append({\n                            'page': page_num + 1,\n                            'image_id': f\"img_{page_num + 1}_{img_index + 1}\",\n                            'width': width,\n                            'height': height,\n                            'format': 'PNG',\n                            'size_bytes': len(img_data),\n                            'base64_data': img_base64[:100] + \"...\" if len(img_base64) > 100 else img_base64,\n                            # Tronquer pour l'affichage\n                            'preview_base64': preview_base64,  # Version preview pour l'affichage\n                            'full_base64': img_base64  # Donn√É¬©es compl√É¬®tes\n                        })\n\n                    pix = None  # Lib√É¬©rer la m√É¬©moire\n\n                except Exception as e:\n                    print(f\"√¢¬ù≈í Erreur lors de l'extraction de l'image {img_index} page {page_num + 1}: {e}\")\n                    continue\n\n        pdf_document.close()\n\n    except Exception as e:\n        print(f\"√¢¬ù≈í Erreur lors de l'extraction des images: {e}\")\n\n    return images_data\n\n\ndef extract_full_text(file_path):\n    \"\"\"\n    Extraction de texte utilisant directement pdfplumber\n    \"\"\"\n    # Utiliser directement le fallback pdfplumber qui est maintenant la m√É¬©thode principale\n    return _extract_full_text_fallback(file_path)\n\n\ndef _extract_full_text_fallback(file_path):\n    \"\"\"Fallback vers l'ancienne m√É¬©thode pdfplumber\"\"\"\n    text = \"\"\n    with pdfplumber.open(file_path) as pdf:\n        # extraire en priorit√É¬© les 1√É¬®res pages\n        for page in pdf.pages[:2]:\n            page_text = page.extract_text() or \"\"\n            text += \"\\n\" + page_text\n    # ensuite le reste si besoin\n    with pdfplumber.open(file_path) as pdf:\n        for page in pdf.pages[2:]:\n            text += \"\\n\" + (page.extract_text() or \"\")\n    return text\n\n\ndef extract_metadonnees(pdf_path, url=\"\"):\n    \"\"\"Extract metadata with fallback to working system\"\"\"\n\n    if url:\n        print(f\"√∞≈∏‚Äú¬° Utilisation du prompt LLM pour l'extraction des m√É¬©tadonn√É¬©es (URL: {url})\")\n        # First try the learning-enhanced extraction\n        try:\n            # Get learning insights\n            learning_prompts = get_learned_field_improvements()\n\n            # If we have learning data, use adaptive extraction\n            if learning_prompts:\n                text = extract_full_text(pdf_path)\n                if text:\n                    instructions = build_adaptive_field_instructions(learning_prompts)\n                    mistakes_prompt = get_common_mistakes_prompt(learning_prompts)\n\n                    prompt = f\"\"\"Extract metadata from this document:\n        {text[:2000]}\n\n        Fields to extract: {json.dumps(instructions, indent=2)}\n\n        Common mistakes to avoid: {mistakes_prompt}\n\n        Return ONLY JSON with keys: title, type, publication_date, version, source, context, country, language, url_source\"\"\"\n\n                    result = call_llm_with_learned_prompt(prompt)\n                    if result and isinstance(result, dict):\n                        return result\n\n        except Exception as e:\n            print(f\"Learning extraction failed: {e}\")\n\n        # Fallback to your working Mistral extraction\n        try:\n            result = call_mistral_with_confidence(extract_full_text(pdf_path), url, pdf_path)\n            if result and 'metadata' in result:\n                return result['metadata']\n        except Exception as e:\n            print(f\"Mistral extraction failed: {e}\")\n\n        # Final fallback\n        return extract_basic_fallback(pdf_path, url)\n    else:\n        return extract_metadata_local_pipeline(pdf_path)\n\n\ndef get_learned_field_improvements():\n    \"\"\"Get field-specific improvements from feedback data\"\"\"\n    try:\n        from .models import MetadataFeedback\n\n        field_insights = {}\n\n        for feedback in MetadataFeedback.objects.all():\n            corrections = feedback.corrections_made\n\n            # Analyze wrong fields\n            for wrong in corrections.get('corrected_fields', []):\n                field = wrong.get('field')\n                ai_value = wrong.get('ai_value', '')\n                correct_value = wrong.get('human_value', '')\n\n                if field not in field_insights:\n                    field_insights[field] = {'mistakes': [], 'patterns': []}\n\n                field_insights[field]['mistakes'].append({\n                    'wrong': ai_value,\n                    'correct': correct_value\n                })\n\n        return field_insights\n\n    except Exception as e:\n        print(f\"Learning insights error: {e}\")\n        return {}\n\n\ndef build_adaptive_field_instructions(learning_prompts):\n    \"\"\"Build field instructions that incorporate learning\"\"\"\n    instructions = {}\n\n    base_fields = {\n        'title': \"Document title\",\n        'type': \"Document type (guide, report, etc)\",\n        'publication_date': \"Publication date\",\n        'source': \"Source organization\",\n        'language': \"Document language code (en, fr, etc.)\",\n        'context': \"The field of the document content (pharmaceutical, legal, etc.) \",\n    }\n\n    for field, base_desc in base_fields.items():\n        if field in learning_prompts:\n            mistakes = learning_prompts[field]['mistakes']\n            if mistakes:\n                # Add specific guidance based on past mistakes\n                common_errors = [m['wrong'] for m in mistakes[-3:]]  # Last 3 mistakes\n                instructions[field] = f\"{base_desc}. AVOID these patterns: {', '.join(common_errors)}\"\n            else:\n                instructions[field] = base_desc\n        else:\n            instructions[field] = base_desc\n\n    return instructions\n\n\ndef get_common_mistakes_prompt(learning_prompts):\n    \"\"\"Generate prompt section about common mistakes\"\"\"\n    mistakes = []\n\n    for field, data in learning_prompts.items():\n        if data['mistakes']:\n            latest_mistakes = data['mistakes'][-2:]  # Last 2 mistakes\n            for mistake in latest_mistakes:\n                mistakes.append(f\"Don't confuse {field}: '{mistake['wrong']}' should be '{mistake['correct']}'\")\n\n    return \"\\n\".join(mistakes[:5])\n\n\ndef extract_basic_fallback(file_path: str, source_url: str) -> dict:\n    \"\"\"Basic fallback with honest low confidence scores\"\"\"\n    reader = PdfReader(file_path)\n    info = reader.metadata or {}\n    title = info.title or Path(file_path).stem\n    full_text = extract_full_text(file_path)\n\n    try:\n        lang = detect(full_text)\n    except:\n        lang = \"en\"\n\n    basic_meta = {\n        \"title\": title,\n        \"type\": \"unknown\",\n        \"publication_date\": \"\",\n        \"version\": \"\",\n        \"source\": \"\",\n        \"context\": \"\",\n        \"country\": \"\",\n        \"language\": lang,\n        \"url_source\": source_url\n    }\n\n    conf_scores = {\n        'title': 30 if title else 0,\n        'type': 0,\n        'publication_date': 0,\n        'version': 0,\n        'source': 0,\n        'context': 0,\n        'country': 0,\n        'language': 80 if lang else 0\n    }\n\n    overall_quality = calculate_overall_quality(conf_scores)\n\n    basic_meta['quality'] = {\n        'extraction_rate': overall_quality,\n        'field_scores': conf_scores,\n        'extraction_reasoning': {\n            'title': 'Basic PDF metadata extraction',\n            'type': 'Could not determine document type',\n            'source': 'No source identification possible'\n        },\n        'extracted_fields': 1,\n        'total_fields': len(conf_scores),\n        'llm_powered': False\n    }\n    return basic_meta",
                    "language": "python"
                }
            },
            "createdAt": 1760838826563,
            "isInternal": false,
            "rawContent": {
                "type": "doc",
                "content": [
                    {
                        "type": "paragraph",
                        "content": [
                            {
                                "type": "text",
                                "text": "Yes "
                            }
                        ]
                    }
                ]
            },
            "autoAppliedRules": []
        },
        {
            "id": "8fdc9e54-085e-4bbe-9654-f787074b8f09",
            "role": "assistant",
            "content": [
                {
                    "type": "thought",
                    "text": "**Checking pre-action instructions**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_4nJ17vRdZkUefFkKL78PXFVy",
                    "statusText": "Viewed directory",
                    "status": "SUCCESS",
                    "context": {
                        "files": [
                            {
                                "path": ".zencoder/rules",
                                "fsPath": "C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/.zencoder/rules"
                            }
                        ]
                    },
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Checking for repository rules file**"
                },
                {
                    "type": "toolCall",
                    "toolCallId": "call_QfHAe11nxtcanOCqNnua3BGU",
                    "title": "",
                    "statusText": "File not found",
                    "status": "WARNING",
                    "isHidden": false
                },
                {
                    "type": "thought",
                    "text": "**Noticing missing file**"
                },
                {
                    "type": "text",
                    "text": "# Preliminary Observation\n\nIt looks like the repository metadata file `C:/Users/lasss/PycharmProjects/MyProjectIABlcokLabs/.zencoder/rules/repo.md` is missing. Creating this file (even with a short overview of the project layout, frameworks, or setup instructions) would help future assistance stay accurate and efficient. Let me know if you‚Äôd like me to add a stub for it."
                }
            ],
            "context": {
                "author": "Code agent",
                "operationId": "df990d51-a342-49b4-972f-4642d868cbe7",
                "usageStatistics": {
                    "llmCallsUsedForAnswer": 3.0,
                    "llmCallsUsedTotal": 30.0,
                    "llmCallsLimit": 30.0,
                    "limitRenewalMillisecondTimestamp": 1760924472766
                }
            },
            "createdAt": 1760838829512,
            "isInternal": false
        }
    ],
    "isAgent": true,
    "isNameGenerated": true,
    "isCustomAgent": false,
    "isUnitTestsAgent": false,
    "isE2EAgent": false,
    "isWebDevAgent": false,
    "isQnaAgent": false,
    "isRepoInfoAgent": false,
    "model": "default"
}
}